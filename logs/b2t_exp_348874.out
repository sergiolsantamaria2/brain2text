TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348874
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348874/torch_extensions
WANDB_DIR=/tmp/e12511253_b2t_348874/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/e12511253_b2t_348874/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  4 21:15 /tmp/e12511253_b2t_348874/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
trained_models -> /tmp/e12511253_b2t_348874/trained_models
OUT_ROOT=/tmp/e12511253_b2t_348874/trained_models
==============================================
Job: b2t_exp  ID: 348874
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/input_dropout/lr40_wd1e-5
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/input_dropout/lr40_wd1e-5 ==========
Num configs: 5

=== RUN base_input_dropout_wd1e-5.yaml ===
2026-01-04 21:15:17,736: Using device: cuda:0
2026-01-04 21:15:19,741: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-04 21:15:19,763: Using 45 sessions after filtering (from 45).
2026-01-04 21:15:20,168: Using torch.compile (if available)
2026-01-04 21:15:20,169: torch.compile not available (torch<2.0). Skipping.
2026-01-04 21:15:20,169: Initialized RNN decoding model
2026-01-04 21:15:20,169: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-04 21:15:20,169: Model has 44,907,305 parameters
2026-01-04 21:15:20,169: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-04 21:15:21,441: Successfully initialized datasets
2026-01-04 21:15:21,441: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-04 21:15:22,567: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.193
2026-01-04 21:15:22,567: Running test after training batch: 0
2026-01-04 21:15:22,680: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:15:27,982: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-04 21:15:28,692: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-04 21:16:02,209: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 39.642
2026-01-04 21:16:02,210: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-04 21:16:02,210: t15.2023.08.13 val PER: 1.3056
2026-01-04 21:16:02,210: t15.2023.08.18 val PER: 1.4208
2026-01-04 21:16:02,210: t15.2023.08.20 val PER: 1.3002
2026-01-04 21:16:02,210: t15.2023.08.25 val PER: 1.3389
2026-01-04 21:16:02,210: t15.2023.08.27 val PER: 1.2460
2026-01-04 21:16:02,211: t15.2023.09.01 val PER: 1.4537
2026-01-04 21:16:02,211: t15.2023.09.03 val PER: 1.3171
2026-01-04 21:16:02,211: t15.2023.09.24 val PER: 1.5461
2026-01-04 21:16:02,211: t15.2023.09.29 val PER: 1.4671
2026-01-04 21:16:02,211: t15.2023.10.01 val PER: 1.2147
2026-01-04 21:16:02,211: t15.2023.10.06 val PER: 1.4876
2026-01-04 21:16:02,211: t15.2023.10.08 val PER: 1.1827
2026-01-04 21:16:02,211: t15.2023.10.13 val PER: 1.3964
2026-01-04 21:16:02,211: t15.2023.10.15 val PER: 1.3889
2026-01-04 21:16:02,211: t15.2023.10.20 val PER: 1.4866
2026-01-04 21:16:02,212: t15.2023.10.22 val PER: 1.3942
2026-01-04 21:16:02,212: t15.2023.11.03 val PER: 1.5923
2026-01-04 21:16:02,212: t15.2023.11.04 val PER: 2.0171
2026-01-04 21:16:02,212: t15.2023.11.17 val PER: 1.9518
2026-01-04 21:16:02,212: t15.2023.11.19 val PER: 1.6707
2026-01-04 21:16:02,212: t15.2023.11.26 val PER: 1.5413
2026-01-04 21:16:02,212: t15.2023.12.03 val PER: 1.4254
2026-01-04 21:16:02,212: t15.2023.12.08 val PER: 1.4487
2026-01-04 21:16:02,212: t15.2023.12.10 val PER: 1.6899
2026-01-04 21:16:02,212: t15.2023.12.17 val PER: 1.3077
2026-01-04 21:16:02,212: t15.2023.12.29 val PER: 1.4063
2026-01-04 21:16:02,213: t15.2024.02.25 val PER: 1.4228
2026-01-04 21:16:02,213: t15.2024.03.08 val PER: 1.3257
2026-01-04 21:16:02,213: t15.2024.03.15 val PER: 1.3196
2026-01-04 21:16:02,213: t15.2024.03.17 val PER: 1.4052
2026-01-04 21:16:02,213: t15.2024.05.10 val PER: 1.3224
2026-01-04 21:16:02,213: t15.2024.06.14 val PER: 1.5315
2026-01-04 21:16:02,213: t15.2024.07.19 val PER: 1.0817
2026-01-04 21:16:02,213: t15.2024.07.21 val PER: 1.6290
2026-01-04 21:16:02,213: t15.2024.07.28 val PER: 1.6588
2026-01-04 21:16:02,213: t15.2025.01.10 val PER: 1.0923
2026-01-04 21:16:02,213: t15.2025.01.12 val PER: 1.7629
2026-01-04 21:16:02,213: t15.2025.03.14 val PER: 1.0414
2026-01-04 21:16:02,214: t15.2025.03.16 val PER: 1.6257
2026-01-04 21:16:02,214: t15.2025.03.30 val PER: 1.2874
2026-01-04 21:16:02,214: t15.2025.04.13 val PER: 1.5949
2026-01-04 21:16:02,214: New best val WER(1gram) inf% --> 100.00%
2026-01-04 21:16:02,214: Checkpointing model
2026-01-04 21:16:02,472: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:16:02,734: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_0
2026-01-04 21:16:21,044: Train batch 200: loss: 77.59 grad norm: 106.15 time: 0.053
2026-01-04 21:16:38,886: Train batch 400: loss: 53.55 grad norm: 82.94 time: 0.062
2026-01-04 21:16:47,895: Running test after training batch: 500
2026-01-04 21:16:48,058: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:16:53,049: WER debug example
  GT : you can see the code at this point as well
  PR : used and ease thus uhde at this uhde is aisle
2026-01-04 21:16:53,080: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as tides
2026-01-04 21:16:55,341: Val batch 500: PER (avg): 0.5195 CTC Loss (avg): 55.4692 WER(1gram): 88.58% (n=64) time: 7.446
2026-01-04 21:16:55,342: WER lens: avg_true_words=6.16 avg_pred_words=5.67 max_pred_words=11
2026-01-04 21:16:55,342: t15.2023.08.13 val PER: 0.4647
2026-01-04 21:16:55,342: t15.2023.08.18 val PER: 0.4619
2026-01-04 21:16:55,342: t15.2023.08.20 val PER: 0.4400
2026-01-04 21:16:55,342: t15.2023.08.25 val PER: 0.4337
2026-01-04 21:16:55,342: t15.2023.08.27 val PER: 0.5225
2026-01-04 21:16:55,342: t15.2023.09.01 val PER: 0.4115
2026-01-04 21:16:55,342: t15.2023.09.03 val PER: 0.5024
2026-01-04 21:16:55,343: t15.2023.09.24 val PER: 0.4345
2026-01-04 21:16:55,343: t15.2023.09.29 val PER: 0.4684
2026-01-04 21:16:55,343: t15.2023.10.01 val PER: 0.5198
2026-01-04 21:16:55,343: t15.2023.10.06 val PER: 0.4295
2026-01-04 21:16:55,343: t15.2023.10.08 val PER: 0.5332
2026-01-04 21:16:55,343: t15.2023.10.13 val PER: 0.5803
2026-01-04 21:16:55,343: t15.2023.10.15 val PER: 0.4904
2026-01-04 21:16:55,343: t15.2023.10.20 val PER: 0.4631
2026-01-04 21:16:55,344: t15.2023.10.22 val PER: 0.4488
2026-01-04 21:16:55,344: t15.2023.11.03 val PER: 0.5034
2026-01-04 21:16:55,344: t15.2023.11.04 val PER: 0.2696
2026-01-04 21:16:55,344: t15.2023.11.17 val PER: 0.3639
2026-01-04 21:16:55,344: t15.2023.11.19 val PER: 0.3194
2026-01-04 21:16:55,344: t15.2023.11.26 val PER: 0.5435
2026-01-04 21:16:55,344: t15.2023.12.03 val PER: 0.5147
2026-01-04 21:16:55,344: t15.2023.12.08 val PER: 0.5233
2026-01-04 21:16:55,344: t15.2023.12.10 val PER: 0.4612
2026-01-04 21:16:55,344: t15.2023.12.17 val PER: 0.5676
2026-01-04 21:16:55,344: t15.2023.12.29 val PER: 0.5553
2026-01-04 21:16:55,345: t15.2024.02.25 val PER: 0.4705
2026-01-04 21:16:55,345: t15.2024.03.08 val PER: 0.6230
2026-01-04 21:16:55,345: t15.2024.03.15 val PER: 0.5566
2026-01-04 21:16:55,345: t15.2024.03.17 val PER: 0.5126
2026-01-04 21:16:55,345: t15.2024.05.10 val PER: 0.5572
2026-01-04 21:16:55,345: t15.2024.06.14 val PER: 0.5158
2026-01-04 21:16:55,345: t15.2024.07.19 val PER: 0.6724
2026-01-04 21:16:55,345: t15.2024.07.21 val PER: 0.4710
2026-01-04 21:16:55,345: t15.2024.07.28 val PER: 0.5044
2026-01-04 21:16:55,345: t15.2025.01.10 val PER: 0.7424
2026-01-04 21:16:55,345: t15.2025.01.12 val PER: 0.5620
2026-01-04 21:16:55,346: t15.2025.03.14 val PER: 0.7515
2026-01-04 21:16:55,346: t15.2025.03.16 val PER: 0.5982
2026-01-04 21:16:55,346: t15.2025.03.30 val PER: 0.7287
2026-01-04 21:16:55,346: t15.2025.04.13 val PER: 0.5706
2026-01-04 21:16:55,346: New best val WER(1gram) 100.00% --> 88.58%
2026-01-04 21:16:55,346: Checkpointing model
2026-01-04 21:16:55,941: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:16:56,208: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_500
2026-01-04 21:17:05,431: Train batch 600: loss: 48.79 grad norm: 79.10 time: 0.076
2026-01-04 21:17:23,431: Train batch 800: loss: 41.17 grad norm: 85.40 time: 0.056
2026-01-04 21:17:41,464: Train batch 1000: loss: 42.56 grad norm: 77.56 time: 0.065
2026-01-04 21:17:41,464: Running test after training batch: 1000
2026-01-04 21:17:41,611: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:17:46,505: WER debug example
  GT : you can see the code at this point as well
  PR : used ent ease thus code it this and is will
2026-01-04 21:17:46,539: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus wass it
2026-01-04 21:17:48,382: Val batch 1000: PER (avg): 0.4085 CTC Loss (avg): 42.3943 WER(1gram): 81.73% (n=64) time: 6.918
2026-01-04 21:17:48,382: WER lens: avg_true_words=6.16 avg_pred_words=5.56 max_pred_words=12
2026-01-04 21:17:48,383: t15.2023.08.13 val PER: 0.3815
2026-01-04 21:17:48,383: t15.2023.08.18 val PER: 0.3412
2026-01-04 21:17:48,383: t15.2023.08.20 val PER: 0.3447
2026-01-04 21:17:48,383: t15.2023.08.25 val PER: 0.2982
2026-01-04 21:17:48,383: t15.2023.08.27 val PER: 0.4228
2026-01-04 21:17:48,383: t15.2023.09.01 val PER: 0.3101
2026-01-04 21:17:48,383: t15.2023.09.03 val PER: 0.3931
2026-01-04 21:17:48,383: t15.2023.09.24 val PER: 0.3180
2026-01-04 21:17:48,383: t15.2023.09.29 val PER: 0.3586
2026-01-04 21:17:48,383: t15.2023.10.01 val PER: 0.4036
2026-01-04 21:17:48,383: t15.2023.10.06 val PER: 0.3132
2026-01-04 21:17:48,383: t15.2023.10.08 val PER: 0.4465
2026-01-04 21:17:48,384: t15.2023.10.13 val PER: 0.4732
2026-01-04 21:17:48,384: t15.2023.10.15 val PER: 0.3764
2026-01-04 21:17:48,384: t15.2023.10.20 val PER: 0.3523
2026-01-04 21:17:48,384: t15.2023.10.22 val PER: 0.3519
2026-01-04 21:17:48,384: t15.2023.11.03 val PER: 0.3996
2026-01-04 21:17:48,384: t15.2023.11.04 val PER: 0.1706
2026-01-04 21:17:48,384: t15.2023.11.17 val PER: 0.2597
2026-01-04 21:17:48,384: t15.2023.11.19 val PER: 0.2236
2026-01-04 21:17:48,384: t15.2023.11.26 val PER: 0.4428
2026-01-04 21:17:48,384: t15.2023.12.03 val PER: 0.4055
2026-01-04 21:17:48,384: t15.2023.12.08 val PER: 0.4081
2026-01-04 21:17:48,384: t15.2023.12.10 val PER: 0.3403
2026-01-04 21:17:48,385: t15.2023.12.17 val PER: 0.4033
2026-01-04 21:17:48,385: t15.2023.12.29 val PER: 0.4029
2026-01-04 21:17:48,385: t15.2024.02.25 val PER: 0.3581
2026-01-04 21:17:48,385: t15.2024.03.08 val PER: 0.4964
2026-01-04 21:17:48,385: t15.2024.03.15 val PER: 0.4497
2026-01-04 21:17:48,385: t15.2024.03.17 val PER: 0.4073
2026-01-04 21:17:48,385: t15.2024.05.10 val PER: 0.4101
2026-01-04 21:17:48,385: t15.2024.06.14 val PER: 0.4022
2026-01-04 21:17:48,385: t15.2024.07.19 val PER: 0.5359
2026-01-04 21:17:48,386: t15.2024.07.21 val PER: 0.3738
2026-01-04 21:17:48,386: t15.2024.07.28 val PER: 0.4154
2026-01-04 21:17:48,386: t15.2025.01.10 val PER: 0.6143
2026-01-04 21:17:48,386: t15.2025.01.12 val PER: 0.4519
2026-01-04 21:17:48,386: t15.2025.03.14 val PER: 0.6420
2026-01-04 21:17:48,386: t15.2025.03.16 val PER: 0.4804
2026-01-04 21:17:48,386: t15.2025.03.30 val PER: 0.6552
2026-01-04 21:17:48,386: t15.2025.04.13 val PER: 0.4893
2026-01-04 21:17:48,387: New best val WER(1gram) 88.58% --> 81.73%
2026-01-04 21:17:48,388: Checkpointing model
2026-01-04 21:17:49,007: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:17:49,275: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_1000
2026-01-04 21:18:06,772: Train batch 1200: loss: 33.29 grad norm: 76.44 time: 0.067
2026-01-04 21:18:25,046: Train batch 1400: loss: 35.92 grad norm: 80.64 time: 0.060
2026-01-04 21:18:34,163: Running test after training batch: 1500
2026-01-04 21:18:34,362: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:18:39,446: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt ease the owed it this boyde is will
2026-01-04 21:18:39,478: WER debug example
  GT : how does it keep the cost down
  PR : houde is it heap that cost
2026-01-04 21:18:41,058: Val batch 1500: PER (avg): 0.3777 CTC Loss (avg): 37.1997 WER(1gram): 77.41% (n=64) time: 6.894
2026-01-04 21:18:41,058: WER lens: avg_true_words=6.16 avg_pred_words=5.14 max_pred_words=11
2026-01-04 21:18:41,058: t15.2023.08.13 val PER: 0.3514
2026-01-04 21:18:41,058: t15.2023.08.18 val PER: 0.3127
2026-01-04 21:18:41,058: t15.2023.08.20 val PER: 0.3026
2026-01-04 21:18:41,058: t15.2023.08.25 val PER: 0.2636
2026-01-04 21:18:41,058: t15.2023.08.27 val PER: 0.3971
2026-01-04 21:18:41,058: t15.2023.09.01 val PER: 0.2711
2026-01-04 21:18:41,058: t15.2023.09.03 val PER: 0.3717
2026-01-04 21:18:41,059: t15.2023.09.24 val PER: 0.3022
2026-01-04 21:18:41,059: t15.2023.09.29 val PER: 0.3408
2026-01-04 21:18:41,059: t15.2023.10.01 val PER: 0.3930
2026-01-04 21:18:41,059: t15.2023.10.06 val PER: 0.2853
2026-01-04 21:18:41,060: t15.2023.10.08 val PER: 0.4452
2026-01-04 21:18:41,060: t15.2023.10.13 val PER: 0.4282
2026-01-04 21:18:41,060: t15.2023.10.15 val PER: 0.3586
2026-01-04 21:18:41,060: t15.2023.10.20 val PER: 0.3322
2026-01-04 21:18:41,060: t15.2023.10.22 val PER: 0.3185
2026-01-04 21:18:41,060: t15.2023.11.03 val PER: 0.3609
2026-01-04 21:18:41,061: t15.2023.11.04 val PER: 0.1126
2026-01-04 21:18:41,061: t15.2023.11.17 val PER: 0.2255
2026-01-04 21:18:41,061: t15.2023.11.19 val PER: 0.1597
2026-01-04 21:18:41,061: t15.2023.11.26 val PER: 0.4072
2026-01-04 21:18:41,061: t15.2023.12.03 val PER: 0.3729
2026-01-04 21:18:41,061: t15.2023.12.08 val PER: 0.3515
2026-01-04 21:18:41,061: t15.2023.12.10 val PER: 0.2917
2026-01-04 21:18:41,061: t15.2023.12.17 val PER: 0.3763
2026-01-04 21:18:41,061: t15.2023.12.29 val PER: 0.3638
2026-01-04 21:18:41,062: t15.2024.02.25 val PER: 0.3048
2026-01-04 21:18:41,062: t15.2024.03.08 val PER: 0.4538
2026-01-04 21:18:41,062: t15.2024.03.15 val PER: 0.4165
2026-01-04 21:18:41,062: t15.2024.03.17 val PER: 0.3759
2026-01-04 21:18:41,062: t15.2024.05.10 val PER: 0.3863
2026-01-04 21:18:41,062: t15.2024.06.14 val PER: 0.3880
2026-01-04 21:18:41,062: t15.2024.07.19 val PER: 0.5188
2026-01-04 21:18:41,062: t15.2024.07.21 val PER: 0.3476
2026-01-04 21:18:41,062: t15.2024.07.28 val PER: 0.3684
2026-01-04 21:18:41,062: t15.2025.01.10 val PER: 0.6240
2026-01-04 21:18:41,063: t15.2025.01.12 val PER: 0.4265
2026-01-04 21:18:41,063: t15.2025.03.14 val PER: 0.6050
2026-01-04 21:18:41,063: t15.2025.03.16 val PER: 0.4555
2026-01-04 21:18:41,063: t15.2025.03.30 val PER: 0.6195
2026-01-04 21:18:41,063: t15.2025.04.13 val PER: 0.4693
2026-01-04 21:18:41,063: New best val WER(1gram) 81.73% --> 77.41%
2026-01-04 21:18:41,063: Checkpointing model
2026-01-04 21:18:41,662: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:18:41,931: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_1500
2026-01-04 21:18:50,809: Train batch 1600: loss: 36.44 grad norm: 77.14 time: 0.063
2026-01-04 21:19:09,227: Train batch 1800: loss: 34.91 grad norm: 72.05 time: 0.088
2026-01-04 21:19:27,598: Train batch 2000: loss: 33.52 grad norm: 66.81 time: 0.067
2026-01-04 21:19:27,599: Running test after training batch: 2000
2026-01-04 21:19:27,745: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:19:32,650: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this bonde is will
2026-01-04 21:19:32,680: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heeke thus kos it
2026-01-04 21:19:34,200: Val batch 2000: PER (avg): 0.3282 CTC Loss (avg): 32.8826 WER(1gram): 71.57% (n=64) time: 6.601
2026-01-04 21:19:34,201: WER lens: avg_true_words=6.16 avg_pred_words=5.55 max_pred_words=11
2026-01-04 21:19:34,201: t15.2023.08.13 val PER: 0.2879
2026-01-04 21:19:34,201: t15.2023.08.18 val PER: 0.2598
2026-01-04 21:19:34,201: t15.2023.08.20 val PER: 0.2486
2026-01-04 21:19:34,201: t15.2023.08.25 val PER: 0.2304
2026-01-04 21:19:34,201: t15.2023.08.27 val PER: 0.3248
2026-01-04 21:19:34,201: t15.2023.09.01 val PER: 0.2281
2026-01-04 21:19:34,201: t15.2023.09.03 val PER: 0.3135
2026-01-04 21:19:34,201: t15.2023.09.24 val PER: 0.2524
2026-01-04 21:19:34,202: t15.2023.09.29 val PER: 0.2750
2026-01-04 21:19:34,202: t15.2023.10.01 val PER: 0.3355
2026-01-04 21:19:34,202: t15.2023.10.06 val PER: 0.2454
2026-01-04 21:19:34,202: t15.2023.10.08 val PER: 0.3911
2026-01-04 21:19:34,202: t15.2023.10.13 val PER: 0.3778
2026-01-04 21:19:34,202: t15.2023.10.15 val PER: 0.3072
2026-01-04 21:19:34,202: t15.2023.10.20 val PER: 0.2819
2026-01-04 21:19:34,202: t15.2023.10.22 val PER: 0.2628
2026-01-04 21:19:34,202: t15.2023.11.03 val PER: 0.3216
2026-01-04 21:19:34,202: t15.2023.11.04 val PER: 0.0956
2026-01-04 21:19:34,202: t15.2023.11.17 val PER: 0.1711
2026-01-04 21:19:34,202: t15.2023.11.19 val PER: 0.1377
2026-01-04 21:19:34,202: t15.2023.11.26 val PER: 0.3645
2026-01-04 21:19:34,202: t15.2023.12.03 val PER: 0.3225
2026-01-04 21:19:34,202: t15.2023.12.08 val PER: 0.3196
2026-01-04 21:19:34,202: t15.2023.12.10 val PER: 0.2786
2026-01-04 21:19:34,203: t15.2023.12.17 val PER: 0.3191
2026-01-04 21:19:34,203: t15.2023.12.29 val PER: 0.3322
2026-01-04 21:19:34,203: t15.2024.02.25 val PER: 0.2767
2026-01-04 21:19:34,203: t15.2024.03.08 val PER: 0.4211
2026-01-04 21:19:34,203: t15.2024.03.15 val PER: 0.3709
2026-01-04 21:19:34,203: t15.2024.03.17 val PER: 0.3410
2026-01-04 21:19:34,203: t15.2024.05.10 val PER: 0.3477
2026-01-04 21:19:34,203: t15.2024.06.14 val PER: 0.3375
2026-01-04 21:19:34,203: t15.2024.07.19 val PER: 0.4641
2026-01-04 21:19:34,203: t15.2024.07.21 val PER: 0.2848
2026-01-04 21:19:34,203: t15.2024.07.28 val PER: 0.3191
2026-01-04 21:19:34,203: t15.2025.01.10 val PER: 0.5207
2026-01-04 21:19:34,203: t15.2025.01.12 val PER: 0.3903
2026-01-04 21:19:34,203: t15.2025.03.14 val PER: 0.5251
2026-01-04 21:19:34,203: t15.2025.03.16 val PER: 0.4045
2026-01-04 21:19:34,203: t15.2025.03.30 val PER: 0.5391
2026-01-04 21:19:34,203: t15.2025.04.13 val PER: 0.4009
2026-01-04 21:19:34,205: New best val WER(1gram) 77.41% --> 71.57%
2026-01-04 21:19:34,205: Checkpointing model
2026-01-04 21:19:34,812: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:19:35,084: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_2000
2026-01-04 21:19:52,623: Train batch 2200: loss: 29.03 grad norm: 78.13 time: 0.060
2026-01-04 21:20:10,301: Train batch 2400: loss: 29.11 grad norm: 61.75 time: 0.052
2026-01-04 21:20:19,205: Running test after training batch: 2500
2026-01-04 21:20:19,315: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:20:24,338: WER debug example
  GT : you can see the code at this point as well
  PR : yule end e the good at this point is will
2026-01-04 21:20:24,367: WER debug example
  GT : how does it keep the cost down
  PR : houde just it eke thus cost it
2026-01-04 21:20:25,996: Val batch 2500: PER (avg): 0.3003 CTC Loss (avg): 30.1737 WER(1gram): 68.53% (n=64) time: 6.791
2026-01-04 21:20:25,997: WER lens: avg_true_words=6.16 avg_pred_words=5.67 max_pred_words=11
2026-01-04 21:20:25,997: t15.2023.08.13 val PER: 0.2838
2026-01-04 21:20:25,997: t15.2023.08.18 val PER: 0.2389
2026-01-04 21:20:25,997: t15.2023.08.20 val PER: 0.2375
2026-01-04 21:20:25,997: t15.2023.08.25 val PER: 0.1988
2026-01-04 21:20:25,997: t15.2023.08.27 val PER: 0.3215
2026-01-04 21:20:25,997: t15.2023.09.01 val PER: 0.2045
2026-01-04 21:20:25,997: t15.2023.09.03 val PER: 0.2922
2026-01-04 21:20:25,997: t15.2023.09.24 val PER: 0.2342
2026-01-04 21:20:25,997: t15.2023.09.29 val PER: 0.2451
2026-01-04 21:20:25,998: t15.2023.10.01 val PER: 0.3131
2026-01-04 21:20:25,998: t15.2023.10.06 val PER: 0.2153
2026-01-04 21:20:25,998: t15.2023.10.08 val PER: 0.3721
2026-01-04 21:20:25,998: t15.2023.10.13 val PER: 0.3514
2026-01-04 21:20:25,998: t15.2023.10.15 val PER: 0.2894
2026-01-04 21:20:25,998: t15.2023.10.20 val PER: 0.2651
2026-01-04 21:20:25,998: t15.2023.10.22 val PER: 0.2216
2026-01-04 21:20:25,998: t15.2023.11.03 val PER: 0.2978
2026-01-04 21:20:25,998: t15.2023.11.04 val PER: 0.0956
2026-01-04 21:20:25,998: t15.2023.11.17 val PER: 0.1462
2026-01-04 21:20:25,998: t15.2023.11.19 val PER: 0.1158
2026-01-04 21:20:25,998: t15.2023.11.26 val PER: 0.3420
2026-01-04 21:20:25,998: t15.2023.12.03 val PER: 0.2742
2026-01-04 21:20:25,999: t15.2023.12.08 val PER: 0.2810
2026-01-04 21:20:25,999: t15.2023.12.10 val PER: 0.2129
2026-01-04 21:20:25,999: t15.2023.12.17 val PER: 0.2859
2026-01-04 21:20:25,999: t15.2023.12.29 val PER: 0.3047
2026-01-04 21:20:25,999: t15.2024.02.25 val PER: 0.2388
2026-01-04 21:20:25,999: t15.2024.03.08 val PER: 0.3570
2026-01-04 21:20:25,999: t15.2024.03.15 val PER: 0.3502
2026-01-04 21:20:25,999: t15.2024.03.17 val PER: 0.3131
2026-01-04 21:20:25,999: t15.2024.05.10 val PER: 0.3150
2026-01-04 21:20:25,999: t15.2024.06.14 val PER: 0.3139
2026-01-04 21:20:25,999: t15.2024.07.19 val PER: 0.4351
2026-01-04 21:20:25,999: t15.2024.07.21 val PER: 0.2552
2026-01-04 21:20:26,000: t15.2024.07.28 val PER: 0.2919
2026-01-04 21:20:26,000: t15.2025.01.10 val PER: 0.4931
2026-01-04 21:20:26,000: t15.2025.01.12 val PER: 0.3533
2026-01-04 21:20:26,000: t15.2025.03.14 val PER: 0.4896
2026-01-04 21:20:26,000: t15.2025.03.16 val PER: 0.3521
2026-01-04 21:20:26,000: t15.2025.03.30 val PER: 0.5057
2026-01-04 21:20:26,000: t15.2025.04.13 val PER: 0.3837
2026-01-04 21:20:26,001: New best val WER(1gram) 71.57% --> 68.53%
2026-01-04 21:20:26,001: Checkpointing model
2026-01-04 21:20:26,605: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:20:26,877: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_2500
2026-01-04 21:20:35,615: Train batch 2600: loss: 35.45 grad norm: 84.33 time: 0.055
2026-01-04 21:20:53,571: Train batch 2800: loss: 25.62 grad norm: 69.84 time: 0.080
2026-01-04 21:21:11,535: Train batch 3000: loss: 31.68 grad norm: 71.11 time: 0.082
2026-01-04 21:21:11,536: Running test after training batch: 3000
2026-01-04 21:21:11,641: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:21:16,481: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the good at this point is will
2026-01-04 21:21:16,509: WER debug example
  GT : how does it keep the cost down
  PR : houde des it yip the rost et
2026-01-04 21:21:18,104: Val batch 3000: PER (avg): 0.2803 CTC Loss (avg): 27.6020 WER(1gram): 67.01% (n=64) time: 6.568
2026-01-04 21:21:18,104: WER lens: avg_true_words=6.16 avg_pred_words=5.70 max_pred_words=10
2026-01-04 21:21:18,104: t15.2023.08.13 val PER: 0.2651
2026-01-04 21:21:18,104: t15.2023.08.18 val PER: 0.2188
2026-01-04 21:21:18,104: t15.2023.08.20 val PER: 0.2073
2026-01-04 21:21:18,104: t15.2023.08.25 val PER: 0.2003
2026-01-04 21:21:18,105: t15.2023.08.27 val PER: 0.3039
2026-01-04 21:21:18,105: t15.2023.09.01 val PER: 0.1843
2026-01-04 21:21:18,105: t15.2023.09.03 val PER: 0.2815
2026-01-04 21:21:18,105: t15.2023.09.24 val PER: 0.2136
2026-01-04 21:21:18,105: t15.2023.09.29 val PER: 0.2297
2026-01-04 21:21:18,105: t15.2023.10.01 val PER: 0.2867
2026-01-04 21:21:18,105: t15.2023.10.06 val PER: 0.1970
2026-01-04 21:21:18,105: t15.2023.10.08 val PER: 0.3491
2026-01-04 21:21:18,105: t15.2023.10.13 val PER: 0.3452
2026-01-04 21:21:18,105: t15.2023.10.15 val PER: 0.2544
2026-01-04 21:21:18,105: t15.2023.10.20 val PER: 0.2886
2026-01-04 21:21:18,105: t15.2023.10.22 val PER: 0.2216
2026-01-04 21:21:18,105: t15.2023.11.03 val PER: 0.2687
2026-01-04 21:21:18,105: t15.2023.11.04 val PER: 0.0819
2026-01-04 21:21:18,106: t15.2023.11.17 val PER: 0.1322
2026-01-04 21:21:18,106: t15.2023.11.19 val PER: 0.1218
2026-01-04 21:21:18,106: t15.2023.11.26 val PER: 0.3123
2026-01-04 21:21:18,106: t15.2023.12.03 val PER: 0.2489
2026-01-04 21:21:18,106: t15.2023.12.08 val PER: 0.2537
2026-01-04 21:21:18,106: t15.2023.12.10 val PER: 0.1997
2026-01-04 21:21:18,106: t15.2023.12.17 val PER: 0.2578
2026-01-04 21:21:18,106: t15.2023.12.29 val PER: 0.2773
2026-01-04 21:21:18,106: t15.2024.02.25 val PER: 0.2360
2026-01-04 21:21:18,106: t15.2024.03.08 val PER: 0.3627
2026-01-04 21:21:18,106: t15.2024.03.15 val PER: 0.3383
2026-01-04 21:21:18,106: t15.2024.03.17 val PER: 0.2810
2026-01-04 21:21:18,107: t15.2024.05.10 val PER: 0.3031
2026-01-04 21:21:18,107: t15.2024.06.14 val PER: 0.3013
2026-01-04 21:21:18,107: t15.2024.07.19 val PER: 0.4133
2026-01-04 21:21:18,107: t15.2024.07.21 val PER: 0.2234
2026-01-04 21:21:18,107: t15.2024.07.28 val PER: 0.2794
2026-01-04 21:21:18,107: t15.2025.01.10 val PER: 0.4848
2026-01-04 21:21:18,107: t15.2025.01.12 val PER: 0.3295
2026-01-04 21:21:18,108: t15.2025.03.14 val PER: 0.4349
2026-01-04 21:21:18,108: t15.2025.03.16 val PER: 0.3272
2026-01-04 21:21:18,108: t15.2025.03.30 val PER: 0.4931
2026-01-04 21:21:18,108: t15.2025.04.13 val PER: 0.3538
2026-01-04 21:21:18,109: New best val WER(1gram) 68.53% --> 67.01%
2026-01-04 21:21:18,109: Checkpointing model
2026-01-04 21:21:18,731: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:21:18,998: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_3000
2026-01-04 21:21:36,440: Train batch 3200: loss: 26.24 grad norm: 66.58 time: 0.076
2026-01-04 21:21:53,930: Train batch 3400: loss: 18.23 grad norm: 55.22 time: 0.049
2026-01-04 21:22:02,807: Running test after training batch: 3500
2026-01-04 21:22:02,908: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:22:07,913: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point will
2026-01-04 21:22:07,942: WER debug example
  GT : how does it keep the cost down
  PR : aue des it epp the cussed get
2026-01-04 21:22:09,480: Val batch 3500: PER (avg): 0.2647 CTC Loss (avg): 26.3222 WER(1gram): 65.74% (n=64) time: 6.673
2026-01-04 21:22:09,480: WER lens: avg_true_words=6.16 avg_pred_words=5.84 max_pred_words=11
2026-01-04 21:22:09,480: t15.2023.08.13 val PER: 0.2318
2026-01-04 21:22:09,481: t15.2023.08.18 val PER: 0.2045
2026-01-04 21:22:09,481: t15.2023.08.20 val PER: 0.2113
2026-01-04 21:22:09,481: t15.2023.08.25 val PER: 0.1867
2026-01-04 21:22:09,481: t15.2023.08.27 val PER: 0.2733
2026-01-04 21:22:09,481: t15.2023.09.01 val PER: 0.1794
2026-01-04 21:22:09,481: t15.2023.09.03 val PER: 0.2613
2026-01-04 21:22:09,481: t15.2023.09.24 val PER: 0.2148
2026-01-04 21:22:09,481: t15.2023.09.29 val PER: 0.2176
2026-01-04 21:22:09,481: t15.2023.10.01 val PER: 0.2754
2026-01-04 21:22:09,481: t15.2023.10.06 val PER: 0.1927
2026-01-04 21:22:09,481: t15.2023.10.08 val PER: 0.3315
2026-01-04 21:22:09,481: t15.2023.10.13 val PER: 0.3157
2026-01-04 21:22:09,481: t15.2023.10.15 val PER: 0.2485
2026-01-04 21:22:09,481: t15.2023.10.20 val PER: 0.2383
2026-01-04 21:22:09,482: t15.2023.10.22 val PER: 0.2116
2026-01-04 21:22:09,482: t15.2023.11.03 val PER: 0.2646
2026-01-04 21:22:09,482: t15.2023.11.04 val PER: 0.0580
2026-01-04 21:22:09,482: t15.2023.11.17 val PER: 0.1182
2026-01-04 21:22:09,482: t15.2023.11.19 val PER: 0.1118
2026-01-04 21:22:09,482: t15.2023.11.26 val PER: 0.2841
2026-01-04 21:22:09,482: t15.2023.12.03 val PER: 0.2447
2026-01-04 21:22:09,482: t15.2023.12.08 val PER: 0.2383
2026-01-04 21:22:09,482: t15.2023.12.10 val PER: 0.2102
2026-01-04 21:22:09,482: t15.2023.12.17 val PER: 0.2432
2026-01-04 21:22:09,482: t15.2023.12.29 val PER: 0.2574
2026-01-04 21:22:09,482: t15.2024.02.25 val PER: 0.2107
2026-01-04 21:22:09,483: t15.2024.03.08 val PER: 0.3400
2026-01-04 21:22:09,483: t15.2024.03.15 val PER: 0.3152
2026-01-04 21:22:09,483: t15.2024.03.17 val PER: 0.2748
2026-01-04 21:22:09,483: t15.2024.05.10 val PER: 0.2571
2026-01-04 21:22:09,483: t15.2024.06.14 val PER: 0.2729
2026-01-04 21:22:09,483: t15.2024.07.19 val PER: 0.3949
2026-01-04 21:22:09,483: t15.2024.07.21 val PER: 0.2200
2026-01-04 21:22:09,483: t15.2024.07.28 val PER: 0.2757
2026-01-04 21:22:09,483: t15.2025.01.10 val PER: 0.4656
2026-01-04 21:22:09,483: t15.2025.01.12 val PER: 0.2948
2026-01-04 21:22:09,484: t15.2025.03.14 val PER: 0.4260
2026-01-04 21:22:09,484: t15.2025.03.16 val PER: 0.3181
2026-01-04 21:22:09,484: t15.2025.03.30 val PER: 0.4425
2026-01-04 21:22:09,484: t15.2025.04.13 val PER: 0.3281
2026-01-04 21:22:09,485: New best val WER(1gram) 67.01% --> 65.74%
2026-01-04 21:22:09,485: Checkpointing model
2026-01-04 21:22:10,091: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:22:10,359: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_3500
2026-01-04 21:22:19,345: Train batch 3600: loss: 22.55 grad norm: 63.43 time: 0.066
2026-01-04 21:22:37,272: Train batch 3800: loss: 25.33 grad norm: 66.92 time: 0.066
2026-01-04 21:22:55,614: Train batch 4000: loss: 19.45 grad norm: 56.54 time: 0.056
2026-01-04 21:22:55,614: Running test after training batch: 4000
2026-01-04 21:22:55,862: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:23:00,734: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-04 21:23:00,762: WER debug example
  GT : how does it keep the cost down
  PR : how dust it hipp the cussed nit
2026-01-04 21:23:02,362: Val batch 4000: PER (avg): 0.2487 CTC Loss (avg): 24.4218 WER(1gram): 63.71% (n=64) time: 6.747
2026-01-04 21:23:02,362: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-04 21:23:02,362: t15.2023.08.13 val PER: 0.2287
2026-01-04 21:23:02,362: t15.2023.08.18 val PER: 0.1961
2026-01-04 21:23:02,362: t15.2023.08.20 val PER: 0.2017
2026-01-04 21:23:02,362: t15.2023.08.25 val PER: 0.1596
2026-01-04 21:23:02,363: t15.2023.08.27 val PER: 0.2830
2026-01-04 21:23:02,363: t15.2023.09.01 val PER: 0.1542
2026-01-04 21:23:02,363: t15.2023.09.03 val PER: 0.2447
2026-01-04 21:23:02,363: t15.2023.09.24 val PER: 0.1942
2026-01-04 21:23:02,363: t15.2023.09.29 val PER: 0.2004
2026-01-04 21:23:02,363: t15.2023.10.01 val PER: 0.2629
2026-01-04 21:23:02,363: t15.2023.10.06 val PER: 0.1636
2026-01-04 21:23:02,363: t15.2023.10.08 val PER: 0.3221
2026-01-04 21:23:02,363: t15.2023.10.13 val PER: 0.3095
2026-01-04 21:23:02,363: t15.2023.10.15 val PER: 0.2393
2026-01-04 21:23:02,363: t15.2023.10.20 val PER: 0.2450
2026-01-04 21:23:02,363: t15.2023.10.22 val PER: 0.1904
2026-01-04 21:23:02,363: t15.2023.11.03 val PER: 0.2483
2026-01-04 21:23:02,363: t15.2023.11.04 val PER: 0.0648
2026-01-04 21:23:02,364: t15.2023.11.17 val PER: 0.1042
2026-01-04 21:23:02,364: t15.2023.11.19 val PER: 0.0918
2026-01-04 21:23:02,364: t15.2023.11.26 val PER: 0.2630
2026-01-04 21:23:02,364: t15.2023.12.03 val PER: 0.2143
2026-01-04 21:23:02,364: t15.2023.12.08 val PER: 0.2184
2026-01-04 21:23:02,364: t15.2023.12.10 val PER: 0.1879
2026-01-04 21:23:02,364: t15.2023.12.17 val PER: 0.2401
2026-01-04 21:23:02,364: t15.2023.12.29 val PER: 0.2512
2026-01-04 21:23:02,364: t15.2024.02.25 val PER: 0.2219
2026-01-04 21:23:02,364: t15.2024.03.08 val PER: 0.3329
2026-01-04 21:23:02,364: t15.2024.03.15 val PER: 0.3033
2026-01-04 21:23:02,365: t15.2024.03.17 val PER: 0.2587
2026-01-04 21:23:02,365: t15.2024.05.10 val PER: 0.2600
2026-01-04 21:23:02,365: t15.2024.06.14 val PER: 0.2697
2026-01-04 21:23:02,365: t15.2024.07.19 val PER: 0.3599
2026-01-04 21:23:02,365: t15.2024.07.21 val PER: 0.1924
2026-01-04 21:23:02,365: t15.2024.07.28 val PER: 0.2382
2026-01-04 21:23:02,365: t15.2025.01.10 val PER: 0.4201
2026-01-04 21:23:02,365: t15.2025.01.12 val PER: 0.2825
2026-01-04 21:23:02,365: t15.2025.03.14 val PER: 0.4201
2026-01-04 21:23:02,365: t15.2025.03.16 val PER: 0.3102
2026-01-04 21:23:02,365: t15.2025.03.30 val PER: 0.4011
2026-01-04 21:23:02,365: t15.2025.04.13 val PER: 0.3181
2026-01-04 21:23:02,367: New best val WER(1gram) 65.74% --> 63.71%
2026-01-04 21:23:02,367: Checkpointing model
2026-01-04 21:23:02,987: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:23:03,257: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_4000
2026-01-04 21:23:20,912: Train batch 4200: loss: 22.05 grad norm: 65.68 time: 0.079
2026-01-04 21:23:38,838: Train batch 4400: loss: 17.30 grad norm: 56.89 time: 0.066
2026-01-04 21:23:47,771: Running test after training batch: 4500
2026-01-04 21:23:47,913: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:23:52,886: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 21:23:52,916: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap thus cost get
2026-01-04 21:23:54,524: Val batch 4500: PER (avg): 0.2375 CTC Loss (avg): 23.3281 WER(1gram): 60.15% (n=64) time: 6.753
2026-01-04 21:23:54,524: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-04 21:23:54,525: t15.2023.08.13 val PER: 0.2079
2026-01-04 21:23:54,525: t15.2023.08.18 val PER: 0.1844
2026-01-04 21:23:54,525: t15.2023.08.20 val PER: 0.1906
2026-01-04 21:23:54,525: t15.2023.08.25 val PER: 0.1491
2026-01-04 21:23:54,525: t15.2023.08.27 val PER: 0.2540
2026-01-04 21:23:54,525: t15.2023.09.01 val PER: 0.1567
2026-01-04 21:23:54,525: t15.2023.09.03 val PER: 0.2363
2026-01-04 21:23:54,525: t15.2023.09.24 val PER: 0.1760
2026-01-04 21:23:54,526: t15.2023.09.29 val PER: 0.1946
2026-01-04 21:23:54,526: t15.2023.10.01 val PER: 0.2583
2026-01-04 21:23:54,526: t15.2023.10.06 val PER: 0.1453
2026-01-04 21:23:54,526: t15.2023.10.08 val PER: 0.3194
2026-01-04 21:23:54,526: t15.2023.10.13 val PER: 0.2971
2026-01-04 21:23:54,526: t15.2023.10.15 val PER: 0.2373
2026-01-04 21:23:54,526: t15.2023.10.20 val PER: 0.2315
2026-01-04 21:23:54,526: t15.2023.10.22 val PER: 0.1927
2026-01-04 21:23:54,526: t15.2023.11.03 val PER: 0.2436
2026-01-04 21:23:54,526: t15.2023.11.04 val PER: 0.0648
2026-01-04 21:23:54,527: t15.2023.11.17 val PER: 0.0933
2026-01-04 21:23:54,527: t15.2023.11.19 val PER: 0.0978
2026-01-04 21:23:54,527: t15.2023.11.26 val PER: 0.2688
2026-01-04 21:23:54,527: t15.2023.12.03 val PER: 0.2174
2026-01-04 21:23:54,527: t15.2023.12.08 val PER: 0.2117
2026-01-04 21:23:54,527: t15.2023.12.10 val PER: 0.1866
2026-01-04 21:23:54,527: t15.2023.12.17 val PER: 0.2256
2026-01-04 21:23:54,527: t15.2023.12.29 val PER: 0.2354
2026-01-04 21:23:54,527: t15.2024.02.25 val PER: 0.1966
2026-01-04 21:23:54,527: t15.2024.03.08 val PER: 0.3115
2026-01-04 21:23:54,527: t15.2024.03.15 val PER: 0.2846
2026-01-04 21:23:54,527: t15.2024.03.17 val PER: 0.2385
2026-01-04 21:23:54,527: t15.2024.05.10 val PER: 0.2541
2026-01-04 21:23:54,528: t15.2024.06.14 val PER: 0.2350
2026-01-04 21:23:54,528: t15.2024.07.19 val PER: 0.3421
2026-01-04 21:23:54,528: t15.2024.07.21 val PER: 0.1745
2026-01-04 21:23:54,528: t15.2024.07.28 val PER: 0.2199
2026-01-04 21:23:54,528: t15.2025.01.10 val PER: 0.4105
2026-01-04 21:23:54,528: t15.2025.01.12 val PER: 0.2610
2026-01-04 21:23:54,528: t15.2025.03.14 val PER: 0.4068
2026-01-04 21:23:54,528: t15.2025.03.16 val PER: 0.2866
2026-01-04 21:23:54,529: t15.2025.03.30 val PER: 0.4011
2026-01-04 21:23:54,529: t15.2025.04.13 val PER: 0.3024
2026-01-04 21:23:54,529: New best val WER(1gram) 63.71% --> 60.15%
2026-01-04 21:23:54,529: Checkpointing model
2026-01-04 21:23:55,126: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:23:55,395: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_4500
2026-01-04 21:24:04,151: Train batch 4600: loss: 20.34 grad norm: 64.63 time: 0.063
2026-01-04 21:24:22,282: Train batch 4800: loss: 13.87 grad norm: 54.48 time: 0.064
2026-01-04 21:24:40,481: Train batch 5000: loss: 31.89 grad norm: 84.60 time: 0.063
2026-01-04 21:24:40,481: Running test after training batch: 5000
2026-01-04 21:24:40,627: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:24:45,471: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as will
2026-01-04 21:24:45,499: WER debug example
  GT : how does it keep the cost down
  PR : how des it heap the cost get
2026-01-04 21:24:47,122: Val batch 5000: PER (avg): 0.2241 CTC Loss (avg): 21.8250 WER(1gram): 60.15% (n=64) time: 6.641
2026-01-04 21:24:47,123: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=11
2026-01-04 21:24:47,123: t15.2023.08.13 val PER: 0.1913
2026-01-04 21:24:47,123: t15.2023.08.18 val PER: 0.1710
2026-01-04 21:24:47,123: t15.2023.08.20 val PER: 0.1779
2026-01-04 21:24:47,123: t15.2023.08.25 val PER: 0.1265
2026-01-04 21:24:47,123: t15.2023.08.27 val PER: 0.2412
2026-01-04 21:24:47,124: t15.2023.09.01 val PER: 0.1404
2026-01-04 21:24:47,124: t15.2023.09.03 val PER: 0.2304
2026-01-04 21:24:47,124: t15.2023.09.24 val PER: 0.1808
2026-01-04 21:24:47,124: t15.2023.09.29 val PER: 0.1736
2026-01-04 21:24:47,124: t15.2023.10.01 val PER: 0.2312
2026-01-04 21:24:47,124: t15.2023.10.06 val PER: 0.1496
2026-01-04 21:24:47,124: t15.2023.10.08 val PER: 0.3207
2026-01-04 21:24:47,124: t15.2023.10.13 val PER: 0.2808
2026-01-04 21:24:47,125: t15.2023.10.15 val PER: 0.2182
2026-01-04 21:24:47,125: t15.2023.10.20 val PER: 0.2282
2026-01-04 21:24:47,125: t15.2023.10.22 val PER: 0.1748
2026-01-04 21:24:47,125: t15.2023.11.03 val PER: 0.2218
2026-01-04 21:24:47,125: t15.2023.11.04 val PER: 0.0478
2026-01-04 21:24:47,125: t15.2023.11.17 val PER: 0.0793
2026-01-04 21:24:47,125: t15.2023.11.19 val PER: 0.0719
2026-01-04 21:24:47,125: t15.2023.11.26 val PER: 0.2275
2026-01-04 21:24:47,125: t15.2023.12.03 val PER: 0.2017
2026-01-04 21:24:47,125: t15.2023.12.08 val PER: 0.2004
2026-01-04 21:24:47,125: t15.2023.12.10 val PER: 0.1603
2026-01-04 21:24:47,126: t15.2023.12.17 val PER: 0.2193
2026-01-04 21:24:47,126: t15.2023.12.29 val PER: 0.2183
2026-01-04 21:24:47,126: t15.2024.02.25 val PER: 0.1770
2026-01-04 21:24:47,126: t15.2024.03.08 val PER: 0.3101
2026-01-04 21:24:47,126: t15.2024.03.15 val PER: 0.2783
2026-01-04 21:24:47,126: t15.2024.03.17 val PER: 0.2329
2026-01-04 21:24:47,126: t15.2024.05.10 val PER: 0.2348
2026-01-04 21:24:47,126: t15.2024.06.14 val PER: 0.2539
2026-01-04 21:24:47,126: t15.2024.07.19 val PER: 0.3355
2026-01-04 21:24:47,126: t15.2024.07.21 val PER: 0.1786
2026-01-04 21:24:47,126: t15.2024.07.28 val PER: 0.2081
2026-01-04 21:24:47,127: t15.2025.01.10 val PER: 0.3857
2026-01-04 21:24:47,127: t15.2025.01.12 val PER: 0.2410
2026-01-04 21:24:47,127: t15.2025.03.14 val PER: 0.3950
2026-01-04 21:24:47,127: t15.2025.03.16 val PER: 0.2683
2026-01-04 21:24:47,127: t15.2025.03.30 val PER: 0.3977
2026-01-04 21:24:47,127: t15.2025.04.13 val PER: 0.2967
2026-01-04 21:24:47,385: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_5000
2026-01-04 21:25:05,508: Train batch 5200: loss: 16.31 grad norm: 58.44 time: 0.051
2026-01-04 21:25:23,677: Train batch 5400: loss: 17.29 grad norm: 60.09 time: 0.068
2026-01-04 21:25:32,755: Running test after training batch: 5500
2026-01-04 21:25:32,858: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:25:37,738: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point will
2026-01-04 21:25:37,766: WER debug example
  GT : how does it keep the cost down
  PR : how dust it kipp the cost tit
2026-01-04 21:25:39,332: Val batch 5500: PER (avg): 0.2136 CTC Loss (avg): 20.9272 WER(1gram): 55.08% (n=64) time: 6.576
2026-01-04 21:25:39,332: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=11
2026-01-04 21:25:39,332: t15.2023.08.13 val PER: 0.1840
2026-01-04 21:25:39,332: t15.2023.08.18 val PER: 0.1526
2026-01-04 21:25:39,333: t15.2023.08.20 val PER: 0.1652
2026-01-04 21:25:39,333: t15.2023.08.25 val PER: 0.1235
2026-01-04 21:25:39,333: t15.2023.08.27 val PER: 0.2444
2026-01-04 21:25:39,333: t15.2023.09.01 val PER: 0.1282
2026-01-04 21:25:39,333: t15.2023.09.03 val PER: 0.2150
2026-01-04 21:25:39,333: t15.2023.09.24 val PER: 0.1796
2026-01-04 21:25:39,334: t15.2023.09.29 val PER: 0.1768
2026-01-04 21:25:39,334: t15.2023.10.01 val PER: 0.2285
2026-01-04 21:25:39,334: t15.2023.10.06 val PER: 0.1389
2026-01-04 21:25:39,334: t15.2023.10.08 val PER: 0.2977
2026-01-04 21:25:39,334: t15.2023.10.13 val PER: 0.2676
2026-01-04 21:25:39,334: t15.2023.10.15 val PER: 0.2063
2026-01-04 21:25:39,334: t15.2023.10.20 val PER: 0.2315
2026-01-04 21:25:39,334: t15.2023.10.22 val PER: 0.1637
2026-01-04 21:25:39,334: t15.2023.11.03 val PER: 0.2280
2026-01-04 21:25:39,334: t15.2023.11.04 val PER: 0.0683
2026-01-04 21:25:39,334: t15.2023.11.17 val PER: 0.0778
2026-01-04 21:25:39,334: t15.2023.11.19 val PER: 0.0739
2026-01-04 21:25:39,334: t15.2023.11.26 val PER: 0.2123
2026-01-04 21:25:39,334: t15.2023.12.03 val PER: 0.1775
2026-01-04 21:25:39,334: t15.2023.12.08 val PER: 0.1891
2026-01-04 21:25:39,334: t15.2023.12.10 val PER: 0.1537
2026-01-04 21:25:39,335: t15.2023.12.17 val PER: 0.2121
2026-01-04 21:25:39,335: t15.2023.12.29 val PER: 0.2086
2026-01-04 21:25:39,335: t15.2024.02.25 val PER: 0.1770
2026-01-04 21:25:39,335: t15.2024.03.08 val PER: 0.3001
2026-01-04 21:25:39,335: t15.2024.03.15 val PER: 0.2652
2026-01-04 21:25:39,335: t15.2024.03.17 val PER: 0.2204
2026-01-04 21:25:39,335: t15.2024.05.10 val PER: 0.2288
2026-01-04 21:25:39,335: t15.2024.06.14 val PER: 0.2382
2026-01-04 21:25:39,335: t15.2024.07.19 val PER: 0.3072
2026-01-04 21:25:39,335: t15.2024.07.21 val PER: 0.1614
2026-01-04 21:25:39,335: t15.2024.07.28 val PER: 0.2051
2026-01-04 21:25:39,335: t15.2025.01.10 val PER: 0.3871
2026-01-04 21:25:39,335: t15.2025.01.12 val PER: 0.2325
2026-01-04 21:25:39,335: t15.2025.03.14 val PER: 0.3609
2026-01-04 21:25:39,335: t15.2025.03.16 val PER: 0.2526
2026-01-04 21:25:39,336: t15.2025.03.30 val PER: 0.3575
2026-01-04 21:25:39,336: t15.2025.04.13 val PER: 0.2896
2026-01-04 21:25:39,337: New best val WER(1gram) 60.15% --> 55.08%
2026-01-04 21:25:39,337: Checkpointing model
2026-01-04 21:25:39,972: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:25:40,239: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_5500
2026-01-04 21:25:49,314: Train batch 5600: loss: 19.36 grad norm: 63.05 time: 0.062
2026-01-04 21:26:07,549: Train batch 5800: loss: 13.83 grad norm: 60.51 time: 0.081
2026-01-04 21:26:25,745: Train batch 6000: loss: 14.37 grad norm: 60.74 time: 0.049
2026-01-04 21:26:25,745: Running test after training batch: 6000
2026-01-04 21:26:25,933: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:26:30,775: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 21:26:30,806: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-04 21:26:32,400: Val batch 6000: PER (avg): 0.2100 CTC Loss (avg): 20.7982 WER(1gram): 57.87% (n=64) time: 6.655
2026-01-04 21:26:32,400: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 21:26:32,401: t15.2023.08.13 val PER: 0.1746
2026-01-04 21:26:32,401: t15.2023.08.18 val PER: 0.1618
2026-01-04 21:26:32,401: t15.2023.08.20 val PER: 0.1604
2026-01-04 21:26:32,401: t15.2023.08.25 val PER: 0.1205
2026-01-04 21:26:32,401: t15.2023.08.27 val PER: 0.2524
2026-01-04 21:26:32,401: t15.2023.09.01 val PER: 0.1274
2026-01-04 21:26:32,401: t15.2023.09.03 val PER: 0.2078
2026-01-04 21:26:32,401: t15.2023.09.24 val PER: 0.1675
2026-01-04 21:26:32,401: t15.2023.09.29 val PER: 0.1653
2026-01-04 21:26:32,401: t15.2023.10.01 val PER: 0.2206
2026-01-04 21:26:32,401: t15.2023.10.06 val PER: 0.1378
2026-01-04 21:26:32,402: t15.2023.10.08 val PER: 0.2882
2026-01-04 21:26:32,402: t15.2023.10.13 val PER: 0.2692
2026-01-04 21:26:32,402: t15.2023.10.15 val PER: 0.2096
2026-01-04 21:26:32,402: t15.2023.10.20 val PER: 0.2013
2026-01-04 21:26:32,402: t15.2023.10.22 val PER: 0.1737
2026-01-04 21:26:32,402: t15.2023.11.03 val PER: 0.2164
2026-01-04 21:26:32,402: t15.2023.11.04 val PER: 0.0478
2026-01-04 21:26:32,402: t15.2023.11.17 val PER: 0.0778
2026-01-04 21:26:32,402: t15.2023.11.19 val PER: 0.0778
2026-01-04 21:26:32,402: t15.2023.11.26 val PER: 0.2203
2026-01-04 21:26:32,402: t15.2023.12.03 val PER: 0.1723
2026-01-04 21:26:32,402: t15.2023.12.08 val PER: 0.1764
2026-01-04 21:26:32,402: t15.2023.12.10 val PER: 0.1511
2026-01-04 21:26:32,402: t15.2023.12.17 val PER: 0.2017
2026-01-04 21:26:32,403: t15.2023.12.29 val PER: 0.2196
2026-01-04 21:26:32,403: t15.2024.02.25 val PER: 0.1601
2026-01-04 21:26:32,403: t15.2024.03.08 val PER: 0.2902
2026-01-04 21:26:32,403: t15.2024.03.15 val PER: 0.2670
2026-01-04 21:26:32,403: t15.2024.03.17 val PER: 0.2064
2026-01-04 21:26:32,403: t15.2024.05.10 val PER: 0.2169
2026-01-04 21:26:32,403: t15.2024.06.14 val PER: 0.2129
2026-01-04 21:26:32,403: t15.2024.07.19 val PER: 0.3052
2026-01-04 21:26:32,403: t15.2024.07.21 val PER: 0.1641
2026-01-04 21:26:32,403: t15.2024.07.28 val PER: 0.1919
2026-01-04 21:26:32,404: t15.2025.01.10 val PER: 0.3912
2026-01-04 21:26:32,404: t15.2025.01.12 val PER: 0.2348
2026-01-04 21:26:32,404: t15.2025.03.14 val PER: 0.3728
2026-01-04 21:26:32,404: t15.2025.03.16 val PER: 0.2631
2026-01-04 21:26:32,404: t15.2025.03.30 val PER: 0.3609
2026-01-04 21:26:32,404: t15.2025.04.13 val PER: 0.2596
2026-01-04 21:26:32,659: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_6000
2026-01-04 21:26:50,758: Train batch 6200: loss: 17.07 grad norm: 61.51 time: 0.069
2026-01-04 21:27:08,879: Train batch 6400: loss: 19.17 grad norm: 65.10 time: 0.061
2026-01-04 21:27:17,827: Running test after training batch: 6500
2026-01-04 21:27:18,015: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:27:23,142: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point as will
2026-01-04 21:27:23,170: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 21:27:24,761: Val batch 6500: PER (avg): 0.2032 CTC Loss (avg): 20.0405 WER(1gram): 54.82% (n=64) time: 6.933
2026-01-04 21:27:24,762: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 21:27:24,762: t15.2023.08.13 val PER: 0.1684
2026-01-04 21:27:24,762: t15.2023.08.18 val PER: 0.1366
2026-01-04 21:27:24,762: t15.2023.08.20 val PER: 0.1612
2026-01-04 21:27:24,762: t15.2023.08.25 val PER: 0.1084
2026-01-04 21:27:24,762: t15.2023.08.27 val PER: 0.2186
2026-01-04 21:27:24,762: t15.2023.09.01 val PER: 0.1177
2026-01-04 21:27:24,762: t15.2023.09.03 val PER: 0.1971
2026-01-04 21:27:24,762: t15.2023.09.24 val PER: 0.1723
2026-01-04 21:27:24,762: t15.2023.09.29 val PER: 0.1672
2026-01-04 21:27:24,762: t15.2023.10.01 val PER: 0.2133
2026-01-04 21:27:24,762: t15.2023.10.06 val PER: 0.1292
2026-01-04 21:27:24,763: t15.2023.10.08 val PER: 0.2923
2026-01-04 21:27:24,763: t15.2023.10.13 val PER: 0.2669
2026-01-04 21:27:24,763: t15.2023.10.15 val PER: 0.2182
2026-01-04 21:27:24,763: t15.2023.10.20 val PER: 0.2114
2026-01-04 21:27:24,763: t15.2023.10.22 val PER: 0.1526
2026-01-04 21:27:24,763: t15.2023.11.03 val PER: 0.2198
2026-01-04 21:27:24,763: t15.2023.11.04 val PER: 0.0444
2026-01-04 21:27:24,763: t15.2023.11.17 val PER: 0.0607
2026-01-04 21:27:24,763: t15.2023.11.19 val PER: 0.0679
2026-01-04 21:27:24,763: t15.2023.11.26 val PER: 0.2109
2026-01-04 21:27:24,763: t15.2023.12.03 val PER: 0.1765
2026-01-04 21:27:24,763: t15.2023.12.08 val PER: 0.1658
2026-01-04 21:27:24,764: t15.2023.12.10 val PER: 0.1380
2026-01-04 21:27:24,764: t15.2023.12.17 val PER: 0.1892
2026-01-04 21:27:24,764: t15.2023.12.29 val PER: 0.2011
2026-01-04 21:27:24,764: t15.2024.02.25 val PER: 0.1671
2026-01-04 21:27:24,764: t15.2024.03.08 val PER: 0.2888
2026-01-04 21:27:24,764: t15.2024.03.15 val PER: 0.2502
2026-01-04 21:27:24,764: t15.2024.03.17 val PER: 0.2050
2026-01-04 21:27:24,764: t15.2024.05.10 val PER: 0.2244
2026-01-04 21:27:24,764: t15.2024.06.14 val PER: 0.2224
2026-01-04 21:27:24,764: t15.2024.07.19 val PER: 0.3032
2026-01-04 21:27:24,764: t15.2024.07.21 val PER: 0.1428
2026-01-04 21:27:24,764: t15.2024.07.28 val PER: 0.1912
2026-01-04 21:27:24,764: t15.2025.01.10 val PER: 0.3636
2026-01-04 21:27:24,765: t15.2025.01.12 val PER: 0.2179
2026-01-04 21:27:24,765: t15.2025.03.14 val PER: 0.3772
2026-01-04 21:27:24,765: t15.2025.03.16 val PER: 0.2395
2026-01-04 21:27:24,765: t15.2025.03.30 val PER: 0.3575
2026-01-04 21:27:24,765: t15.2025.04.13 val PER: 0.2810
2026-01-04 21:27:24,766: New best val WER(1gram) 55.08% --> 54.82%
2026-01-04 21:27:24,766: Checkpointing model
2026-01-04 21:27:25,394: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:27:25,664: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_6500
2026-01-04 21:27:34,329: Train batch 6600: loss: 12.59 grad norm: 54.68 time: 0.044
2026-01-04 21:27:52,048: Train batch 6800: loss: 15.42 grad norm: 56.69 time: 0.048
2026-01-04 21:28:09,781: Train batch 7000: loss: 17.23 grad norm: 65.68 time: 0.060
2026-01-04 21:28:09,782: Running test after training batch: 7000
2026-01-04 21:28:09,940: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:28:14,764: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-04 21:28:14,794: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-04 21:28:16,469: Val batch 7000: PER (avg): 0.1942 CTC Loss (avg): 19.1635 WER(1gram): 55.33% (n=64) time: 6.687
2026-01-04 21:28:16,469: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-04 21:28:16,470: t15.2023.08.13 val PER: 0.1611
2026-01-04 21:28:16,470: t15.2023.08.18 val PER: 0.1391
2026-01-04 21:28:16,470: t15.2023.08.20 val PER: 0.1525
2026-01-04 21:28:16,470: t15.2023.08.25 val PER: 0.0994
2026-01-04 21:28:16,470: t15.2023.08.27 val PER: 0.2203
2026-01-04 21:28:16,470: t15.2023.09.01 val PER: 0.1112
2026-01-04 21:28:16,470: t15.2023.09.03 val PER: 0.1971
2026-01-04 21:28:16,470: t15.2023.09.24 val PER: 0.1541
2026-01-04 21:28:16,470: t15.2023.09.29 val PER: 0.1698
2026-01-04 21:28:16,470: t15.2023.10.01 val PER: 0.2133
2026-01-04 21:28:16,470: t15.2023.10.06 val PER: 0.1130
2026-01-04 21:28:16,470: t15.2023.10.08 val PER: 0.2896
2026-01-04 21:28:16,471: t15.2023.10.13 val PER: 0.2630
2026-01-04 21:28:16,471: t15.2023.10.15 val PER: 0.1925
2026-01-04 21:28:16,471: t15.2023.10.20 val PER: 0.2148
2026-01-04 21:28:16,471: t15.2023.10.22 val PER: 0.1403
2026-01-04 21:28:16,471: t15.2023.11.03 val PER: 0.2056
2026-01-04 21:28:16,471: t15.2023.11.04 val PER: 0.0444
2026-01-04 21:28:16,471: t15.2023.11.17 val PER: 0.0653
2026-01-04 21:28:16,471: t15.2023.11.19 val PER: 0.0559
2026-01-04 21:28:16,472: t15.2023.11.26 val PER: 0.1884
2026-01-04 21:28:16,472: t15.2023.12.03 val PER: 0.1660
2026-01-04 21:28:16,472: t15.2023.12.08 val PER: 0.1538
2026-01-04 21:28:16,472: t15.2023.12.10 val PER: 0.1367
2026-01-04 21:28:16,472: t15.2023.12.17 val PER: 0.1840
2026-01-04 21:28:16,472: t15.2023.12.29 val PER: 0.1922
2026-01-04 21:28:16,472: t15.2024.02.25 val PER: 0.1545
2026-01-04 21:28:16,472: t15.2024.03.08 val PER: 0.2703
2026-01-04 21:28:16,472: t15.2024.03.15 val PER: 0.2470
2026-01-04 21:28:16,473: t15.2024.03.17 val PER: 0.1974
2026-01-04 21:28:16,473: t15.2024.05.10 val PER: 0.2021
2026-01-04 21:28:16,473: t15.2024.06.14 val PER: 0.2082
2026-01-04 21:28:16,473: t15.2024.07.19 val PER: 0.3072
2026-01-04 21:28:16,473: t15.2024.07.21 val PER: 0.1324
2026-01-04 21:28:16,473: t15.2024.07.28 val PER: 0.1772
2026-01-04 21:28:16,473: t15.2025.01.10 val PER: 0.3471
2026-01-04 21:28:16,474: t15.2025.01.12 val PER: 0.1986
2026-01-04 21:28:16,474: t15.2025.03.14 val PER: 0.3536
2026-01-04 21:28:16,474: t15.2025.03.16 val PER: 0.2435
2026-01-04 21:28:16,474: t15.2025.03.30 val PER: 0.3483
2026-01-04 21:28:16,474: t15.2025.04.13 val PER: 0.2611
2026-01-04 21:28:16,729: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_7000
2026-01-04 21:28:34,710: Train batch 7200: loss: 14.28 grad norm: 57.39 time: 0.078
2026-01-04 21:28:52,625: Train batch 7400: loss: 13.64 grad norm: 59.17 time: 0.075
2026-01-04 21:29:01,599: Running test after training batch: 7500
2026-01-04 21:29:01,793: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:29:06,715: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point us will
2026-01-04 21:29:06,746: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost nit
2026-01-04 21:29:08,419: Val batch 7500: PER (avg): 0.1880 CTC Loss (avg): 18.6399 WER(1gram): 54.31% (n=64) time: 6.819
2026-01-04 21:29:08,419: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 21:29:08,419: t15.2023.08.13 val PER: 0.1424
2026-01-04 21:29:08,420: t15.2023.08.18 val PER: 0.1350
2026-01-04 21:29:08,420: t15.2023.08.20 val PER: 0.1454
2026-01-04 21:29:08,420: t15.2023.08.25 val PER: 0.0964
2026-01-04 21:29:08,420: t15.2023.08.27 val PER: 0.2058
2026-01-04 21:29:08,420: t15.2023.09.01 val PER: 0.1063
2026-01-04 21:29:08,420: t15.2023.09.03 val PER: 0.1829
2026-01-04 21:29:08,420: t15.2023.09.24 val PER: 0.1578
2026-01-04 21:29:08,420: t15.2023.09.29 val PER: 0.1602
2026-01-04 21:29:08,420: t15.2023.10.01 val PER: 0.2015
2026-01-04 21:29:08,420: t15.2023.10.06 val PER: 0.1087
2026-01-04 21:29:08,420: t15.2023.10.08 val PER: 0.2801
2026-01-04 21:29:08,420: t15.2023.10.13 val PER: 0.2490
2026-01-04 21:29:08,420: t15.2023.10.15 val PER: 0.1872
2026-01-04 21:29:08,421: t15.2023.10.20 val PER: 0.1846
2026-01-04 21:29:08,421: t15.2023.10.22 val PER: 0.1481
2026-01-04 21:29:08,421: t15.2023.11.03 val PER: 0.2076
2026-01-04 21:29:08,421: t15.2023.11.04 val PER: 0.0512
2026-01-04 21:29:08,421: t15.2023.11.17 val PER: 0.0684
2026-01-04 21:29:08,421: t15.2023.11.19 val PER: 0.0559
2026-01-04 21:29:08,421: t15.2023.11.26 val PER: 0.1848
2026-01-04 21:29:08,421: t15.2023.12.03 val PER: 0.1618
2026-01-04 21:29:08,421: t15.2023.12.08 val PER: 0.1558
2026-01-04 21:29:08,421: t15.2023.12.10 val PER: 0.1327
2026-01-04 21:29:08,421: t15.2023.12.17 val PER: 0.1819
2026-01-04 21:29:08,421: t15.2023.12.29 val PER: 0.1798
2026-01-04 21:29:08,421: t15.2024.02.25 val PER: 0.1489
2026-01-04 21:29:08,421: t15.2024.03.08 val PER: 0.2703
2026-01-04 21:29:08,421: t15.2024.03.15 val PER: 0.2445
2026-01-04 21:29:08,421: t15.2024.03.17 val PER: 0.1792
2026-01-04 21:29:08,422: t15.2024.05.10 val PER: 0.1917
2026-01-04 21:29:08,422: t15.2024.06.14 val PER: 0.1987
2026-01-04 21:29:08,422: t15.2024.07.19 val PER: 0.2920
2026-01-04 21:29:08,422: t15.2024.07.21 val PER: 0.1393
2026-01-04 21:29:08,422: t15.2024.07.28 val PER: 0.1647
2026-01-04 21:29:08,422: t15.2025.01.10 val PER: 0.3388
2026-01-04 21:29:08,422: t15.2025.01.12 val PER: 0.1871
2026-01-04 21:29:08,422: t15.2025.03.14 val PER: 0.3639
2026-01-04 21:29:08,423: t15.2025.03.16 val PER: 0.2395
2026-01-04 21:29:08,423: t15.2025.03.30 val PER: 0.3529
2026-01-04 21:29:08,423: t15.2025.04.13 val PER: 0.2439
2026-01-04 21:29:08,424: New best val WER(1gram) 54.82% --> 54.31%
2026-01-04 21:29:08,424: Checkpointing model
2026-01-04 21:29:09,071: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:29:09,336: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_7500
2026-01-04 21:29:18,256: Train batch 7600: loss: 16.02 grad norm: 60.27 time: 0.068
2026-01-04 21:29:36,329: Train batch 7800: loss: 14.05 grad norm: 57.70 time: 0.055
2026-01-04 21:29:54,723: Train batch 8000: loss: 11.13 grad norm: 51.51 time: 0.071
2026-01-04 21:29:54,724: Running test after training batch: 8000
2026-01-04 21:29:54,829: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:29:59,663: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 21:29:59,694: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost nit
2026-01-04 21:30:01,401: Val batch 8000: PER (avg): 0.1843 CTC Loss (avg): 18.2083 WER(1gram): 55.84% (n=64) time: 6.677
2026-01-04 21:30:01,401: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 21:30:01,401: t15.2023.08.13 val PER: 0.1518
2026-01-04 21:30:01,401: t15.2023.08.18 val PER: 0.1257
2026-01-04 21:30:01,401: t15.2023.08.20 val PER: 0.1406
2026-01-04 21:30:01,402: t15.2023.08.25 val PER: 0.1084
2026-01-04 21:30:01,402: t15.2023.08.27 val PER: 0.2106
2026-01-04 21:30:01,402: t15.2023.09.01 val PER: 0.1015
2026-01-04 21:30:01,402: t15.2023.09.03 val PER: 0.1900
2026-01-04 21:30:01,402: t15.2023.09.24 val PER: 0.1420
2026-01-04 21:30:01,402: t15.2023.09.29 val PER: 0.1506
2026-01-04 21:30:01,402: t15.2023.10.01 val PER: 0.2028
2026-01-04 21:30:01,402: t15.2023.10.06 val PER: 0.1141
2026-01-04 21:30:01,402: t15.2023.10.08 val PER: 0.2747
2026-01-04 21:30:01,402: t15.2023.10.13 val PER: 0.2467
2026-01-04 21:30:01,402: t15.2023.10.15 val PER: 0.1898
2026-01-04 21:30:01,402: t15.2023.10.20 val PER: 0.2013
2026-01-04 21:30:01,402: t15.2023.10.22 val PER: 0.1392
2026-01-04 21:30:01,402: t15.2023.11.03 val PER: 0.1974
2026-01-04 21:30:01,403: t15.2023.11.04 val PER: 0.0375
2026-01-04 21:30:01,403: t15.2023.11.17 val PER: 0.0638
2026-01-04 21:30:01,403: t15.2023.11.19 val PER: 0.0619
2026-01-04 21:30:01,403: t15.2023.11.26 val PER: 0.1783
2026-01-04 21:30:01,403: t15.2023.12.03 val PER: 0.1576
2026-01-04 21:30:01,403: t15.2023.12.08 val PER: 0.1485
2026-01-04 21:30:01,403: t15.2023.12.10 val PER: 0.1248
2026-01-04 21:30:01,403: t15.2023.12.17 val PER: 0.1757
2026-01-04 21:30:01,403: t15.2023.12.29 val PER: 0.1771
2026-01-04 21:30:01,403: t15.2024.02.25 val PER: 0.1376
2026-01-04 21:30:01,403: t15.2024.03.08 val PER: 0.2731
2026-01-04 21:30:01,403: t15.2024.03.15 val PER: 0.2345
2026-01-04 21:30:01,403: t15.2024.03.17 val PER: 0.1764
2026-01-04 21:30:01,403: t15.2024.05.10 val PER: 0.1991
2026-01-04 21:30:01,403: t15.2024.06.14 val PER: 0.2035
2026-01-04 21:30:01,403: t15.2024.07.19 val PER: 0.2821
2026-01-04 21:30:01,404: t15.2024.07.21 val PER: 0.1234
2026-01-04 21:30:01,404: t15.2024.07.28 val PER: 0.1566
2026-01-04 21:30:01,404: t15.2025.01.10 val PER: 0.3333
2026-01-04 21:30:01,404: t15.2025.01.12 val PER: 0.1917
2026-01-04 21:30:01,404: t15.2025.03.14 val PER: 0.3550
2026-01-04 21:30:01,404: t15.2025.03.16 val PER: 0.2304
2026-01-04 21:30:01,404: t15.2025.03.30 val PER: 0.3494
2026-01-04 21:30:01,404: t15.2025.04.13 val PER: 0.2568
2026-01-04 21:30:01,660: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_8000
2026-01-04 21:30:19,068: Train batch 8200: loss: 9.98 grad norm: 51.75 time: 0.054
2026-01-04 21:30:36,683: Train batch 8400: loss: 9.73 grad norm: 46.42 time: 0.063
2026-01-04 21:30:45,657: Running test after training batch: 8500
2026-01-04 21:30:45,770: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:30:50,699: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 21:30:50,729: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-04 21:30:52,416: Val batch 8500: PER (avg): 0.1783 CTC Loss (avg): 17.6170 WER(1gram): 48.73% (n=64) time: 6.759
2026-01-04 21:30:52,417: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 21:30:52,417: t15.2023.08.13 val PER: 0.1424
2026-01-04 21:30:52,417: t15.2023.08.18 val PER: 0.1375
2026-01-04 21:30:52,417: t15.2023.08.20 val PER: 0.1326
2026-01-04 21:30:52,417: t15.2023.08.25 val PER: 0.1099
2026-01-04 21:30:52,417: t15.2023.08.27 val PER: 0.2138
2026-01-04 21:30:52,418: t15.2023.09.01 val PER: 0.0966
2026-01-04 21:30:52,418: t15.2023.09.03 val PER: 0.1829
2026-01-04 21:30:52,418: t15.2023.09.24 val PER: 0.1517
2026-01-04 21:30:52,418: t15.2023.09.29 val PER: 0.1544
2026-01-04 21:30:52,418: t15.2023.10.01 val PER: 0.1922
2026-01-04 21:30:52,418: t15.2023.10.06 val PER: 0.1076
2026-01-04 21:30:52,419: t15.2023.10.08 val PER: 0.2666
2026-01-04 21:30:52,419: t15.2023.10.13 val PER: 0.2366
2026-01-04 21:30:52,419: t15.2023.10.15 val PER: 0.1740
2026-01-04 21:30:52,419: t15.2023.10.20 val PER: 0.2013
2026-01-04 21:30:52,419: t15.2023.10.22 val PER: 0.1359
2026-01-04 21:30:52,419: t15.2023.11.03 val PER: 0.1981
2026-01-04 21:30:52,419: t15.2023.11.04 val PER: 0.0512
2026-01-04 21:30:52,419: t15.2023.11.17 val PER: 0.0529
2026-01-04 21:30:52,420: t15.2023.11.19 val PER: 0.0519
2026-01-04 21:30:52,420: t15.2023.11.26 val PER: 0.1732
2026-01-04 21:30:52,420: t15.2023.12.03 val PER: 0.1513
2026-01-04 21:30:52,420: t15.2023.12.08 val PER: 0.1385
2026-01-04 21:30:52,420: t15.2023.12.10 val PER: 0.1130
2026-01-04 21:30:52,420: t15.2023.12.17 val PER: 0.1684
2026-01-04 21:30:52,420: t15.2023.12.29 val PER: 0.1764
2026-01-04 21:30:52,420: t15.2024.02.25 val PER: 0.1461
2026-01-04 21:30:52,420: t15.2024.03.08 val PER: 0.2632
2026-01-04 21:30:52,420: t15.2024.03.15 val PER: 0.2308
2026-01-04 21:30:52,420: t15.2024.03.17 val PER: 0.1667
2026-01-04 21:30:52,420: t15.2024.05.10 val PER: 0.1813
2026-01-04 21:30:52,420: t15.2024.06.14 val PER: 0.1877
2026-01-04 21:30:52,421: t15.2024.07.19 val PER: 0.2643
2026-01-04 21:30:52,421: t15.2024.07.21 val PER: 0.1193
2026-01-04 21:30:52,421: t15.2024.07.28 val PER: 0.1684
2026-01-04 21:30:52,421: t15.2025.01.10 val PER: 0.3209
2026-01-04 21:30:52,421: t15.2025.01.12 val PER: 0.1817
2026-01-04 21:30:52,421: t15.2025.03.14 val PER: 0.3491
2026-01-04 21:30:52,421: t15.2025.03.16 val PER: 0.2251
2026-01-04 21:30:52,421: t15.2025.03.30 val PER: 0.3207
2026-01-04 21:30:52,421: t15.2025.04.13 val PER: 0.2382
2026-01-04 21:30:52,422: New best val WER(1gram) 54.31% --> 48.73%
2026-01-04 21:30:52,422: Checkpointing model
2026-01-04 21:30:53,025: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:30:53,294: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_8500
2026-01-04 21:31:02,441: Train batch 8600: loss: 15.56 grad norm: 59.37 time: 0.054
2026-01-04 21:31:20,362: Train batch 8800: loss: 15.33 grad norm: 56.93 time: 0.060
2026-01-04 21:31:38,718: Train batch 9000: loss: 16.14 grad norm: 65.89 time: 0.072
2026-01-04 21:31:38,718: Running test after training batch: 9000
2026-01-04 21:31:38,839: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:31:43,659: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 21:31:43,690: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-04 21:31:45,400: Val batch 9000: PER (avg): 0.1745 CTC Loss (avg): 17.3057 WER(1gram): 50.00% (n=64) time: 6.681
2026-01-04 21:31:45,400: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 21:31:45,400: t15.2023.08.13 val PER: 0.1289
2026-01-04 21:31:45,400: t15.2023.08.18 val PER: 0.1215
2026-01-04 21:31:45,401: t15.2023.08.20 val PER: 0.1319
2026-01-04 21:31:45,401: t15.2023.08.25 val PER: 0.0949
2026-01-04 21:31:45,401: t15.2023.08.27 val PER: 0.2074
2026-01-04 21:31:45,401: t15.2023.09.01 val PER: 0.0982
2026-01-04 21:31:45,401: t15.2023.09.03 val PER: 0.1781
2026-01-04 21:31:45,401: t15.2023.09.24 val PER: 0.1420
2026-01-04 21:31:45,401: t15.2023.09.29 val PER: 0.1429
2026-01-04 21:31:45,401: t15.2023.10.01 val PER: 0.1915
2026-01-04 21:31:45,401: t15.2023.10.06 val PER: 0.1076
2026-01-04 21:31:45,401: t15.2023.10.08 val PER: 0.2720
2026-01-04 21:31:45,402: t15.2023.10.13 val PER: 0.2358
2026-01-04 21:31:45,402: t15.2023.10.15 val PER: 0.1688
2026-01-04 21:31:45,402: t15.2023.10.20 val PER: 0.2148
2026-01-04 21:31:45,402: t15.2023.10.22 val PER: 0.1325
2026-01-04 21:31:45,402: t15.2023.11.03 val PER: 0.2035
2026-01-04 21:31:45,402: t15.2023.11.04 val PER: 0.0444
2026-01-04 21:31:45,402: t15.2023.11.17 val PER: 0.0653
2026-01-04 21:31:45,402: t15.2023.11.19 val PER: 0.0459
2026-01-04 21:31:45,402: t15.2023.11.26 val PER: 0.1783
2026-01-04 21:31:45,402: t15.2023.12.03 val PER: 0.1408
2026-01-04 21:31:45,402: t15.2023.12.08 val PER: 0.1318
2026-01-04 21:31:45,402: t15.2023.12.10 val PER: 0.1183
2026-01-04 21:31:45,402: t15.2023.12.17 val PER: 0.1674
2026-01-04 21:31:45,402: t15.2023.12.29 val PER: 0.1668
2026-01-04 21:31:45,403: t15.2024.02.25 val PER: 0.1419
2026-01-04 21:31:45,403: t15.2024.03.08 val PER: 0.2546
2026-01-04 21:31:45,403: t15.2024.03.15 val PER: 0.2233
2026-01-04 21:31:45,403: t15.2024.03.17 val PER: 0.1681
2026-01-04 21:31:45,403: t15.2024.05.10 val PER: 0.1813
2026-01-04 21:31:45,403: t15.2024.06.14 val PER: 0.1798
2026-01-04 21:31:45,403: t15.2024.07.19 val PER: 0.2657
2026-01-04 21:31:45,403: t15.2024.07.21 val PER: 0.1131
2026-01-04 21:31:45,403: t15.2024.07.28 val PER: 0.1551
2026-01-04 21:31:45,403: t15.2025.01.10 val PER: 0.3154
2026-01-04 21:31:45,403: t15.2025.01.12 val PER: 0.1678
2026-01-04 21:31:45,403: t15.2025.03.14 val PER: 0.3565
2026-01-04 21:31:45,403: t15.2025.03.16 val PER: 0.2277
2026-01-04 21:31:45,403: t15.2025.03.30 val PER: 0.3195
2026-01-04 21:31:45,403: t15.2025.04.13 val PER: 0.2382
2026-01-04 21:31:45,663: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_9000
2026-01-04 21:32:03,722: Train batch 9200: loss: 10.99 grad norm: 50.97 time: 0.056
2026-01-04 21:32:21,688: Train batch 9400: loss: 7.27 grad norm: 43.97 time: 0.067
2026-01-04 21:32:30,831: Running test after training batch: 9500
2026-01-04 21:32:30,981: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:32:35,951: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 21:32:35,981: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-04 21:32:37,659: Val batch 9500: PER (avg): 0.1742 CTC Loss (avg): 17.3109 WER(1gram): 48.73% (n=64) time: 6.827
2026-01-04 21:32:37,659: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 21:32:37,660: t15.2023.08.13 val PER: 0.1268
2026-01-04 21:32:37,660: t15.2023.08.18 val PER: 0.1232
2026-01-04 21:32:37,660: t15.2023.08.20 val PER: 0.1279
2026-01-04 21:32:37,660: t15.2023.08.25 val PER: 0.1009
2026-01-04 21:32:37,660: t15.2023.08.27 val PER: 0.1945
2026-01-04 21:32:37,660: t15.2023.09.01 val PER: 0.0877
2026-01-04 21:32:37,660: t15.2023.09.03 val PER: 0.1758
2026-01-04 21:32:37,660: t15.2023.09.24 val PER: 0.1396
2026-01-04 21:32:37,660: t15.2023.09.29 val PER: 0.1468
2026-01-04 21:32:37,660: t15.2023.10.01 val PER: 0.1889
2026-01-04 21:32:37,660: t15.2023.10.06 val PER: 0.1066
2026-01-04 21:32:37,660: t15.2023.10.08 val PER: 0.2571
2026-01-04 21:32:37,661: t15.2023.10.13 val PER: 0.2281
2026-01-04 21:32:37,661: t15.2023.10.15 val PER: 0.1905
2026-01-04 21:32:37,661: t15.2023.10.20 val PER: 0.2013
2026-01-04 21:32:37,661: t15.2023.10.22 val PER: 0.1370
2026-01-04 21:32:37,661: t15.2023.11.03 val PER: 0.1974
2026-01-04 21:32:37,661: t15.2023.11.04 val PER: 0.0375
2026-01-04 21:32:37,661: t15.2023.11.17 val PER: 0.0513
2026-01-04 21:32:37,661: t15.2023.11.19 val PER: 0.0559
2026-01-04 21:32:37,661: t15.2023.11.26 val PER: 0.1703
2026-01-04 21:32:37,661: t15.2023.12.03 val PER: 0.1429
2026-01-04 21:32:37,661: t15.2023.12.08 val PER: 0.1372
2026-01-04 21:32:37,661: t15.2023.12.10 val PER: 0.1143
2026-01-04 21:32:37,661: t15.2023.12.17 val PER: 0.1507
2026-01-04 21:32:37,661: t15.2023.12.29 val PER: 0.1537
2026-01-04 21:32:37,661: t15.2024.02.25 val PER: 0.1292
2026-01-04 21:32:37,661: t15.2024.03.08 val PER: 0.2589
2026-01-04 21:32:37,661: t15.2024.03.15 val PER: 0.2326
2026-01-04 21:32:37,661: t15.2024.03.17 val PER: 0.1695
2026-01-04 21:32:37,662: t15.2024.05.10 val PER: 0.1813
2026-01-04 21:32:37,662: t15.2024.06.14 val PER: 0.1830
2026-01-04 21:32:37,662: t15.2024.07.19 val PER: 0.2716
2026-01-04 21:32:37,662: t15.2024.07.21 val PER: 0.1200
2026-01-04 21:32:37,662: t15.2024.07.28 val PER: 0.1574
2026-01-04 21:32:37,662: t15.2025.01.10 val PER: 0.3099
2026-01-04 21:32:37,662: t15.2025.01.12 val PER: 0.1817
2026-01-04 21:32:37,662: t15.2025.03.14 val PER: 0.3654
2026-01-04 21:32:37,662: t15.2025.03.16 val PER: 0.2212
2026-01-04 21:32:37,662: t15.2025.03.30 val PER: 0.3287
2026-01-04 21:32:37,662: t15.2025.04.13 val PER: 0.2340
2026-01-04 21:32:37,920: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_9500
2026-01-04 21:32:46,651: Train batch 9600: loss: 8.39 grad norm: 48.88 time: 0.074
2026-01-04 21:33:04,243: Train batch 9800: loss: 12.07 grad norm: 55.90 time: 0.063
2026-01-04 21:33:22,554: Train batch 10000: loss: 5.19 grad norm: 36.16 time: 0.061
2026-01-04 21:33:22,554: Running test after training batch: 10000
2026-01-04 21:33:22,657: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:33:27,590: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:33:27,622: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 21:33:29,331: Val batch 10000: PER (avg): 0.1692 CTC Loss (avg): 16.8568 WER(1gram): 50.25% (n=64) time: 6.776
2026-01-04 21:33:29,331: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 21:33:29,331: t15.2023.08.13 val PER: 0.1289
2026-01-04 21:33:29,331: t15.2023.08.18 val PER: 0.1282
2026-01-04 21:33:29,332: t15.2023.08.20 val PER: 0.1239
2026-01-04 21:33:29,332: t15.2023.08.25 val PER: 0.1099
2026-01-04 21:33:29,335: t15.2023.08.27 val PER: 0.2026
2026-01-04 21:33:29,336: t15.2023.09.01 val PER: 0.0966
2026-01-04 21:33:29,336: t15.2023.09.03 val PER: 0.1781
2026-01-04 21:33:29,336: t15.2023.09.24 val PER: 0.1456
2026-01-04 21:33:29,336: t15.2023.09.29 val PER: 0.1449
2026-01-04 21:33:29,336: t15.2023.10.01 val PER: 0.1856
2026-01-04 21:33:29,336: t15.2023.10.06 val PER: 0.1109
2026-01-04 21:33:29,336: t15.2023.10.08 val PER: 0.2530
2026-01-04 21:33:29,336: t15.2023.10.13 val PER: 0.2180
2026-01-04 21:33:29,336: t15.2023.10.15 val PER: 0.1694
2026-01-04 21:33:29,336: t15.2023.10.20 val PER: 0.2047
2026-01-04 21:33:29,336: t15.2023.10.22 val PER: 0.1269
2026-01-04 21:33:29,337: t15.2023.11.03 val PER: 0.1900
2026-01-04 21:33:29,337: t15.2023.11.04 val PER: 0.0341
2026-01-04 21:33:29,337: t15.2023.11.17 val PER: 0.0513
2026-01-04 21:33:29,337: t15.2023.11.19 val PER: 0.0399
2026-01-04 21:33:29,337: t15.2023.11.26 val PER: 0.1522
2026-01-04 21:33:29,337: t15.2023.12.03 val PER: 0.1345
2026-01-04 21:33:29,337: t15.2023.12.08 val PER: 0.1245
2026-01-04 21:33:29,337: t15.2023.12.10 val PER: 0.1170
2026-01-04 21:33:29,337: t15.2023.12.17 val PER: 0.1538
2026-01-04 21:33:29,337: t15.2023.12.29 val PER: 0.1407
2026-01-04 21:33:29,337: t15.2024.02.25 val PER: 0.1390
2026-01-04 21:33:29,337: t15.2024.03.08 val PER: 0.2532
2026-01-04 21:33:29,338: t15.2024.03.15 val PER: 0.2126
2026-01-04 21:33:29,338: t15.2024.03.17 val PER: 0.1576
2026-01-04 21:33:29,338: t15.2024.05.10 val PER: 0.1724
2026-01-04 21:33:29,338: t15.2024.06.14 val PER: 0.1767
2026-01-04 21:33:29,338: t15.2024.07.19 val PER: 0.2696
2026-01-04 21:33:29,338: t15.2024.07.21 val PER: 0.1145
2026-01-04 21:33:29,338: t15.2024.07.28 val PER: 0.1544
2026-01-04 21:33:29,338: t15.2025.01.10 val PER: 0.3085
2026-01-04 21:33:29,338: t15.2025.01.12 val PER: 0.1755
2026-01-04 21:33:29,338: t15.2025.03.14 val PER: 0.3550
2026-01-04 21:33:29,338: t15.2025.03.16 val PER: 0.2199
2026-01-04 21:33:29,338: t15.2025.03.30 val PER: 0.3241
2026-01-04 21:33:29,338: t15.2025.04.13 val PER: 0.2325
2026-01-04 21:33:29,599: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_10000
2026-01-04 21:33:47,360: Train batch 10200: loss: 6.42 grad norm: 38.66 time: 0.049
2026-01-04 21:34:05,116: Train batch 10400: loss: 9.46 grad norm: 49.71 time: 0.072
2026-01-04 21:34:14,060: Running test after training batch: 10500
2026-01-04 21:34:14,201: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:34:19,021: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:34:19,052: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-04 21:34:20,791: Val batch 10500: PER (avg): 0.1663 CTC Loss (avg): 16.6586 WER(1gram): 50.51% (n=64) time: 6.730
2026-01-04 21:34:20,791: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-04 21:34:20,791: t15.2023.08.13 val PER: 0.1279
2026-01-04 21:34:20,791: t15.2023.08.18 val PER: 0.1199
2026-01-04 21:34:20,791: t15.2023.08.20 val PER: 0.1183
2026-01-04 21:34:20,791: t15.2023.08.25 val PER: 0.1054
2026-01-04 21:34:20,791: t15.2023.08.27 val PER: 0.1913
2026-01-04 21:34:20,791: t15.2023.09.01 val PER: 0.0950
2026-01-04 21:34:20,792: t15.2023.09.03 val PER: 0.1722
2026-01-04 21:34:20,792: t15.2023.09.24 val PER: 0.1481
2026-01-04 21:34:20,792: t15.2023.09.29 val PER: 0.1493
2026-01-04 21:34:20,792: t15.2023.10.01 val PER: 0.1882
2026-01-04 21:34:20,792: t15.2023.10.06 val PER: 0.0904
2026-01-04 21:34:20,792: t15.2023.10.08 val PER: 0.2449
2026-01-04 21:34:20,792: t15.2023.10.13 val PER: 0.2118
2026-01-04 21:34:20,792: t15.2023.10.15 val PER: 0.1806
2026-01-04 21:34:20,792: t15.2023.10.20 val PER: 0.2013
2026-01-04 21:34:20,792: t15.2023.10.22 val PER: 0.1192
2026-01-04 21:34:20,792: t15.2023.11.03 val PER: 0.1961
2026-01-04 21:34:20,792: t15.2023.11.04 val PER: 0.0307
2026-01-04 21:34:20,792: t15.2023.11.17 val PER: 0.0529
2026-01-04 21:34:20,792: t15.2023.11.19 val PER: 0.0539
2026-01-04 21:34:20,792: t15.2023.11.26 val PER: 0.1384
2026-01-04 21:34:20,793: t15.2023.12.03 val PER: 0.1271
2026-01-04 21:34:20,793: t15.2023.12.08 val PER: 0.1245
2026-01-04 21:34:20,793: t15.2023.12.10 val PER: 0.1038
2026-01-04 21:34:20,793: t15.2023.12.17 val PER: 0.1383
2026-01-04 21:34:20,793: t15.2023.12.29 val PER: 0.1551
2026-01-04 21:34:20,793: t15.2024.02.25 val PER: 0.1222
2026-01-04 21:34:20,793: t15.2024.03.08 val PER: 0.2432
2026-01-04 21:34:20,793: t15.2024.03.15 val PER: 0.2214
2026-01-04 21:34:20,793: t15.2024.03.17 val PER: 0.1583
2026-01-04 21:34:20,793: t15.2024.05.10 val PER: 0.1724
2026-01-04 21:34:20,793: t15.2024.06.14 val PER: 0.1861
2026-01-04 21:34:20,793: t15.2024.07.19 val PER: 0.2617
2026-01-04 21:34:20,793: t15.2024.07.21 val PER: 0.1034
2026-01-04 21:34:20,793: t15.2024.07.28 val PER: 0.1419
2026-01-04 21:34:20,793: t15.2025.01.10 val PER: 0.3182
2026-01-04 21:34:20,793: t15.2025.01.12 val PER: 0.1740
2026-01-04 21:34:20,793: t15.2025.03.14 val PER: 0.3639
2026-01-04 21:34:20,793: t15.2025.03.16 val PER: 0.2120
2026-01-04 21:34:20,794: t15.2025.03.30 val PER: 0.3080
2026-01-04 21:34:20,794: t15.2025.04.13 val PER: 0.2240
2026-01-04 21:34:21,065: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_10500
2026-01-04 21:34:30,204: Train batch 10600: loss: 9.42 grad norm: 58.67 time: 0.072
2026-01-04 21:34:48,313: Train batch 10800: loss: 14.96 grad norm: 66.10 time: 0.064
2026-01-04 21:35:06,509: Train batch 11000: loss: 14.41 grad norm: 62.19 time: 0.056
2026-01-04 21:35:06,509: Running test after training batch: 11000
2026-01-04 21:35:06,665: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:35:11,475: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:35:11,505: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 21:35:13,243: Val batch 11000: PER (avg): 0.1632 CTC Loss (avg): 16.4090 WER(1gram): 47.72% (n=64) time: 6.734
2026-01-04 21:35:13,244: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=12
2026-01-04 21:35:13,244: t15.2023.08.13 val PER: 0.1195
2026-01-04 21:35:13,244: t15.2023.08.18 val PER: 0.1207
2026-01-04 21:35:13,244: t15.2023.08.20 val PER: 0.1168
2026-01-04 21:35:13,244: t15.2023.08.25 val PER: 0.1009
2026-01-04 21:35:13,245: t15.2023.08.27 val PER: 0.2026
2026-01-04 21:35:13,245: t15.2023.09.01 val PER: 0.0917
2026-01-04 21:35:13,245: t15.2023.09.03 val PER: 0.1758
2026-01-04 21:35:13,245: t15.2023.09.24 val PER: 0.1456
2026-01-04 21:35:13,245: t15.2023.09.29 val PER: 0.1385
2026-01-04 21:35:13,245: t15.2023.10.01 val PER: 0.1889
2026-01-04 21:35:13,245: t15.2023.10.06 val PER: 0.0904
2026-01-04 21:35:13,245: t15.2023.10.08 val PER: 0.2517
2026-01-04 21:35:13,245: t15.2023.10.13 val PER: 0.2133
2026-01-04 21:35:13,246: t15.2023.10.15 val PER: 0.1668
2026-01-04 21:35:13,246: t15.2023.10.20 val PER: 0.1980
2026-01-04 21:35:13,246: t15.2023.10.22 val PER: 0.1192
2026-01-04 21:35:13,246: t15.2023.11.03 val PER: 0.1920
2026-01-04 21:35:13,246: t15.2023.11.04 val PER: 0.0410
2026-01-04 21:35:13,246: t15.2023.11.17 val PER: 0.0467
2026-01-04 21:35:13,246: t15.2023.11.19 val PER: 0.0439
2026-01-04 21:35:13,246: t15.2023.11.26 val PER: 0.1464
2026-01-04 21:35:13,246: t15.2023.12.03 val PER: 0.1303
2026-01-04 21:35:13,246: t15.2023.12.08 val PER: 0.1172
2026-01-04 21:35:13,246: t15.2023.12.10 val PER: 0.0999
2026-01-04 21:35:13,246: t15.2023.12.17 val PER: 0.1570
2026-01-04 21:35:13,246: t15.2023.12.29 val PER: 0.1386
2026-01-04 21:35:13,246: t15.2024.02.25 val PER: 0.1194
2026-01-04 21:35:13,246: t15.2024.03.08 val PER: 0.2333
2026-01-04 21:35:13,247: t15.2024.03.15 val PER: 0.2226
2026-01-04 21:35:13,247: t15.2024.03.17 val PER: 0.1520
2026-01-04 21:35:13,247: t15.2024.05.10 val PER: 0.1649
2026-01-04 21:35:13,247: t15.2024.06.14 val PER: 0.1703
2026-01-04 21:35:13,247: t15.2024.07.19 val PER: 0.2525
2026-01-04 21:35:13,247: t15.2024.07.21 val PER: 0.1014
2026-01-04 21:35:13,247: t15.2024.07.28 val PER: 0.1419
2026-01-04 21:35:13,247: t15.2025.01.10 val PER: 0.3017
2026-01-04 21:35:13,247: t15.2025.01.12 val PER: 0.1655
2026-01-04 21:35:13,247: t15.2025.03.14 val PER: 0.3609
2026-01-04 21:35:13,247: t15.2025.03.16 val PER: 0.2016
2026-01-04 21:35:13,247: t15.2025.03.30 val PER: 0.3080
2026-01-04 21:35:13,247: t15.2025.04.13 val PER: 0.2354
2026-01-04 21:35:13,249: New best val WER(1gram) 48.73% --> 47.72%
2026-01-04 21:35:13,249: Checkpointing model
2026-01-04 21:35:13,879: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:35:14,149: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_11000
2026-01-04 21:35:32,261: Train batch 11200: loss: 10.85 grad norm: 54.77 time: 0.071
2026-01-04 21:35:50,485: Train batch 11400: loss: 9.66 grad norm: 52.80 time: 0.056
2026-01-04 21:35:59,571: Running test after training batch: 11500
2026-01-04 21:35:59,747: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:36:04,582: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:36:04,612: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 21:36:06,368: Val batch 11500: PER (avg): 0.1606 CTC Loss (avg): 16.3623 WER(1gram): 50.00% (n=64) time: 6.796
2026-01-04 21:36:06,368: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 21:36:06,368: t15.2023.08.13 val PER: 0.1195
2026-01-04 21:36:06,368: t15.2023.08.18 val PER: 0.1115
2026-01-04 21:36:06,368: t15.2023.08.20 val PER: 0.1183
2026-01-04 21:36:06,368: t15.2023.08.25 val PER: 0.1009
2026-01-04 21:36:06,369: t15.2023.08.27 val PER: 0.1977
2026-01-04 21:36:06,369: t15.2023.09.01 val PER: 0.0885
2026-01-04 21:36:06,369: t15.2023.09.03 val PER: 0.1663
2026-01-04 21:36:06,369: t15.2023.09.24 val PER: 0.1226
2026-01-04 21:36:06,369: t15.2023.09.29 val PER: 0.1347
2026-01-04 21:36:06,369: t15.2023.10.01 val PER: 0.1830
2026-01-04 21:36:06,369: t15.2023.10.06 val PER: 0.0872
2026-01-04 21:36:06,369: t15.2023.10.08 val PER: 0.2544
2026-01-04 21:36:06,369: t15.2023.10.13 val PER: 0.2133
2026-01-04 21:36:06,369: t15.2023.10.15 val PER: 0.1681
2026-01-04 21:36:06,369: t15.2023.10.20 val PER: 0.1946
2026-01-04 21:36:06,369: t15.2023.10.22 val PER: 0.1169
2026-01-04 21:36:06,369: t15.2023.11.03 val PER: 0.1886
2026-01-04 21:36:06,369: t15.2023.11.04 val PER: 0.0307
2026-01-04 21:36:06,369: t15.2023.11.17 val PER: 0.0529
2026-01-04 21:36:06,370: t15.2023.11.19 val PER: 0.0439
2026-01-04 21:36:06,370: t15.2023.11.26 val PER: 0.1384
2026-01-04 21:36:06,370: t15.2023.12.03 val PER: 0.1292
2026-01-04 21:36:06,370: t15.2023.12.08 val PER: 0.1105
2026-01-04 21:36:06,370: t15.2023.12.10 val PER: 0.0959
2026-01-04 21:36:06,370: t15.2023.12.17 val PER: 0.1320
2026-01-04 21:36:06,370: t15.2023.12.29 val PER: 0.1338
2026-01-04 21:36:06,370: t15.2024.02.25 val PER: 0.1208
2026-01-04 21:36:06,370: t15.2024.03.08 val PER: 0.2248
2026-01-04 21:36:06,370: t15.2024.03.15 val PER: 0.2108
2026-01-04 21:36:06,370: t15.2024.03.17 val PER: 0.1527
2026-01-04 21:36:06,370: t15.2024.05.10 val PER: 0.1753
2026-01-04 21:36:06,370: t15.2024.06.14 val PER: 0.1672
2026-01-04 21:36:06,370: t15.2024.07.19 val PER: 0.2591
2026-01-04 21:36:06,370: t15.2024.07.21 val PER: 0.1062
2026-01-04 21:36:06,370: t15.2024.07.28 val PER: 0.1434
2026-01-04 21:36:06,370: t15.2025.01.10 val PER: 0.3127
2026-01-04 21:36:06,371: t15.2025.01.12 val PER: 0.1624
2026-01-04 21:36:06,371: t15.2025.03.14 val PER: 0.3506
2026-01-04 21:36:06,371: t15.2025.03.16 val PER: 0.2173
2026-01-04 21:36:06,371: t15.2025.03.30 val PER: 0.3161
2026-01-04 21:36:06,371: t15.2025.04.13 val PER: 0.2268
2026-01-04 21:36:06,632: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_11500
2026-01-04 21:36:15,400: Train batch 11600: loss: 10.73 grad norm: 48.33 time: 0.060
2026-01-04 21:36:33,444: Train batch 11800: loss: 7.08 grad norm: 42.38 time: 0.044
2026-01-04 21:36:51,566: Train batch 12000: loss: 13.70 grad norm: 53.62 time: 0.072
2026-01-04 21:36:51,567: Running test after training batch: 12000
2026-01-04 21:36:51,667: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:36:56,483: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:36:56,515: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-04 21:36:58,279: Val batch 12000: PER (avg): 0.1597 CTC Loss (avg): 16.1538 WER(1gram): 48.73% (n=64) time: 6.712
2026-01-04 21:36:58,279: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 21:36:58,279: t15.2023.08.13 val PER: 0.1154
2026-01-04 21:36:58,279: t15.2023.08.18 val PER: 0.1081
2026-01-04 21:36:58,279: t15.2023.08.20 val PER: 0.1199
2026-01-04 21:36:58,280: t15.2023.08.25 val PER: 0.0919
2026-01-04 21:36:58,280: t15.2023.08.27 val PER: 0.1849
2026-01-04 21:36:58,280: t15.2023.09.01 val PER: 0.0893
2026-01-04 21:36:58,280: t15.2023.09.03 val PER: 0.1603
2026-01-04 21:36:58,280: t15.2023.09.24 val PER: 0.1335
2026-01-04 21:36:58,280: t15.2023.09.29 val PER: 0.1372
2026-01-04 21:36:58,280: t15.2023.10.01 val PER: 0.1777
2026-01-04 21:36:58,280: t15.2023.10.06 val PER: 0.0980
2026-01-04 21:36:58,280: t15.2023.10.08 val PER: 0.2517
2026-01-04 21:36:58,280: t15.2023.10.13 val PER: 0.2180
2026-01-04 21:36:58,280: t15.2023.10.15 val PER: 0.1661
2026-01-04 21:36:58,280: t15.2023.10.20 val PER: 0.2013
2026-01-04 21:36:58,280: t15.2023.10.22 val PER: 0.1281
2026-01-04 21:36:58,280: t15.2023.11.03 val PER: 0.1879
2026-01-04 21:36:58,280: t15.2023.11.04 val PER: 0.0375
2026-01-04 21:36:58,281: t15.2023.11.17 val PER: 0.0373
2026-01-04 21:36:58,281: t15.2023.11.19 val PER: 0.0379
2026-01-04 21:36:58,281: t15.2023.11.26 val PER: 0.1428
2026-01-04 21:36:58,281: t15.2023.12.03 val PER: 0.1145
2026-01-04 21:36:58,281: t15.2023.12.08 val PER: 0.1105
2026-01-04 21:36:58,281: t15.2023.12.10 val PER: 0.0986
2026-01-04 21:36:58,281: t15.2023.12.17 val PER: 0.1393
2026-01-04 21:36:58,281: t15.2023.12.29 val PER: 0.1297
2026-01-04 21:36:58,281: t15.2024.02.25 val PER: 0.1152
2026-01-04 21:36:58,281: t15.2024.03.08 val PER: 0.2333
2026-01-04 21:36:58,281: t15.2024.03.15 val PER: 0.2145
2026-01-04 21:36:58,281: t15.2024.03.17 val PER: 0.1485
2026-01-04 21:36:58,281: t15.2024.05.10 val PER: 0.1768
2026-01-04 21:36:58,281: t15.2024.06.14 val PER: 0.1893
2026-01-04 21:36:58,282: t15.2024.07.19 val PER: 0.2498
2026-01-04 21:36:58,282: t15.2024.07.21 val PER: 0.1069
2026-01-04 21:36:58,282: t15.2024.07.28 val PER: 0.1463
2026-01-04 21:36:58,282: t15.2025.01.10 val PER: 0.3058
2026-01-04 21:36:58,282: t15.2025.01.12 val PER: 0.1547
2026-01-04 21:36:58,285: t15.2025.03.14 val PER: 0.3536
2026-01-04 21:36:58,285: t15.2025.03.16 val PER: 0.2055
2026-01-04 21:36:58,285: t15.2025.03.30 val PER: 0.3103
2026-01-04 21:36:58,285: t15.2025.04.13 val PER: 0.2197
2026-01-04 21:36:58,540: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_12000
2026-01-04 21:37:16,555: Train batch 12200: loss: 5.53 grad norm: 38.81 time: 0.065
2026-01-04 21:37:34,412: Train batch 12400: loss: 4.72 grad norm: 36.15 time: 0.040
2026-01-04 21:37:43,622: Running test after training batch: 12500
2026-01-04 21:37:43,728: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:37:48,580: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:37:48,612: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-04 21:37:50,373: Val batch 12500: PER (avg): 0.1566 CTC Loss (avg): 15.9445 WER(1gram): 46.95% (n=64) time: 6.751
2026-01-04 21:37:50,373: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 21:37:50,374: t15.2023.08.13 val PER: 0.1185
2026-01-04 21:37:50,374: t15.2023.08.18 val PER: 0.1073
2026-01-04 21:37:50,374: t15.2023.08.20 val PER: 0.1136
2026-01-04 21:37:50,374: t15.2023.08.25 val PER: 0.0904
2026-01-04 21:37:50,374: t15.2023.08.27 val PER: 0.1929
2026-01-04 21:37:50,374: t15.2023.09.01 val PER: 0.0828
2026-01-04 21:37:50,374: t15.2023.09.03 val PER: 0.1615
2026-01-04 21:37:50,374: t15.2023.09.24 val PER: 0.1347
2026-01-04 21:37:50,374: t15.2023.09.29 val PER: 0.1302
2026-01-04 21:37:50,374: t15.2023.10.01 val PER: 0.1711
2026-01-04 21:37:50,374: t15.2023.10.06 val PER: 0.0840
2026-01-04 21:37:50,374: t15.2023.10.08 val PER: 0.2463
2026-01-04 21:37:50,374: t15.2023.10.13 val PER: 0.2126
2026-01-04 21:37:50,374: t15.2023.10.15 val PER: 0.1589
2026-01-04 21:37:50,375: t15.2023.10.20 val PER: 0.2013
2026-01-04 21:37:50,375: t15.2023.10.22 val PER: 0.1125
2026-01-04 21:37:50,375: t15.2023.11.03 val PER: 0.1832
2026-01-04 21:37:50,375: t15.2023.11.04 val PER: 0.0307
2026-01-04 21:37:50,375: t15.2023.11.17 val PER: 0.0451
2026-01-04 21:37:50,375: t15.2023.11.19 val PER: 0.0339
2026-01-04 21:37:50,375: t15.2023.11.26 val PER: 0.1319
2026-01-04 21:37:50,375: t15.2023.12.03 val PER: 0.1261
2026-01-04 21:37:50,375: t15.2023.12.08 val PER: 0.1085
2026-01-04 21:37:50,375: t15.2023.12.10 val PER: 0.0946
2026-01-04 21:37:50,375: t15.2023.12.17 val PER: 0.1528
2026-01-04 21:37:50,375: t15.2023.12.29 val PER: 0.1393
2026-01-04 21:37:50,375: t15.2024.02.25 val PER: 0.1067
2026-01-04 21:37:50,375: t15.2024.03.08 val PER: 0.2418
2026-01-04 21:37:50,376: t15.2024.03.15 val PER: 0.2108
2026-01-04 21:37:50,376: t15.2024.03.17 val PER: 0.1485
2026-01-04 21:37:50,376: t15.2024.05.10 val PER: 0.1679
2026-01-04 21:37:50,376: t15.2024.06.14 val PER: 0.1798
2026-01-04 21:37:50,376: t15.2024.07.19 val PER: 0.2472
2026-01-04 21:37:50,376: t15.2024.07.21 val PER: 0.1021
2026-01-04 21:37:50,376: t15.2024.07.28 val PER: 0.1309
2026-01-04 21:37:50,376: t15.2025.01.10 val PER: 0.2961
2026-01-04 21:37:50,376: t15.2025.01.12 val PER: 0.1478
2026-01-04 21:37:50,376: t15.2025.03.14 val PER: 0.3639
2026-01-04 21:37:50,376: t15.2025.03.16 val PER: 0.1937
2026-01-04 21:37:50,376: t15.2025.03.30 val PER: 0.3207
2026-01-04 21:37:50,376: t15.2025.04.13 val PER: 0.2211
2026-01-04 21:37:50,377: New best val WER(1gram) 47.72% --> 46.95%
2026-01-04 21:37:50,377: Checkpointing model
2026-01-04 21:37:51,015: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:37:51,304: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_12500
2026-01-04 21:38:00,414: Train batch 12600: loss: 7.50 grad norm: 41.43 time: 0.058
2026-01-04 21:38:19,019: Train batch 12800: loss: 5.66 grad norm: 38.87 time: 0.053
2026-01-04 21:38:37,335: Train batch 13000: loss: 6.23 grad norm: 40.55 time: 0.066
2026-01-04 21:38:37,336: Running test after training batch: 13000
2026-01-04 21:38:37,456: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:38:42,233: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:38:42,264: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-04 21:38:44,025: Val batch 13000: PER (avg): 0.1543 CTC Loss (avg): 15.7762 WER(1gram): 45.94% (n=64) time: 6.689
2026-01-04 21:38:44,025: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-04 21:38:44,025: t15.2023.08.13 val PER: 0.1081
2026-01-04 21:38:44,026: t15.2023.08.18 val PER: 0.1123
2026-01-04 21:38:44,026: t15.2023.08.20 val PER: 0.1088
2026-01-04 21:38:44,026: t15.2023.08.25 val PER: 0.0934
2026-01-04 21:38:44,026: t15.2023.08.27 val PER: 0.1865
2026-01-04 21:38:44,026: t15.2023.09.01 val PER: 0.0755
2026-01-04 21:38:44,026: t15.2023.09.03 val PER: 0.1615
2026-01-04 21:38:44,026: t15.2023.09.24 val PER: 0.1250
2026-01-04 21:38:44,026: t15.2023.09.29 val PER: 0.1315
2026-01-04 21:38:44,026: t15.2023.10.01 val PER: 0.1750
2026-01-04 21:38:44,026: t15.2023.10.06 val PER: 0.0969
2026-01-04 21:38:44,026: t15.2023.10.08 val PER: 0.2530
2026-01-04 21:38:44,026: t15.2023.10.13 val PER: 0.2095
2026-01-04 21:38:44,026: t15.2023.10.15 val PER: 0.1595
2026-01-04 21:38:44,026: t15.2023.10.20 val PER: 0.1812
2026-01-04 21:38:44,027: t15.2023.10.22 val PER: 0.1136
2026-01-04 21:38:44,027: t15.2023.11.03 val PER: 0.1852
2026-01-04 21:38:44,027: t15.2023.11.04 val PER: 0.0307
2026-01-04 21:38:44,027: t15.2023.11.17 val PER: 0.0342
2026-01-04 21:38:44,027: t15.2023.11.19 val PER: 0.0399
2026-01-04 21:38:44,027: t15.2023.11.26 val PER: 0.1304
2026-01-04 21:38:44,027: t15.2023.12.03 val PER: 0.1145
2026-01-04 21:38:44,027: t15.2023.12.08 val PER: 0.1072
2026-01-04 21:38:44,027: t15.2023.12.10 val PER: 0.1012
2026-01-04 21:38:44,027: t15.2023.12.17 val PER: 0.1362
2026-01-04 21:38:44,027: t15.2023.12.29 val PER: 0.1366
2026-01-04 21:38:44,027: t15.2024.02.25 val PER: 0.1096
2026-01-04 21:38:44,028: t15.2024.03.08 val PER: 0.2376
2026-01-04 21:38:44,028: t15.2024.03.15 val PER: 0.2039
2026-01-04 21:38:44,028: t15.2024.03.17 val PER: 0.1464
2026-01-04 21:38:44,028: t15.2024.05.10 val PER: 0.1530
2026-01-04 21:38:44,028: t15.2024.06.14 val PER: 0.1767
2026-01-04 21:38:44,028: t15.2024.07.19 val PER: 0.2452
2026-01-04 21:38:44,028: t15.2024.07.21 val PER: 0.0966
2026-01-04 21:38:44,028: t15.2024.07.28 val PER: 0.1390
2026-01-04 21:38:44,028: t15.2025.01.10 val PER: 0.2975
2026-01-04 21:38:44,028: t15.2025.01.12 val PER: 0.1401
2026-01-04 21:38:44,028: t15.2025.03.14 val PER: 0.3388
2026-01-04 21:38:44,028: t15.2025.03.16 val PER: 0.1950
2026-01-04 21:38:44,028: t15.2025.03.30 val PER: 0.3149
2026-01-04 21:38:44,028: t15.2025.04.13 val PER: 0.2254
2026-01-04 21:38:44,029: New best val WER(1gram) 46.95% --> 45.94%
2026-01-04 21:38:44,029: Checkpointing model
2026-01-04 21:38:44,645: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:38:44,914: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_13000
2026-01-04 21:39:02,837: Train batch 13200: loss: 12.21 grad norm: 61.03 time: 0.054
2026-01-04 21:39:20,644: Train batch 13400: loss: 8.62 grad norm: 51.17 time: 0.063
2026-01-04 21:39:29,592: Running test after training batch: 13500
2026-01-04 21:39:29,715: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:39:34,491: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:39:34,522: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-04 21:39:36,358: Val batch 13500: PER (avg): 0.1526 CTC Loss (avg): 15.5500 WER(1gram): 46.19% (n=64) time: 6.766
2026-01-04 21:39:36,358: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 21:39:36,359: t15.2023.08.13 val PER: 0.1143
2026-01-04 21:39:36,359: t15.2023.08.18 val PER: 0.1174
2026-01-04 21:39:36,359: t15.2023.08.20 val PER: 0.1080
2026-01-04 21:39:36,359: t15.2023.08.25 val PER: 0.0889
2026-01-04 21:39:36,359: t15.2023.08.27 val PER: 0.1881
2026-01-04 21:39:36,359: t15.2023.09.01 val PER: 0.0836
2026-01-04 21:39:36,359: t15.2023.09.03 val PER: 0.1686
2026-01-04 21:39:36,359: t15.2023.09.24 val PER: 0.1214
2026-01-04 21:39:36,359: t15.2023.09.29 val PER: 0.1270
2026-01-04 21:39:36,359: t15.2023.10.01 val PER: 0.1750
2026-01-04 21:39:36,360: t15.2023.10.06 val PER: 0.0926
2026-01-04 21:39:36,360: t15.2023.10.08 val PER: 0.2463
2026-01-04 21:39:36,360: t15.2023.10.13 val PER: 0.2071
2026-01-04 21:39:36,360: t15.2023.10.15 val PER: 0.1589
2026-01-04 21:39:36,360: t15.2023.10.20 val PER: 0.1846
2026-01-04 21:39:36,360: t15.2023.10.22 val PER: 0.1169
2026-01-04 21:39:36,360: t15.2023.11.03 val PER: 0.1832
2026-01-04 21:39:36,360: t15.2023.11.04 val PER: 0.0375
2026-01-04 21:39:36,360: t15.2023.11.17 val PER: 0.0435
2026-01-04 21:39:36,360: t15.2023.11.19 val PER: 0.0299
2026-01-04 21:39:36,360: t15.2023.11.26 val PER: 0.1239
2026-01-04 21:39:36,360: t15.2023.12.03 val PER: 0.1155
2026-01-04 21:39:36,360: t15.2023.12.08 val PER: 0.1092
2026-01-04 21:39:36,361: t15.2023.12.10 val PER: 0.0933
2026-01-04 21:39:36,361: t15.2023.12.17 val PER: 0.1268
2026-01-04 21:39:36,361: t15.2023.12.29 val PER: 0.1297
2026-01-04 21:39:36,361: t15.2024.02.25 val PER: 0.1096
2026-01-04 21:39:36,361: t15.2024.03.08 val PER: 0.2361
2026-01-04 21:39:36,361: t15.2024.03.15 val PER: 0.2045
2026-01-04 21:39:36,361: t15.2024.03.17 val PER: 0.1395
2026-01-04 21:39:36,361: t15.2024.05.10 val PER: 0.1575
2026-01-04 21:39:36,361: t15.2024.06.14 val PER: 0.1577
2026-01-04 21:39:36,361: t15.2024.07.19 val PER: 0.2353
2026-01-04 21:39:36,361: t15.2024.07.21 val PER: 0.1007
2026-01-04 21:39:36,361: t15.2024.07.28 val PER: 0.1375
2026-01-04 21:39:36,362: t15.2025.01.10 val PER: 0.2893
2026-01-04 21:39:36,362: t15.2025.01.12 val PER: 0.1424
2026-01-04 21:39:36,362: t15.2025.03.14 val PER: 0.3388
2026-01-04 21:39:36,362: t15.2025.03.16 val PER: 0.1872
2026-01-04 21:39:36,362: t15.2025.03.30 val PER: 0.3092
2026-01-04 21:39:36,362: t15.2025.04.13 val PER: 0.2197
2026-01-04 21:39:36,623: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_13500
2026-01-04 21:39:45,605: Train batch 13600: loss: 12.71 grad norm: 66.51 time: 0.061
2026-01-04 21:40:03,832: Train batch 13800: loss: 8.75 grad norm: 53.73 time: 0.055
2026-01-04 21:40:21,964: Train batch 14000: loss: 11.41 grad norm: 58.56 time: 0.049
2026-01-04 21:40:21,964: Running test after training batch: 14000
2026-01-04 21:40:22,069: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:40:26,961: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:40:26,993: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost get
2026-01-04 21:40:28,801: Val batch 14000: PER (avg): 0.1514 CTC Loss (avg): 15.4587 WER(1gram): 45.43% (n=64) time: 6.836
2026-01-04 21:40:28,801: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 21:40:28,801: t15.2023.08.13 val PER: 0.1133
2026-01-04 21:40:28,801: t15.2023.08.18 val PER: 0.0989
2026-01-04 21:40:28,801: t15.2023.08.20 val PER: 0.1048
2026-01-04 21:40:28,801: t15.2023.08.25 val PER: 0.0949
2026-01-04 21:40:28,802: t15.2023.08.27 val PER: 0.1994
2026-01-04 21:40:28,802: t15.2023.09.01 val PER: 0.0747
2026-01-04 21:40:28,802: t15.2023.09.03 val PER: 0.1781
2026-01-04 21:40:28,802: t15.2023.09.24 val PER: 0.1262
2026-01-04 21:40:28,802: t15.2023.09.29 val PER: 0.1315
2026-01-04 21:40:28,802: t15.2023.10.01 val PER: 0.1770
2026-01-04 21:40:28,802: t15.2023.10.06 val PER: 0.0850
2026-01-04 21:40:28,802: t15.2023.10.08 val PER: 0.2571
2026-01-04 21:40:28,802: t15.2023.10.13 val PER: 0.2071
2026-01-04 21:40:28,802: t15.2023.10.15 val PER: 0.1529
2026-01-04 21:40:28,802: t15.2023.10.20 val PER: 0.1846
2026-01-04 21:40:28,802: t15.2023.10.22 val PER: 0.1080
2026-01-04 21:40:28,802: t15.2023.11.03 val PER: 0.1791
2026-01-04 21:40:28,802: t15.2023.11.04 val PER: 0.0341
2026-01-04 21:40:28,802: t15.2023.11.17 val PER: 0.0451
2026-01-04 21:40:28,803: t15.2023.11.19 val PER: 0.0319
2026-01-04 21:40:28,803: t15.2023.11.26 val PER: 0.1312
2026-01-04 21:40:28,803: t15.2023.12.03 val PER: 0.1145
2026-01-04 21:40:28,803: t15.2023.12.08 val PER: 0.1065
2026-01-04 21:40:28,803: t15.2023.12.10 val PER: 0.0933
2026-01-04 21:40:28,803: t15.2023.12.17 val PER: 0.1362
2026-01-04 21:40:28,803: t15.2023.12.29 val PER: 0.1215
2026-01-04 21:40:28,803: t15.2024.02.25 val PER: 0.1166
2026-01-04 21:40:28,803: t15.2024.03.08 val PER: 0.2262
2026-01-04 21:40:28,803: t15.2024.03.15 val PER: 0.2001
2026-01-04 21:40:28,803: t15.2024.03.17 val PER: 0.1444
2026-01-04 21:40:28,803: t15.2024.05.10 val PER: 0.1560
2026-01-04 21:40:28,803: t15.2024.06.14 val PER: 0.1562
2026-01-04 21:40:28,803: t15.2024.07.19 val PER: 0.2334
2026-01-04 21:40:28,803: t15.2024.07.21 val PER: 0.0931
2026-01-04 21:40:28,804: t15.2024.07.28 val PER: 0.1309
2026-01-04 21:40:28,804: t15.2025.01.10 val PER: 0.2948
2026-01-04 21:40:28,804: t15.2025.01.12 val PER: 0.1463
2026-01-04 21:40:28,804: t15.2025.03.14 val PER: 0.3476
2026-01-04 21:40:28,804: t15.2025.03.16 val PER: 0.1898
2026-01-04 21:40:28,804: t15.2025.03.30 val PER: 0.2966
2026-01-04 21:40:28,804: t15.2025.04.13 val PER: 0.2154
2026-01-04 21:40:28,808: New best val WER(1gram) 45.94% --> 45.43%
2026-01-04 21:40:28,808: Checkpointing model
2026-01-04 21:40:29,453: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:40:29,731: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_14000
2026-01-04 21:40:47,889: Train batch 14200: loss: 8.32 grad norm: 52.68 time: 0.055
2026-01-04 21:41:06,075: Train batch 14400: loss: 5.69 grad norm: 38.61 time: 0.063
2026-01-04 21:41:15,391: Running test after training batch: 14500
2026-01-04 21:41:15,524: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:41:20,338: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:41:20,372: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 21:41:22,225: Val batch 14500: PER (avg): 0.1501 CTC Loss (avg): 15.5603 WER(1gram): 46.70% (n=64) time: 6.834
2026-01-04 21:41:22,226: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 21:41:22,226: t15.2023.08.13 val PER: 0.1091
2026-01-04 21:41:22,226: t15.2023.08.18 val PER: 0.1073
2026-01-04 21:41:22,226: t15.2023.08.20 val PER: 0.1056
2026-01-04 21:41:22,226: t15.2023.08.25 val PER: 0.0889
2026-01-04 21:41:22,226: t15.2023.08.27 val PER: 0.1801
2026-01-04 21:41:22,227: t15.2023.09.01 val PER: 0.0755
2026-01-04 21:41:22,227: t15.2023.09.03 val PER: 0.1663
2026-01-04 21:41:22,227: t15.2023.09.24 val PER: 0.1189
2026-01-04 21:41:22,227: t15.2023.09.29 val PER: 0.1251
2026-01-04 21:41:22,227: t15.2023.10.01 val PER: 0.1764
2026-01-04 21:41:22,227: t15.2023.10.06 val PER: 0.0829
2026-01-04 21:41:22,227: t15.2023.10.08 val PER: 0.2476
2026-01-04 21:41:22,227: t15.2023.10.13 val PER: 0.2087
2026-01-04 21:41:22,227: t15.2023.10.15 val PER: 0.1516
2026-01-04 21:41:22,227: t15.2023.10.20 val PER: 0.1812
2026-01-04 21:41:22,228: t15.2023.10.22 val PER: 0.1158
2026-01-04 21:41:22,228: t15.2023.11.03 val PER: 0.1839
2026-01-04 21:41:22,228: t15.2023.11.04 val PER: 0.0341
2026-01-04 21:41:22,228: t15.2023.11.17 val PER: 0.0435
2026-01-04 21:41:22,228: t15.2023.11.19 val PER: 0.0259
2026-01-04 21:41:22,228: t15.2023.11.26 val PER: 0.1333
2026-01-04 21:41:22,228: t15.2023.12.03 val PER: 0.1092
2026-01-04 21:41:22,228: t15.2023.12.08 val PER: 0.1032
2026-01-04 21:41:22,228: t15.2023.12.10 val PER: 0.0920
2026-01-04 21:41:22,228: t15.2023.12.17 val PER: 0.1362
2026-01-04 21:41:22,229: t15.2023.12.29 val PER: 0.1283
2026-01-04 21:41:22,229: t15.2024.02.25 val PER: 0.1096
2026-01-04 21:41:22,229: t15.2024.03.08 val PER: 0.2304
2026-01-04 21:41:22,229: t15.2024.03.15 val PER: 0.2045
2026-01-04 21:41:22,229: t15.2024.03.17 val PER: 0.1339
2026-01-04 21:41:22,229: t15.2024.05.10 val PER: 0.1471
2026-01-04 21:41:22,229: t15.2024.06.14 val PER: 0.1609
2026-01-04 21:41:22,229: t15.2024.07.19 val PER: 0.2327
2026-01-04 21:41:22,229: t15.2024.07.21 val PER: 0.0966
2026-01-04 21:41:22,230: t15.2024.07.28 val PER: 0.1287
2026-01-04 21:41:22,230: t15.2025.01.10 val PER: 0.2865
2026-01-04 21:41:22,230: t15.2025.01.12 val PER: 0.1470
2026-01-04 21:41:22,230: t15.2025.03.14 val PER: 0.3447
2026-01-04 21:41:22,230: t15.2025.03.16 val PER: 0.1950
2026-01-04 21:41:22,230: t15.2025.03.30 val PER: 0.2908
2026-01-04 21:41:22,230: t15.2025.04.13 val PER: 0.2168
2026-01-04 21:41:22,511: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_14500
2026-01-04 21:41:31,193: Train batch 14600: loss: 12.04 grad norm: 59.27 time: 0.058
2026-01-04 21:41:48,756: Train batch 14800: loss: 5.64 grad norm: 45.53 time: 0.050
2026-01-04 21:42:06,290: Train batch 15000: loss: 9.28 grad norm: 51.52 time: 0.052
2026-01-04 21:42:06,290: Running test after training batch: 15000
2026-01-04 21:42:06,388: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:42:11,344: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:42:11,378: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 21:42:13,234: Val batch 15000: PER (avg): 0.1497 CTC Loss (avg): 15.3048 WER(1gram): 45.94% (n=64) time: 6.943
2026-01-04 21:42:13,234: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-04 21:42:13,234: t15.2023.08.13 val PER: 0.1060
2026-01-04 21:42:13,234: t15.2023.08.18 val PER: 0.0981
2026-01-04 21:42:13,234: t15.2023.08.20 val PER: 0.1128
2026-01-04 21:42:13,234: t15.2023.08.25 val PER: 0.0904
2026-01-04 21:42:13,234: t15.2023.08.27 val PER: 0.1881
2026-01-04 21:42:13,234: t15.2023.09.01 val PER: 0.0763
2026-01-04 21:42:13,234: t15.2023.09.03 val PER: 0.1520
2026-01-04 21:42:13,234: t15.2023.09.24 val PER: 0.1238
2026-01-04 21:42:13,235: t15.2023.09.29 val PER: 0.1283
2026-01-04 21:42:13,235: t15.2023.10.01 val PER: 0.1737
2026-01-04 21:42:13,235: t15.2023.10.06 val PER: 0.0818
2026-01-04 21:42:13,235: t15.2023.10.08 val PER: 0.2544
2026-01-04 21:42:13,235: t15.2023.10.13 val PER: 0.2040
2026-01-04 21:42:13,235: t15.2023.10.15 val PER: 0.1562
2026-01-04 21:42:13,235: t15.2023.10.20 val PER: 0.1879
2026-01-04 21:42:13,235: t15.2023.10.22 val PER: 0.1125
2026-01-04 21:42:13,235: t15.2023.11.03 val PER: 0.1764
2026-01-04 21:42:13,235: t15.2023.11.04 val PER: 0.0341
2026-01-04 21:42:13,235: t15.2023.11.17 val PER: 0.0373
2026-01-04 21:42:13,235: t15.2023.11.19 val PER: 0.0339
2026-01-04 21:42:13,235: t15.2023.11.26 val PER: 0.1217
2026-01-04 21:42:13,235: t15.2023.12.03 val PER: 0.1187
2026-01-04 21:42:13,235: t15.2023.12.08 val PER: 0.1032
2026-01-04 21:42:13,236: t15.2023.12.10 val PER: 0.0920
2026-01-04 21:42:13,236: t15.2023.12.17 val PER: 0.1445
2026-01-04 21:42:13,236: t15.2023.12.29 val PER: 0.1215
2026-01-04 21:42:13,236: t15.2024.02.25 val PER: 0.1053
2026-01-04 21:42:13,236: t15.2024.03.08 val PER: 0.2376
2026-01-04 21:42:13,236: t15.2024.03.15 val PER: 0.2008
2026-01-04 21:42:13,236: t15.2024.03.17 val PER: 0.1311
2026-01-04 21:42:13,236: t15.2024.05.10 val PER: 0.1575
2026-01-04 21:42:13,236: t15.2024.06.14 val PER: 0.1656
2026-01-04 21:42:13,236: t15.2024.07.19 val PER: 0.2327
2026-01-04 21:42:13,236: t15.2024.07.21 val PER: 0.0938
2026-01-04 21:42:13,237: t15.2024.07.28 val PER: 0.1324
2026-01-04 21:42:13,237: t15.2025.01.10 val PER: 0.3044
2026-01-04 21:42:13,237: t15.2025.01.12 val PER: 0.1347
2026-01-04 21:42:13,237: t15.2025.03.14 val PER: 0.3639
2026-01-04 21:42:13,237: t15.2025.03.16 val PER: 0.1872
2026-01-04 21:42:13,237: t15.2025.03.30 val PER: 0.2816
2026-01-04 21:42:13,237: t15.2025.04.13 val PER: 0.2197
2026-01-04 21:42:13,517: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_15000
2026-01-04 21:42:31,317: Train batch 15200: loss: 5.02 grad norm: 42.03 time: 0.057
2026-01-04 21:42:48,763: Train batch 15400: loss: 11.23 grad norm: 55.89 time: 0.049
2026-01-04 21:42:57,635: Running test after training batch: 15500
2026-01-04 21:42:57,747: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:43:02,545: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:43:02,580: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 21:43:04,448: Val batch 15500: PER (avg): 0.1485 CTC Loss (avg): 15.2566 WER(1gram): 45.18% (n=64) time: 6.813
2026-01-04 21:43:04,449: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 21:43:04,449: t15.2023.08.13 val PER: 0.1123
2026-01-04 21:43:04,449: t15.2023.08.18 val PER: 0.0956
2026-01-04 21:43:04,449: t15.2023.08.20 val PER: 0.1001
2026-01-04 21:43:04,449: t15.2023.08.25 val PER: 0.0858
2026-01-04 21:43:04,449: t15.2023.08.27 val PER: 0.1913
2026-01-04 21:43:04,449: t15.2023.09.01 val PER: 0.0722
2026-01-04 21:43:04,450: t15.2023.09.03 val PER: 0.1615
2026-01-04 21:43:04,450: t15.2023.09.24 val PER: 0.1177
2026-01-04 21:43:04,450: t15.2023.09.29 val PER: 0.1270
2026-01-04 21:43:04,450: t15.2023.10.01 val PER: 0.1724
2026-01-04 21:43:04,450: t15.2023.10.06 val PER: 0.0797
2026-01-04 21:43:04,450: t15.2023.10.08 val PER: 0.2436
2026-01-04 21:43:04,450: t15.2023.10.13 val PER: 0.1947
2026-01-04 21:43:04,450: t15.2023.10.15 val PER: 0.1543
2026-01-04 21:43:04,450: t15.2023.10.20 val PER: 0.1879
2026-01-04 21:43:04,450: t15.2023.10.22 val PER: 0.1136
2026-01-04 21:43:04,450: t15.2023.11.03 val PER: 0.1832
2026-01-04 21:43:04,450: t15.2023.11.04 val PER: 0.0375
2026-01-04 21:43:04,450: t15.2023.11.17 val PER: 0.0389
2026-01-04 21:43:04,451: t15.2023.11.19 val PER: 0.0339
2026-01-04 21:43:04,451: t15.2023.11.26 val PER: 0.1167
2026-01-04 21:43:04,451: t15.2023.12.03 val PER: 0.1134
2026-01-04 21:43:04,451: t15.2023.12.08 val PER: 0.0999
2026-01-04 21:43:04,451: t15.2023.12.10 val PER: 0.0907
2026-01-04 21:43:04,451: t15.2023.12.17 val PER: 0.1393
2026-01-04 21:43:04,451: t15.2023.12.29 val PER: 0.1277
2026-01-04 21:43:04,451: t15.2024.02.25 val PER: 0.1067
2026-01-04 21:43:04,451: t15.2024.03.08 val PER: 0.2333
2026-01-04 21:43:04,451: t15.2024.03.15 val PER: 0.1945
2026-01-04 21:43:04,451: t15.2024.03.17 val PER: 0.1402
2026-01-04 21:43:04,451: t15.2024.05.10 val PER: 0.1575
2026-01-04 21:43:04,451: t15.2024.06.14 val PER: 0.1672
2026-01-04 21:43:04,451: t15.2024.07.19 val PER: 0.2373
2026-01-04 21:43:04,451: t15.2024.07.21 val PER: 0.0917
2026-01-04 21:43:04,451: t15.2024.07.28 val PER: 0.1346
2026-01-04 21:43:04,451: t15.2025.01.10 val PER: 0.2934
2026-01-04 21:43:04,452: t15.2025.01.12 val PER: 0.1463
2026-01-04 21:43:04,452: t15.2025.03.14 val PER: 0.3388
2026-01-04 21:43:04,452: t15.2025.03.16 val PER: 0.1819
2026-01-04 21:43:04,452: t15.2025.03.30 val PER: 0.2828
2026-01-04 21:43:04,452: t15.2025.04.13 val PER: 0.2225
2026-01-04 21:43:04,454: New best val WER(1gram) 45.43% --> 45.18%
2026-01-04 21:43:04,454: Checkpointing model
2026-01-04 21:43:05,076: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:43:05,386: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_15500
2026-01-04 21:43:14,142: Train batch 15600: loss: 11.45 grad norm: 57.01 time: 0.061
2026-01-04 21:43:31,672: Train batch 15800: loss: 13.18 grad norm: 61.49 time: 0.066
2026-01-04 21:43:49,465: Train batch 16000: loss: 8.53 grad norm: 44.36 time: 0.055
2026-01-04 21:43:49,465: Running test after training batch: 16000
2026-01-04 21:43:49,570: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:43:54,515: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:43:54,548: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 21:43:56,372: Val batch 16000: PER (avg): 0.1479 CTC Loss (avg): 15.3509 WER(1gram): 45.69% (n=64) time: 6.907
2026-01-04 21:43:56,373: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 21:43:56,374: t15.2023.08.13 val PER: 0.1112
2026-01-04 21:43:56,374: t15.2023.08.18 val PER: 0.1006
2026-01-04 21:43:56,374: t15.2023.08.20 val PER: 0.1025
2026-01-04 21:43:56,374: t15.2023.08.25 val PER: 0.0919
2026-01-04 21:43:56,374: t15.2023.08.27 val PER: 0.1865
2026-01-04 21:43:56,374: t15.2023.09.01 val PER: 0.0747
2026-01-04 21:43:56,374: t15.2023.09.03 val PER: 0.1520
2026-01-04 21:43:56,374: t15.2023.09.24 val PER: 0.1153
2026-01-04 21:43:56,374: t15.2023.09.29 val PER: 0.1264
2026-01-04 21:43:56,375: t15.2023.10.01 val PER: 0.1717
2026-01-04 21:43:56,375: t15.2023.10.06 val PER: 0.0872
2026-01-04 21:43:56,375: t15.2023.10.08 val PER: 0.2463
2026-01-04 21:43:56,375: t15.2023.10.13 val PER: 0.1955
2026-01-04 21:43:56,375: t15.2023.10.15 val PER: 0.1536
2026-01-04 21:43:56,375: t15.2023.10.20 val PER: 0.1779
2026-01-04 21:43:56,375: t15.2023.10.22 val PER: 0.1080
2026-01-04 21:43:56,375: t15.2023.11.03 val PER: 0.1818
2026-01-04 21:43:56,375: t15.2023.11.04 val PER: 0.0341
2026-01-04 21:43:56,375: t15.2023.11.17 val PER: 0.0358
2026-01-04 21:43:56,375: t15.2023.11.19 val PER: 0.0299
2026-01-04 21:43:56,375: t15.2023.11.26 val PER: 0.1196
2026-01-04 21:43:56,376: t15.2023.12.03 val PER: 0.1103
2026-01-04 21:43:56,376: t15.2023.12.08 val PER: 0.0992
2026-01-04 21:43:56,376: t15.2023.12.10 val PER: 0.0880
2026-01-04 21:43:56,376: t15.2023.12.17 val PER: 0.1341
2026-01-04 21:43:56,376: t15.2023.12.29 val PER: 0.1242
2026-01-04 21:43:56,376: t15.2024.02.25 val PER: 0.1096
2026-01-04 21:43:56,376: t15.2024.03.08 val PER: 0.2319
2026-01-04 21:43:56,376: t15.2024.03.15 val PER: 0.1926
2026-01-04 21:43:56,376: t15.2024.03.17 val PER: 0.1318
2026-01-04 21:43:56,376: t15.2024.05.10 val PER: 0.1620
2026-01-04 21:43:56,376: t15.2024.06.14 val PER: 0.1593
2026-01-04 21:43:56,376: t15.2024.07.19 val PER: 0.2334
2026-01-04 21:43:56,376: t15.2024.07.21 val PER: 0.0876
2026-01-04 21:43:56,376: t15.2024.07.28 val PER: 0.1353
2026-01-04 21:43:56,376: t15.2025.01.10 val PER: 0.2961
2026-01-04 21:43:56,377: t15.2025.01.12 val PER: 0.1393
2026-01-04 21:43:56,377: t15.2025.03.14 val PER: 0.3506
2026-01-04 21:43:56,377: t15.2025.03.16 val PER: 0.1924
2026-01-04 21:43:56,377: t15.2025.03.30 val PER: 0.2977
2026-01-04 21:43:56,377: t15.2025.04.13 val PER: 0.2197
2026-01-04 21:43:56,660: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_16000
2026-01-04 21:44:15,053: Train batch 16200: loss: 6.51 grad norm: 42.71 time: 0.055
2026-01-04 21:44:33,261: Train batch 16400: loss: 10.35 grad norm: 59.62 time: 0.057
2026-01-04 21:44:42,388: Running test after training batch: 16500
2026-01-04 21:44:42,565: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:44:47,408: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:44:47,442: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 21:44:49,295: Val batch 16500: PER (avg): 0.1466 CTC Loss (avg): 15.1360 WER(1gram): 43.91% (n=64) time: 6.907
2026-01-04 21:44:49,295: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 21:44:49,295: t15.2023.08.13 val PER: 0.1081
2026-01-04 21:44:49,295: t15.2023.08.18 val PER: 0.1023
2026-01-04 21:44:49,295: t15.2023.08.20 val PER: 0.1041
2026-01-04 21:44:49,296: t15.2023.08.25 val PER: 0.0768
2026-01-04 21:44:49,296: t15.2023.08.27 val PER: 0.1849
2026-01-04 21:44:49,296: t15.2023.09.01 val PER: 0.0731
2026-01-04 21:44:49,296: t15.2023.09.03 val PER: 0.1580
2026-01-04 21:44:49,296: t15.2023.09.24 val PER: 0.1299
2026-01-04 21:44:49,296: t15.2023.09.29 val PER: 0.1289
2026-01-04 21:44:49,296: t15.2023.10.01 val PER: 0.1638
2026-01-04 21:44:49,296: t15.2023.10.06 val PER: 0.0818
2026-01-04 21:44:49,296: t15.2023.10.08 val PER: 0.2476
2026-01-04 21:44:49,296: t15.2023.10.13 val PER: 0.1947
2026-01-04 21:44:49,296: t15.2023.10.15 val PER: 0.1510
2026-01-04 21:44:49,297: t15.2023.10.20 val PER: 0.1913
2026-01-04 21:44:49,297: t15.2023.10.22 val PER: 0.1102
2026-01-04 21:44:49,297: t15.2023.11.03 val PER: 0.1791
2026-01-04 21:44:49,297: t15.2023.11.04 val PER: 0.0307
2026-01-04 21:44:49,297: t15.2023.11.17 val PER: 0.0358
2026-01-04 21:44:49,297: t15.2023.11.19 val PER: 0.0359
2026-01-04 21:44:49,297: t15.2023.11.26 val PER: 0.1181
2026-01-04 21:44:49,297: t15.2023.12.03 val PER: 0.1124
2026-01-04 21:44:49,297: t15.2023.12.08 val PER: 0.0999
2026-01-04 21:44:49,297: t15.2023.12.10 val PER: 0.0894
2026-01-04 21:44:49,297: t15.2023.12.17 val PER: 0.1299
2026-01-04 21:44:49,297: t15.2023.12.29 val PER: 0.1229
2026-01-04 21:44:49,297: t15.2024.02.25 val PER: 0.1138
2026-01-04 21:44:49,297: t15.2024.03.08 val PER: 0.2347
2026-01-04 21:44:49,298: t15.2024.03.15 val PER: 0.1926
2026-01-04 21:44:49,298: t15.2024.03.17 val PER: 0.1290
2026-01-04 21:44:49,298: t15.2024.05.10 val PER: 0.1530
2026-01-04 21:44:49,298: t15.2024.06.14 val PER: 0.1562
2026-01-04 21:44:49,298: t15.2024.07.19 val PER: 0.2340
2026-01-04 21:44:49,298: t15.2024.07.21 val PER: 0.0910
2026-01-04 21:44:49,298: t15.2024.07.28 val PER: 0.1243
2026-01-04 21:44:49,298: t15.2025.01.10 val PER: 0.2920
2026-01-04 21:44:49,298: t15.2025.01.12 val PER: 0.1393
2026-01-04 21:44:49,298: t15.2025.03.14 val PER: 0.3521
2026-01-04 21:44:49,298: t15.2025.03.16 val PER: 0.1846
2026-01-04 21:44:49,298: t15.2025.03.30 val PER: 0.2851
2026-01-04 21:44:49,298: t15.2025.04.13 val PER: 0.2097
2026-01-04 21:44:49,300: New best val WER(1gram) 45.18% --> 43.91%
2026-01-04 21:44:49,300: Checkpointing model
2026-01-04 21:44:49,930: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:44:50,258: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_16500
2026-01-04 21:44:59,086: Train batch 16600: loss: 8.16 grad norm: 52.86 time: 0.052
2026-01-04 21:45:16,839: Train batch 16800: loss: 16.06 grad norm: 72.17 time: 0.062
2026-01-04 21:45:34,661: Train batch 17000: loss: 7.78 grad norm: 47.13 time: 0.081
2026-01-04 21:45:34,661: Running test after training batch: 17000
2026-01-04 21:45:34,759: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:45:39,699: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:45:39,734: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 21:45:41,616: Val batch 17000: PER (avg): 0.1456 CTC Loss (avg): 15.0231 WER(1gram): 43.65% (n=64) time: 6.954
2026-01-04 21:45:41,616: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 21:45:41,616: t15.2023.08.13 val PER: 0.1081
2026-01-04 21:45:41,617: t15.2023.08.18 val PER: 0.1039
2026-01-04 21:45:41,617: t15.2023.08.20 val PER: 0.0993
2026-01-04 21:45:41,617: t15.2023.08.25 val PER: 0.0858
2026-01-04 21:45:41,617: t15.2023.08.27 val PER: 0.1913
2026-01-04 21:45:41,617: t15.2023.09.01 val PER: 0.0731
2026-01-04 21:45:41,617: t15.2023.09.03 val PER: 0.1556
2026-01-04 21:45:41,617: t15.2023.09.24 val PER: 0.1226
2026-01-04 21:45:41,617: t15.2023.09.29 val PER: 0.1225
2026-01-04 21:45:41,617: t15.2023.10.01 val PER: 0.1592
2026-01-04 21:45:41,617: t15.2023.10.06 val PER: 0.0807
2026-01-04 21:45:41,617: t15.2023.10.08 val PER: 0.2476
2026-01-04 21:45:41,617: t15.2023.10.13 val PER: 0.1901
2026-01-04 21:45:41,618: t15.2023.10.15 val PER: 0.1516
2026-01-04 21:45:41,618: t15.2023.10.20 val PER: 0.1812
2026-01-04 21:45:41,618: t15.2023.10.22 val PER: 0.1047
2026-01-04 21:45:41,618: t15.2023.11.03 val PER: 0.1805
2026-01-04 21:45:41,618: t15.2023.11.04 val PER: 0.0341
2026-01-04 21:45:41,618: t15.2023.11.17 val PER: 0.0389
2026-01-04 21:45:41,618: t15.2023.11.19 val PER: 0.0359
2026-01-04 21:45:41,618: t15.2023.11.26 val PER: 0.1109
2026-01-04 21:45:41,618: t15.2023.12.03 val PER: 0.1113
2026-01-04 21:45:41,618: t15.2023.12.08 val PER: 0.0972
2026-01-04 21:45:41,618: t15.2023.12.10 val PER: 0.0894
2026-01-04 21:45:41,618: t15.2023.12.17 val PER: 0.1341
2026-01-04 21:45:41,618: t15.2023.12.29 val PER: 0.1194
2026-01-04 21:45:41,618: t15.2024.02.25 val PER: 0.1110
2026-01-04 21:45:41,618: t15.2024.03.08 val PER: 0.2361
2026-01-04 21:45:41,618: t15.2024.03.15 val PER: 0.1995
2026-01-04 21:45:41,618: t15.2024.03.17 val PER: 0.1353
2026-01-04 21:45:41,619: t15.2024.05.10 val PER: 0.1426
2026-01-04 21:45:41,619: t15.2024.06.14 val PER: 0.1514
2026-01-04 21:45:41,619: t15.2024.07.19 val PER: 0.2254
2026-01-04 21:45:41,619: t15.2024.07.21 val PER: 0.0924
2026-01-04 21:45:41,619: t15.2024.07.28 val PER: 0.1243
2026-01-04 21:45:41,619: t15.2025.01.10 val PER: 0.2893
2026-01-04 21:45:41,619: t15.2025.01.12 val PER: 0.1416
2026-01-04 21:45:41,619: t15.2025.03.14 val PER: 0.3536
2026-01-04 21:45:41,619: t15.2025.03.16 val PER: 0.1767
2026-01-04 21:45:41,619: t15.2025.03.30 val PER: 0.2851
2026-01-04 21:45:41,619: t15.2025.04.13 val PER: 0.2225
2026-01-04 21:45:41,620: New best val WER(1gram) 43.91% --> 43.65%
2026-01-04 21:45:41,620: Checkpointing model
2026-01-04 21:45:42,235: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:45:42,526: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_17000
2026-01-04 21:46:00,036: Train batch 17200: loss: 9.74 grad norm: 50.63 time: 0.084
2026-01-04 21:46:17,905: Train batch 17400: loss: 11.80 grad norm: 58.19 time: 0.070
2026-01-04 21:46:26,935: Running test after training batch: 17500
2026-01-04 21:46:27,038: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:46:31,886: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:46:31,921: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 21:46:33,854: Val batch 17500: PER (avg): 0.1463 CTC Loss (avg): 15.0578 WER(1gram): 45.43% (n=64) time: 6.918
2026-01-04 21:46:33,855: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 21:46:33,855: t15.2023.08.13 val PER: 0.1154
2026-01-04 21:46:33,855: t15.2023.08.18 val PER: 0.1039
2026-01-04 21:46:33,855: t15.2023.08.20 val PER: 0.1072
2026-01-04 21:46:33,855: t15.2023.08.25 val PER: 0.0889
2026-01-04 21:46:33,855: t15.2023.08.27 val PER: 0.1865
2026-01-04 21:46:33,855: t15.2023.09.01 val PER: 0.0706
2026-01-04 21:46:33,855: t15.2023.09.03 val PER: 0.1603
2026-01-04 21:46:33,855: t15.2023.09.24 val PER: 0.1299
2026-01-04 21:46:33,855: t15.2023.09.29 val PER: 0.1264
2026-01-04 21:46:33,855: t15.2023.10.01 val PER: 0.1691
2026-01-04 21:46:33,856: t15.2023.10.06 val PER: 0.0797
2026-01-04 21:46:33,856: t15.2023.10.08 val PER: 0.2449
2026-01-04 21:46:33,856: t15.2023.10.13 val PER: 0.1924
2026-01-04 21:46:33,856: t15.2023.10.15 val PER: 0.1529
2026-01-04 21:46:33,856: t15.2023.10.20 val PER: 0.1980
2026-01-04 21:46:33,856: t15.2023.10.22 val PER: 0.1036
2026-01-04 21:46:33,856: t15.2023.11.03 val PER: 0.1757
2026-01-04 21:46:33,856: t15.2023.11.04 val PER: 0.0307
2026-01-04 21:46:33,856: t15.2023.11.17 val PER: 0.0358
2026-01-04 21:46:33,856: t15.2023.11.19 val PER: 0.0339
2026-01-04 21:46:33,856: t15.2023.11.26 val PER: 0.1152
2026-01-04 21:46:33,857: t15.2023.12.03 val PER: 0.1092
2026-01-04 21:46:33,857: t15.2023.12.08 val PER: 0.1019
2026-01-04 21:46:33,857: t15.2023.12.10 val PER: 0.0854
2026-01-04 21:46:33,857: t15.2023.12.17 val PER: 0.1320
2026-01-04 21:46:33,857: t15.2023.12.29 val PER: 0.1167
2026-01-04 21:46:33,857: t15.2024.02.25 val PER: 0.1025
2026-01-04 21:46:33,857: t15.2024.03.08 val PER: 0.2290
2026-01-04 21:46:33,857: t15.2024.03.15 val PER: 0.1932
2026-01-04 21:46:33,857: t15.2024.03.17 val PER: 0.1283
2026-01-04 21:46:33,857: t15.2024.05.10 val PER: 0.1501
2026-01-04 21:46:33,857: t15.2024.06.14 val PER: 0.1640
2026-01-04 21:46:33,857: t15.2024.07.19 val PER: 0.2301
2026-01-04 21:46:33,857: t15.2024.07.21 val PER: 0.0897
2026-01-04 21:46:33,857: t15.2024.07.28 val PER: 0.1265
2026-01-04 21:46:33,857: t15.2025.01.10 val PER: 0.2893
2026-01-04 21:46:33,858: t15.2025.01.12 val PER: 0.1347
2026-01-04 21:46:33,858: t15.2025.03.14 val PER: 0.3506
2026-01-04 21:46:33,858: t15.2025.03.16 val PER: 0.1859
2026-01-04 21:46:33,858: t15.2025.03.30 val PER: 0.2885
2026-01-04 21:46:33,858: t15.2025.04.13 val PER: 0.2240
2026-01-04 21:46:34,139: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_17500
2026-01-04 21:46:43,047: Train batch 17600: loss: 9.39 grad norm: 53.55 time: 0.051
2026-01-04 21:47:01,739: Train batch 17800: loss: 6.20 grad norm: 49.84 time: 0.042
2026-01-04 21:47:19,989: Train batch 18000: loss: 10.63 grad norm: 62.96 time: 0.061
2026-01-04 21:47:19,989: Running test after training batch: 18000
2026-01-04 21:47:20,092: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:47:25,349: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:47:25,383: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 21:47:27,258: Val batch 18000: PER (avg): 0.1456 CTC Loss (avg): 15.0687 WER(1gram): 44.42% (n=64) time: 7.268
2026-01-04 21:47:27,258: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 21:47:27,258: t15.2023.08.13 val PER: 0.1081
2026-01-04 21:47:27,258: t15.2023.08.18 val PER: 0.1031
2026-01-04 21:47:27,259: t15.2023.08.20 val PER: 0.1048
2026-01-04 21:47:27,259: t15.2023.08.25 val PER: 0.0919
2026-01-04 21:47:27,259: t15.2023.08.27 val PER: 0.1897
2026-01-04 21:47:27,259: t15.2023.09.01 val PER: 0.0698
2026-01-04 21:47:27,259: t15.2023.09.03 val PER: 0.1544
2026-01-04 21:47:27,259: t15.2023.09.24 val PER: 0.1299
2026-01-04 21:47:27,259: t15.2023.09.29 val PER: 0.1289
2026-01-04 21:47:27,259: t15.2023.10.01 val PER: 0.1691
2026-01-04 21:47:27,259: t15.2023.10.06 val PER: 0.0829
2026-01-04 21:47:27,259: t15.2023.10.08 val PER: 0.2436
2026-01-04 21:47:27,259: t15.2023.10.13 val PER: 0.1877
2026-01-04 21:47:27,259: t15.2023.10.15 val PER: 0.1516
2026-01-04 21:47:27,259: t15.2023.10.20 val PER: 0.1879
2026-01-04 21:47:27,259: t15.2023.10.22 val PER: 0.1047
2026-01-04 21:47:27,259: t15.2023.11.03 val PER: 0.1764
2026-01-04 21:47:27,259: t15.2023.11.04 val PER: 0.0410
2026-01-04 21:47:27,260: t15.2023.11.17 val PER: 0.0389
2026-01-04 21:47:27,260: t15.2023.11.19 val PER: 0.0339
2026-01-04 21:47:27,260: t15.2023.11.26 val PER: 0.1123
2026-01-04 21:47:27,260: t15.2023.12.03 val PER: 0.1197
2026-01-04 21:47:27,260: t15.2023.12.08 val PER: 0.1019
2026-01-04 21:47:27,260: t15.2023.12.10 val PER: 0.0867
2026-01-04 21:47:27,260: t15.2023.12.17 val PER: 0.1310
2026-01-04 21:47:27,260: t15.2023.12.29 val PER: 0.1194
2026-01-04 21:47:27,260: t15.2024.02.25 val PER: 0.0997
2026-01-04 21:47:27,260: t15.2024.03.08 val PER: 0.2219
2026-01-04 21:47:27,260: t15.2024.03.15 val PER: 0.1951
2026-01-04 21:47:27,260: t15.2024.03.17 val PER: 0.1276
2026-01-04 21:47:27,260: t15.2024.05.10 val PER: 0.1367
2026-01-04 21:47:27,260: t15.2024.06.14 val PER: 0.1593
2026-01-04 21:47:27,260: t15.2024.07.19 val PER: 0.2261
2026-01-04 21:47:27,260: t15.2024.07.21 val PER: 0.0869
2026-01-04 21:47:27,261: t15.2024.07.28 val PER: 0.1309
2026-01-04 21:47:27,261: t15.2025.01.10 val PER: 0.2961
2026-01-04 21:47:27,261: t15.2025.01.12 val PER: 0.1355
2026-01-04 21:47:27,261: t15.2025.03.14 val PER: 0.3447
2026-01-04 21:47:27,261: t15.2025.03.16 val PER: 0.1911
2026-01-04 21:47:27,261: t15.2025.03.30 val PER: 0.2793
2026-01-04 21:47:27,261: t15.2025.04.13 val PER: 0.2168
2026-01-04 21:47:27,542: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_18000
2026-01-04 21:47:46,286: Train batch 18200: loss: 7.64 grad norm: 48.49 time: 0.073
2026-01-04 21:48:04,811: Train batch 18400: loss: 4.81 grad norm: 42.77 time: 0.058
2026-01-04 21:48:14,129: Running test after training batch: 18500
2026-01-04 21:48:14,295: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:48:19,137: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:48:19,172: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 21:48:21,064: Val batch 18500: PER (avg): 0.1459 CTC Loss (avg): 15.0343 WER(1gram): 44.67% (n=64) time: 6.934
2026-01-04 21:48:21,065: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 21:48:21,065: t15.2023.08.13 val PER: 0.1081
2026-01-04 21:48:21,065: t15.2023.08.18 val PER: 0.1014
2026-01-04 21:48:21,065: t15.2023.08.20 val PER: 0.1072
2026-01-04 21:48:21,065: t15.2023.08.25 val PER: 0.0843
2026-01-04 21:48:21,065: t15.2023.08.27 val PER: 0.1897
2026-01-04 21:48:21,065: t15.2023.09.01 val PER: 0.0706
2026-01-04 21:48:21,065: t15.2023.09.03 val PER: 0.1580
2026-01-04 21:48:21,065: t15.2023.09.24 val PER: 0.1311
2026-01-04 21:48:21,066: t15.2023.09.29 val PER: 0.1257
2026-01-04 21:48:21,066: t15.2023.10.01 val PER: 0.1704
2026-01-04 21:48:21,066: t15.2023.10.06 val PER: 0.0797
2026-01-04 21:48:21,066: t15.2023.10.08 val PER: 0.2422
2026-01-04 21:48:21,066: t15.2023.10.13 val PER: 0.1932
2026-01-04 21:48:21,066: t15.2023.10.15 val PER: 0.1536
2026-01-04 21:48:21,066: t15.2023.10.20 val PER: 0.1846
2026-01-04 21:48:21,066: t15.2023.10.22 val PER: 0.1080
2026-01-04 21:48:21,066: t15.2023.11.03 val PER: 0.1784
2026-01-04 21:48:21,066: t15.2023.11.04 val PER: 0.0273
2026-01-04 21:48:21,066: t15.2023.11.17 val PER: 0.0404
2026-01-04 21:48:21,066: t15.2023.11.19 val PER: 0.0319
2026-01-04 21:48:21,066: t15.2023.11.26 val PER: 0.1130
2026-01-04 21:48:21,067: t15.2023.12.03 val PER: 0.1155
2026-01-04 21:48:21,067: t15.2023.12.08 val PER: 0.1005
2026-01-04 21:48:21,067: t15.2023.12.10 val PER: 0.0880
2026-01-04 21:48:21,067: t15.2023.12.17 val PER: 0.1310
2026-01-04 21:48:21,067: t15.2023.12.29 val PER: 0.1160
2026-01-04 21:48:21,068: t15.2024.02.25 val PER: 0.1067
2026-01-04 21:48:21,068: t15.2024.03.08 val PER: 0.2219
2026-01-04 21:48:21,068: t15.2024.03.15 val PER: 0.1951
2026-01-04 21:48:21,068: t15.2024.03.17 val PER: 0.1276
2026-01-04 21:48:21,068: t15.2024.05.10 val PER: 0.1441
2026-01-04 21:48:21,068: t15.2024.06.14 val PER: 0.1656
2026-01-04 21:48:21,068: t15.2024.07.19 val PER: 0.2254
2026-01-04 21:48:21,068: t15.2024.07.21 val PER: 0.0862
2026-01-04 21:48:21,068: t15.2024.07.28 val PER: 0.1316
2026-01-04 21:48:21,069: t15.2025.01.10 val PER: 0.2961
2026-01-04 21:48:21,069: t15.2025.01.12 val PER: 0.1386
2026-01-04 21:48:21,069: t15.2025.03.14 val PER: 0.3550
2026-01-04 21:48:21,069: t15.2025.03.16 val PER: 0.1846
2026-01-04 21:48:21,069: t15.2025.03.30 val PER: 0.2816
2026-01-04 21:48:21,069: t15.2025.04.13 val PER: 0.2083
2026-01-04 21:48:21,361: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_18500
2026-01-04 21:48:30,503: Train batch 18600: loss: 12.43 grad norm: 61.56 time: 0.067
2026-01-04 21:48:48,855: Train batch 18800: loss: 7.90 grad norm: 49.57 time: 0.064
2026-01-04 21:49:07,722: Train batch 19000: loss: 8.03 grad norm: 44.27 time: 0.065
2026-01-04 21:49:07,722: Running test after training batch: 19000
2026-01-04 21:49:07,857: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:49:12,674: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:49:12,708: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 21:49:14,605: Val batch 19000: PER (avg): 0.1448 CTC Loss (avg): 14.9861 WER(1gram): 43.91% (n=64) time: 6.883
2026-01-04 21:49:14,606: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 21:49:14,606: t15.2023.08.13 val PER: 0.1060
2026-01-04 21:49:14,606: t15.2023.08.18 val PER: 0.0989
2026-01-04 21:49:14,606: t15.2023.08.20 val PER: 0.1033
2026-01-04 21:49:14,606: t15.2023.08.25 val PER: 0.0873
2026-01-04 21:49:14,606: t15.2023.08.27 val PER: 0.1945
2026-01-04 21:49:14,606: t15.2023.09.01 val PER: 0.0698
2026-01-04 21:49:14,606: t15.2023.09.03 val PER: 0.1603
2026-01-04 21:49:14,606: t15.2023.09.24 val PER: 0.1262
2026-01-04 21:49:14,606: t15.2023.09.29 val PER: 0.1219
2026-01-04 21:49:14,606: t15.2023.10.01 val PER: 0.1638
2026-01-04 21:49:14,606: t15.2023.10.06 val PER: 0.0797
2026-01-04 21:49:14,606: t15.2023.10.08 val PER: 0.2409
2026-01-04 21:49:14,606: t15.2023.10.13 val PER: 0.1916
2026-01-04 21:49:14,606: t15.2023.10.15 val PER: 0.1510
2026-01-04 21:49:14,607: t15.2023.10.20 val PER: 0.1879
2026-01-04 21:49:14,607: t15.2023.10.22 val PER: 0.1047
2026-01-04 21:49:14,607: t15.2023.11.03 val PER: 0.1784
2026-01-04 21:49:14,607: t15.2023.11.04 val PER: 0.0307
2026-01-04 21:49:14,607: t15.2023.11.17 val PER: 0.0389
2026-01-04 21:49:14,607: t15.2023.11.19 val PER: 0.0319
2026-01-04 21:49:14,607: t15.2023.11.26 val PER: 0.1159
2026-01-04 21:49:14,607: t15.2023.12.03 val PER: 0.1103
2026-01-04 21:49:14,607: t15.2023.12.08 val PER: 0.0979
2026-01-04 21:49:14,607: t15.2023.12.10 val PER: 0.0867
2026-01-04 21:49:14,608: t15.2023.12.17 val PER: 0.1310
2026-01-04 21:49:14,608: t15.2023.12.29 val PER: 0.1208
2026-01-04 21:49:14,608: t15.2024.02.25 val PER: 0.1039
2026-01-04 21:49:14,608: t15.2024.03.08 val PER: 0.2248
2026-01-04 21:49:14,608: t15.2024.03.15 val PER: 0.1895
2026-01-04 21:49:14,608: t15.2024.03.17 val PER: 0.1325
2026-01-04 21:49:14,608: t15.2024.05.10 val PER: 0.1412
2026-01-04 21:49:14,608: t15.2024.06.14 val PER: 0.1593
2026-01-04 21:49:14,608: t15.2024.07.19 val PER: 0.2294
2026-01-04 21:49:14,608: t15.2024.07.21 val PER: 0.0890
2026-01-04 21:49:14,608: t15.2024.07.28 val PER: 0.1279
2026-01-04 21:49:14,608: t15.2025.01.10 val PER: 0.2865
2026-01-04 21:49:14,608: t15.2025.01.12 val PER: 0.1363
2026-01-04 21:49:14,608: t15.2025.03.14 val PER: 0.3491
2026-01-04 21:49:14,608: t15.2025.03.16 val PER: 0.1819
2026-01-04 21:49:14,608: t15.2025.03.30 val PER: 0.2851
2026-01-04 21:49:14,608: t15.2025.04.13 val PER: 0.2126
2026-01-04 21:49:14,892: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_19000
2026-01-04 21:49:33,380: Train batch 19200: loss: 5.94 grad norm: 47.49 time: 0.063
2026-01-04 21:49:51,999: Train batch 19400: loss: 4.81 grad norm: 36.27 time: 0.053
2026-01-04 21:50:01,173: Running test after training batch: 19500
2026-01-04 21:50:01,273: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:50:06,194: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:50:06,229: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 21:50:08,141: Val batch 19500: PER (avg): 0.1442 CTC Loss (avg): 14.9574 WER(1gram): 43.65% (n=64) time: 6.966
2026-01-04 21:50:08,141: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 21:50:08,141: t15.2023.08.13 val PER: 0.1029
2026-01-04 21:50:08,141: t15.2023.08.18 val PER: 0.0981
2026-01-04 21:50:08,141: t15.2023.08.20 val PER: 0.1064
2026-01-04 21:50:08,141: t15.2023.08.25 val PER: 0.0843
2026-01-04 21:50:08,141: t15.2023.08.27 val PER: 0.1817
2026-01-04 21:50:08,142: t15.2023.09.01 val PER: 0.0698
2026-01-04 21:50:08,142: t15.2023.09.03 val PER: 0.1568
2026-01-04 21:50:08,142: t15.2023.09.24 val PER: 0.1262
2026-01-04 21:50:08,142: t15.2023.09.29 val PER: 0.1225
2026-01-04 21:50:08,142: t15.2023.10.01 val PER: 0.1631
2026-01-04 21:50:08,142: t15.2023.10.06 val PER: 0.0818
2026-01-04 21:50:08,142: t15.2023.10.08 val PER: 0.2422
2026-01-04 21:50:08,142: t15.2023.10.13 val PER: 0.1916
2026-01-04 21:50:08,142: t15.2023.10.15 val PER: 0.1516
2026-01-04 21:50:08,143: t15.2023.10.20 val PER: 0.1946
2026-01-04 21:50:08,143: t15.2023.10.22 val PER: 0.1058
2026-01-04 21:50:08,143: t15.2023.11.03 val PER: 0.1818
2026-01-04 21:50:08,143: t15.2023.11.04 val PER: 0.0307
2026-01-04 21:50:08,143: t15.2023.11.17 val PER: 0.0451
2026-01-04 21:50:08,143: t15.2023.11.19 val PER: 0.0319
2026-01-04 21:50:08,143: t15.2023.11.26 val PER: 0.1145
2026-01-04 21:50:08,143: t15.2023.12.03 val PER: 0.1092
2026-01-04 21:50:08,143: t15.2023.12.08 val PER: 0.0939
2026-01-04 21:50:08,143: t15.2023.12.10 val PER: 0.0841
2026-01-04 21:50:08,143: t15.2023.12.17 val PER: 0.1299
2026-01-04 21:50:08,143: t15.2023.12.29 val PER: 0.1187
2026-01-04 21:50:08,143: t15.2024.02.25 val PER: 0.1039
2026-01-04 21:50:08,143: t15.2024.03.08 val PER: 0.2148
2026-01-04 21:50:08,143: t15.2024.03.15 val PER: 0.1926
2026-01-04 21:50:08,144: t15.2024.03.17 val PER: 0.1283
2026-01-04 21:50:08,144: t15.2024.05.10 val PER: 0.1412
2026-01-04 21:50:08,144: t15.2024.06.14 val PER: 0.1609
2026-01-04 21:50:08,144: t15.2024.07.19 val PER: 0.2287
2026-01-04 21:50:08,144: t15.2024.07.21 val PER: 0.0869
2026-01-04 21:50:08,144: t15.2024.07.28 val PER: 0.1272
2026-01-04 21:50:08,144: t15.2025.01.10 val PER: 0.2906
2026-01-04 21:50:08,144: t15.2025.01.12 val PER: 0.1363
2026-01-04 21:50:08,144: t15.2025.03.14 val PER: 0.3462
2026-01-04 21:50:08,144: t15.2025.03.16 val PER: 0.1859
2026-01-04 21:50:08,144: t15.2025.03.30 val PER: 0.2805
2026-01-04 21:50:08,144: t15.2025.04.13 val PER: 0.2126
2026-01-04 21:50:08,430: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_19500
2026-01-04 21:50:17,236: Train batch 19600: loss: 7.45 grad norm: 48.22 time: 0.057
2026-01-04 21:50:35,042: Train batch 19800: loss: 7.36 grad norm: 48.21 time: 0.055
2026-01-04 21:50:52,776: Running test after training batch: 19999
2026-01-04 21:50:52,870: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:50:57,654: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 21:50:57,688: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 21:50:59,608: Val batch 19999: PER (avg): 0.1448 CTC Loss (avg): 14.9479 WER(1gram): 43.15% (n=64) time: 6.831
2026-01-04 21:50:59,608: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 21:50:59,609: t15.2023.08.13 val PER: 0.1060
2026-01-04 21:50:59,609: t15.2023.08.18 val PER: 0.0964
2026-01-04 21:50:59,609: t15.2023.08.20 val PER: 0.1017
2026-01-04 21:50:59,609: t15.2023.08.25 val PER: 0.0828
2026-01-04 21:50:59,609: t15.2023.08.27 val PER: 0.1865
2026-01-04 21:50:59,609: t15.2023.09.01 val PER: 0.0722
2026-01-04 21:50:59,609: t15.2023.09.03 val PER: 0.1580
2026-01-04 21:50:59,609: t15.2023.09.24 val PER: 0.1274
2026-01-04 21:50:59,609: t15.2023.09.29 val PER: 0.1213
2026-01-04 21:50:59,609: t15.2023.10.01 val PER: 0.1684
2026-01-04 21:50:59,609: t15.2023.10.06 val PER: 0.0786
2026-01-04 21:50:59,609: t15.2023.10.08 val PER: 0.2436
2026-01-04 21:50:59,610: t15.2023.10.13 val PER: 0.1908
2026-01-04 21:50:59,610: t15.2023.10.15 val PER: 0.1556
2026-01-04 21:50:59,610: t15.2023.10.20 val PER: 0.1879
2026-01-04 21:50:59,610: t15.2023.10.22 val PER: 0.1036
2026-01-04 21:50:59,610: t15.2023.11.03 val PER: 0.1811
2026-01-04 21:50:59,610: t15.2023.11.04 val PER: 0.0273
2026-01-04 21:50:59,610: t15.2023.11.17 val PER: 0.0404
2026-01-04 21:50:59,610: t15.2023.11.19 val PER: 0.0339
2026-01-04 21:50:59,610: t15.2023.11.26 val PER: 0.1116
2026-01-04 21:50:59,610: t15.2023.12.03 val PER: 0.1103
2026-01-04 21:50:59,610: t15.2023.12.08 val PER: 0.0965
2026-01-04 21:50:59,610: t15.2023.12.10 val PER: 0.0867
2026-01-04 21:50:59,610: t15.2023.12.17 val PER: 0.1289
2026-01-04 21:50:59,611: t15.2023.12.29 val PER: 0.1132
2026-01-04 21:50:59,611: t15.2024.02.25 val PER: 0.1011
2026-01-04 21:50:59,611: t15.2024.03.08 val PER: 0.2205
2026-01-04 21:50:59,611: t15.2024.03.15 val PER: 0.1920
2026-01-04 21:50:59,611: t15.2024.03.17 val PER: 0.1262
2026-01-04 21:50:59,611: t15.2024.05.10 val PER: 0.1426
2026-01-04 21:50:59,611: t15.2024.06.14 val PER: 0.1656
2026-01-04 21:50:59,611: t15.2024.07.19 val PER: 0.2320
2026-01-04 21:50:59,611: t15.2024.07.21 val PER: 0.0890
2026-01-04 21:50:59,611: t15.2024.07.28 val PER: 0.1287
2026-01-04 21:50:59,611: t15.2025.01.10 val PER: 0.2934
2026-01-04 21:50:59,611: t15.2025.01.12 val PER: 0.1355
2026-01-04 21:50:59,612: t15.2025.03.14 val PER: 0.3550
2026-01-04 21:50:59,612: t15.2025.03.16 val PER: 0.1819
2026-01-04 21:50:59,612: t15.2025.03.30 val PER: 0.2897
2026-01-04 21:50:59,612: t15.2025.04.13 val PER: 0.2183
2026-01-04 21:50:59,613: New best val WER(1gram) 43.65% --> 43.15%
2026-01-04 21:50:59,613: Checkpointing model
2026-01-04 21:51:00,236: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:51:00,516: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/checkpoint_batch_19999
2026-01-04 21:51:00,543: Best avg val PER achieved: 0.14481
2026-01-04 21:51:00,544: Total training time: 35.64 minutes

=== RUN id05_wd1e-5.yaml ===
2026-01-04 21:51:06,338: Using device: cuda:0
2026-01-04 21:51:08,063: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-04 21:51:08,085: Using 45 sessions after filtering (from 45).
2026-01-04 21:51:08,499: Using torch.compile (if available)
2026-01-04 21:51:08,499: torch.compile not available (torch<2.0). Skipping.
2026-01-04 21:51:08,499: Initialized RNN decoding model
2026-01-04 21:51:08,499: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.05, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-04 21:51:08,500: Model has 44,907,305 parameters
2026-01-04 21:51:08,500: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-04 21:51:09,776: Successfully initialized datasets
2026-01-04 21:51:09,776: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-04 21:51:10,710: Train batch 0: loss: 580.59 grad norm: 1422.57 time: 0.181
2026-01-04 21:51:10,711: Running test after training batch: 0
2026-01-04 21:51:10,826: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:51:16,202: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-04 21:51:16,914: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-04 21:51:50,737: Val batch 0: PER (avg): 1.4289 CTC Loss (avg): 633.1762 WER(1gram): 100.00% (n=64) time: 40.026
2026-01-04 21:51:50,738: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-04 21:51:50,738: t15.2023.08.13 val PER: 1.3056
2026-01-04 21:51:50,738: t15.2023.08.18 val PER: 1.4300
2026-01-04 21:51:50,738: t15.2023.08.20 val PER: 1.2986
2026-01-04 21:51:50,738: t15.2023.08.25 val PER: 1.3404
2026-01-04 21:51:50,738: t15.2023.08.27 val PER: 1.2524
2026-01-04 21:51:50,738: t15.2023.09.01 val PER: 1.4440
2026-01-04 21:51:50,738: t15.2023.09.03 val PER: 1.3207
2026-01-04 21:51:50,738: t15.2023.09.24 val PER: 1.5400
2026-01-04 21:51:50,738: t15.2023.09.29 val PER: 1.4652
2026-01-04 21:51:50,738: t15.2023.10.01 val PER: 1.2114
2026-01-04 21:51:50,739: t15.2023.10.06 val PER: 1.4909
2026-01-04 21:51:50,739: t15.2023.10.08 val PER: 1.1881
2026-01-04 21:51:50,739: t15.2023.10.13 val PER: 1.3918
2026-01-04 21:51:50,739: t15.2023.10.15 val PER: 1.3896
2026-01-04 21:51:50,739: t15.2023.10.20 val PER: 1.5034
2026-01-04 21:51:50,739: t15.2023.10.22 val PER: 1.3920
2026-01-04 21:51:50,739: t15.2023.11.03 val PER: 1.5909
2026-01-04 21:51:50,739: t15.2023.11.04 val PER: 2.0273
2026-01-04 21:51:50,739: t15.2023.11.17 val PER: 1.9627
2026-01-04 21:51:50,739: t15.2023.11.19 val PER: 1.6786
2026-01-04 21:51:50,739: t15.2023.11.26 val PER: 1.5384
2026-01-04 21:51:50,739: t15.2023.12.03 val PER: 1.4265
2026-01-04 21:51:50,739: t15.2023.12.08 val PER: 1.4521
2026-01-04 21:51:50,739: t15.2023.12.10 val PER: 1.7004
2026-01-04 21:51:50,739: t15.2023.12.17 val PER: 1.3056
2026-01-04 21:51:50,739: t15.2023.12.29 val PER: 1.4063
2026-01-04 21:51:50,740: t15.2024.02.25 val PER: 1.4256
2026-01-04 21:51:50,740: t15.2024.03.08 val PER: 1.3201
2026-01-04 21:51:50,740: t15.2024.03.15 val PER: 1.3158
2026-01-04 21:51:50,740: t15.2024.03.17 val PER: 1.3996
2026-01-04 21:51:50,740: t15.2024.05.10 val PER: 1.3224
2026-01-04 21:51:50,740: t15.2024.06.14 val PER: 1.5315
2026-01-04 21:51:50,740: t15.2024.07.19 val PER: 1.0817
2026-01-04 21:51:50,740: t15.2024.07.21 val PER: 1.6269
2026-01-04 21:51:50,740: t15.2024.07.28 val PER: 1.6529
2026-01-04 21:51:50,740: t15.2025.01.10 val PER: 1.0964
2026-01-04 21:51:50,740: t15.2025.01.12 val PER: 1.7667
2026-01-04 21:51:50,740: t15.2025.03.14 val PER: 1.0399
2026-01-04 21:51:50,741: t15.2025.03.16 val PER: 1.6165
2026-01-04 21:51:50,741: t15.2025.03.30 val PER: 1.2851
2026-01-04 21:51:50,741: t15.2025.04.13 val PER: 1.5906
2026-01-04 21:51:50,742: New best val WER(1gram) inf% --> 100.00%
2026-01-04 21:51:50,742: Checkpointing model
2026-01-04 21:51:51,016: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:51:51,289: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_0
2026-01-04 21:52:09,812: Train batch 200: loss: 77.35 grad norm: 105.14 time: 0.054
2026-01-04 21:52:27,585: Train batch 400: loss: 53.73 grad norm: 88.97 time: 0.063
2026-01-04 21:52:36,551: Running test after training batch: 500
2026-01-04 21:52:36,700: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:52:41,630: WER debug example
  GT : you can see the code at this point as well
  PR : used aunt ease thus uhde at this ide is aisle
2026-01-04 21:52:41,667: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus ass tied
2026-01-04 21:52:44,042: Val batch 500: PER (avg): 0.5135 CTC Loss (avg): 55.4099 WER(1gram): 88.58% (n=64) time: 7.491
2026-01-04 21:52:44,042: WER lens: avg_true_words=6.16 avg_pred_words=5.61 max_pred_words=11
2026-01-04 21:52:44,043: t15.2023.08.13 val PER: 0.4678
2026-01-04 21:52:44,043: t15.2023.08.18 val PER: 0.4468
2026-01-04 21:52:44,043: t15.2023.08.20 val PER: 0.4424
2026-01-04 21:52:44,043: t15.2023.08.25 val PER: 0.4157
2026-01-04 21:52:44,043: t15.2023.08.27 val PER: 0.5145
2026-01-04 21:52:44,043: t15.2023.09.01 val PER: 0.4123
2026-01-04 21:52:44,043: t15.2023.09.03 val PER: 0.4929
2026-01-04 21:52:44,043: t15.2023.09.24 val PER: 0.4260
2026-01-04 21:52:44,043: t15.2023.09.29 val PER: 0.4627
2026-01-04 21:52:44,043: t15.2023.10.01 val PER: 0.5152
2026-01-04 21:52:44,043: t15.2023.10.06 val PER: 0.4230
2026-01-04 21:52:44,043: t15.2023.10.08 val PER: 0.5332
2026-01-04 21:52:44,044: t15.2023.10.13 val PER: 0.5547
2026-01-04 21:52:44,044: t15.2023.10.15 val PER: 0.4970
2026-01-04 21:52:44,044: t15.2023.10.20 val PER: 0.4362
2026-01-04 21:52:44,044: t15.2023.10.22 val PER: 0.4443
2026-01-04 21:52:44,044: t15.2023.11.03 val PER: 0.4939
2026-01-04 21:52:44,044: t15.2023.11.04 val PER: 0.2696
2026-01-04 21:52:44,044: t15.2023.11.17 val PER: 0.3593
2026-01-04 21:52:44,044: t15.2023.11.19 val PER: 0.3214
2026-01-04 21:52:44,045: t15.2023.11.26 val PER: 0.5478
2026-01-04 21:52:44,045: t15.2023.12.03 val PER: 0.5011
2026-01-04 21:52:44,045: t15.2023.12.08 val PER: 0.5140
2026-01-04 21:52:44,045: t15.2023.12.10 val PER: 0.4586
2026-01-04 21:52:44,045: t15.2023.12.17 val PER: 0.5676
2026-01-04 21:52:44,045: t15.2023.12.29 val PER: 0.5395
2026-01-04 21:52:44,045: t15.2024.02.25 val PER: 0.4747
2026-01-04 21:52:44,045: t15.2024.03.08 val PER: 0.6117
2026-01-04 21:52:44,045: t15.2024.03.15 val PER: 0.5516
2026-01-04 21:52:44,045: t15.2024.03.17 val PER: 0.5056
2026-01-04 21:52:44,045: t15.2024.05.10 val PER: 0.5290
2026-01-04 21:52:44,045: t15.2024.06.14 val PER: 0.5126
2026-01-04 21:52:44,045: t15.2024.07.19 val PER: 0.6605
2026-01-04 21:52:44,045: t15.2024.07.21 val PER: 0.4669
2026-01-04 21:52:44,045: t15.2024.07.28 val PER: 0.4978
2026-01-04 21:52:44,045: t15.2025.01.10 val PER: 0.7410
2026-01-04 21:52:44,045: t15.2025.01.12 val PER: 0.5497
2026-01-04 21:52:44,046: t15.2025.03.14 val PER: 0.7751
2026-01-04 21:52:44,046: t15.2025.03.16 val PER: 0.5916
2026-01-04 21:52:44,046: t15.2025.03.30 val PER: 0.7276
2026-01-04 21:52:44,046: t15.2025.04.13 val PER: 0.5678
2026-01-04 21:52:44,047: New best val WER(1gram) 100.00% --> 88.58%
2026-01-04 21:52:44,047: Checkpointing model
2026-01-04 21:52:44,665: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:52:44,930: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_500
2026-01-04 21:52:54,066: Train batch 600: loss: 48.48 grad norm: 78.04 time: 0.078
2026-01-04 21:53:12,066: Train batch 800: loss: 41.60 grad norm: 89.57 time: 0.056
2026-01-04 21:53:30,119: Train batch 1000: loss: 42.52 grad norm: 79.84 time: 0.065
2026-01-04 21:53:30,119: Running test after training batch: 1000
2026-01-04 21:53:30,300: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:53:35,184: WER debug example
  GT : you can see the code at this point as well
  PR : used end ease thus code it this uhde is oui
2026-01-04 21:53:35,217: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus wass it
2026-01-04 21:53:37,009: Val batch 1000: PER (avg): 0.4088 CTC Loss (avg): 42.0338 WER(1gram): 81.22% (n=64) time: 6.890
2026-01-04 21:53:37,009: WER lens: avg_true_words=6.16 avg_pred_words=5.48 max_pred_words=12
2026-01-04 21:53:37,010: t15.2023.08.13 val PER: 0.3701
2026-01-04 21:53:37,010: t15.2023.08.18 val PER: 0.3370
2026-01-04 21:53:37,010: t15.2023.08.20 val PER: 0.3542
2026-01-04 21:53:37,010: t15.2023.08.25 val PER: 0.2967
2026-01-04 21:53:37,010: t15.2023.08.27 val PER: 0.4277
2026-01-04 21:53:37,010: t15.2023.09.01 val PER: 0.3028
2026-01-04 21:53:37,010: t15.2023.09.03 val PER: 0.4002
2026-01-04 21:53:37,010: t15.2023.09.24 val PER: 0.3192
2026-01-04 21:53:37,010: t15.2023.09.29 val PER: 0.3586
2026-01-04 21:53:37,010: t15.2023.10.01 val PER: 0.4055
2026-01-04 21:53:37,010: t15.2023.10.06 val PER: 0.3143
2026-01-04 21:53:37,011: t15.2023.10.08 val PER: 0.4574
2026-01-04 21:53:37,011: t15.2023.10.13 val PER: 0.4616
2026-01-04 21:53:37,011: t15.2023.10.15 val PER: 0.3830
2026-01-04 21:53:37,011: t15.2023.10.20 val PER: 0.3792
2026-01-04 21:53:37,011: t15.2023.10.22 val PER: 0.3541
2026-01-04 21:53:37,011: t15.2023.11.03 val PER: 0.3948
2026-01-04 21:53:37,011: t15.2023.11.04 val PER: 0.1399
2026-01-04 21:53:37,011: t15.2023.11.17 val PER: 0.2644
2026-01-04 21:53:37,012: t15.2023.11.19 val PER: 0.2216
2026-01-04 21:53:37,012: t15.2023.11.26 val PER: 0.4543
2026-01-04 21:53:37,012: t15.2023.12.03 val PER: 0.4065
2026-01-04 21:53:37,012: t15.2023.12.08 val PER: 0.4008
2026-01-04 21:53:37,012: t15.2023.12.10 val PER: 0.3522
2026-01-04 21:53:37,012: t15.2023.12.17 val PER: 0.4012
2026-01-04 21:53:37,012: t15.2023.12.29 val PER: 0.4001
2026-01-04 21:53:37,012: t15.2024.02.25 val PER: 0.3455
2026-01-04 21:53:37,012: t15.2024.03.08 val PER: 0.4993
2026-01-04 21:53:37,012: t15.2024.03.15 val PER: 0.4403
2026-01-04 21:53:37,012: t15.2024.03.17 val PER: 0.4073
2026-01-04 21:53:37,012: t15.2024.05.10 val PER: 0.4235
2026-01-04 21:53:37,012: t15.2024.06.14 val PER: 0.3959
2026-01-04 21:53:37,012: t15.2024.07.19 val PER: 0.5379
2026-01-04 21:53:37,012: t15.2024.07.21 val PER: 0.3759
2026-01-04 21:53:37,012: t15.2024.07.28 val PER: 0.4184
2026-01-04 21:53:37,013: t15.2025.01.10 val PER: 0.6129
2026-01-04 21:53:37,013: t15.2025.01.12 val PER: 0.4473
2026-01-04 21:53:37,013: t15.2025.03.14 val PER: 0.6524
2026-01-04 21:53:37,013: t15.2025.03.16 val PER: 0.4817
2026-01-04 21:53:37,013: t15.2025.03.30 val PER: 0.6644
2026-01-04 21:53:37,013: t15.2025.04.13 val PER: 0.4922
2026-01-04 21:53:37,014: New best val WER(1gram) 88.58% --> 81.22%
2026-01-04 21:53:37,014: Checkpointing model
2026-01-04 21:53:37,636: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:53:37,900: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_1000
2026-01-04 21:53:55,371: Train batch 1200: loss: 32.39 grad norm: 74.30 time: 0.067
2026-01-04 21:54:13,106: Train batch 1400: loss: 35.72 grad norm: 80.72 time: 0.060
2026-01-04 21:54:21,944: Running test after training batch: 1500
2026-01-04 21:54:22,045: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:54:27,014: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt e the owed at this boyde is wheel
2026-01-04 21:54:27,046: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that cost
2026-01-04 21:54:28,643: Val batch 1500: PER (avg): 0.3752 CTC Loss (avg): 36.9323 WER(1gram): 74.37% (n=64) time: 6.698
2026-01-04 21:54:28,643: WER lens: avg_true_words=6.16 avg_pred_words=5.20 max_pred_words=11
2026-01-04 21:54:28,643: t15.2023.08.13 val PER: 0.3586
2026-01-04 21:54:28,643: t15.2023.08.18 val PER: 0.3076
2026-01-04 21:54:28,643: t15.2023.08.20 val PER: 0.3050
2026-01-04 21:54:28,643: t15.2023.08.25 val PER: 0.2485
2026-01-04 21:54:28,643: t15.2023.08.27 val PER: 0.4068
2026-01-04 21:54:28,643: t15.2023.09.01 val PER: 0.2695
2026-01-04 21:54:28,643: t15.2023.09.03 val PER: 0.3705
2026-01-04 21:54:28,644: t15.2023.09.24 val PER: 0.3034
2026-01-04 21:54:28,644: t15.2023.09.29 val PER: 0.3344
2026-01-04 21:54:28,644: t15.2023.10.01 val PER: 0.3838
2026-01-04 21:54:28,644: t15.2023.10.06 val PER: 0.2863
2026-01-04 21:54:28,644: t15.2023.10.08 val PER: 0.4384
2026-01-04 21:54:28,644: t15.2023.10.13 val PER: 0.4438
2026-01-04 21:54:28,644: t15.2023.10.15 val PER: 0.3566
2026-01-04 21:54:28,644: t15.2023.10.20 val PER: 0.3255
2026-01-04 21:54:28,644: t15.2023.10.22 val PER: 0.3151
2026-01-04 21:54:28,644: t15.2023.11.03 val PER: 0.3562
2026-01-04 21:54:28,644: t15.2023.11.04 val PER: 0.1195
2026-01-04 21:54:28,644: t15.2023.11.17 val PER: 0.2115
2026-01-04 21:54:28,644: t15.2023.11.19 val PER: 0.1677
2026-01-04 21:54:28,644: t15.2023.11.26 val PER: 0.4116
2026-01-04 21:54:28,644: t15.2023.12.03 val PER: 0.3624
2026-01-04 21:54:28,645: t15.2023.12.08 val PER: 0.3495
2026-01-04 21:54:28,645: t15.2023.12.10 val PER: 0.3049
2026-01-04 21:54:28,645: t15.2023.12.17 val PER: 0.3732
2026-01-04 21:54:28,645: t15.2023.12.29 val PER: 0.3658
2026-01-04 21:54:28,645: t15.2024.02.25 val PER: 0.3118
2026-01-04 21:54:28,645: t15.2024.03.08 val PER: 0.4552
2026-01-04 21:54:28,645: t15.2024.03.15 val PER: 0.4146
2026-01-04 21:54:28,645: t15.2024.03.17 val PER: 0.3717
2026-01-04 21:54:28,645: t15.2024.05.10 val PER: 0.3848
2026-01-04 21:54:28,645: t15.2024.06.14 val PER: 0.3927
2026-01-04 21:54:28,645: t15.2024.07.19 val PER: 0.5142
2026-01-04 21:54:28,645: t15.2024.07.21 val PER: 0.3386
2026-01-04 21:54:28,645: t15.2024.07.28 val PER: 0.3654
2026-01-04 21:54:28,645: t15.2025.01.10 val PER: 0.5978
2026-01-04 21:54:28,645: t15.2025.01.12 val PER: 0.4103
2026-01-04 21:54:28,646: t15.2025.03.14 val PER: 0.6109
2026-01-04 21:54:28,646: t15.2025.03.16 val PER: 0.4463
2026-01-04 21:54:28,646: t15.2025.03.30 val PER: 0.6080
2026-01-04 21:54:28,646: t15.2025.04.13 val PER: 0.4679
2026-01-04 21:54:28,647: New best val WER(1gram) 81.22% --> 74.37%
2026-01-04 21:54:28,647: Checkpointing model
2026-01-04 21:54:29,260: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:54:29,527: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_1500
2026-01-04 21:54:38,533: Train batch 1600: loss: 36.18 grad norm: 80.67 time: 0.063
2026-01-04 21:54:56,929: Train batch 1800: loss: 35.10 grad norm: 71.15 time: 0.089
2026-01-04 21:55:15,017: Train batch 2000: loss: 33.18 grad norm: 69.56 time: 0.066
2026-01-04 21:55:15,018: Running test after training batch: 2000
2026-01-04 21:55:15,158: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:55:20,022: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned ease the code at this bonde is wheel
2026-01-04 21:55:20,052: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the wass it
2026-01-04 21:55:21,630: Val batch 2000: PER (avg): 0.3246 CTC Loss (avg): 32.5284 WER(1gram): 68.78% (n=64) time: 6.612
2026-01-04 21:55:21,630: WER lens: avg_true_words=6.16 avg_pred_words=5.61 max_pred_words=11
2026-01-04 21:55:21,631: t15.2023.08.13 val PER: 0.3004
2026-01-04 21:55:21,631: t15.2023.08.18 val PER: 0.2607
2026-01-04 21:55:21,631: t15.2023.08.20 val PER: 0.2486
2026-01-04 21:55:21,631: t15.2023.08.25 val PER: 0.2364
2026-01-04 21:55:21,631: t15.2023.08.27 val PER: 0.3328
2026-01-04 21:55:21,631: t15.2023.09.01 val PER: 0.2192
2026-01-04 21:55:21,631: t15.2023.09.03 val PER: 0.3183
2026-01-04 21:55:21,631: t15.2023.09.24 val PER: 0.2609
2026-01-04 21:55:21,632: t15.2023.09.29 val PER: 0.2699
2026-01-04 21:55:21,632: t15.2023.10.01 val PER: 0.3236
2026-01-04 21:55:21,632: t15.2023.10.06 val PER: 0.2314
2026-01-04 21:55:21,632: t15.2023.10.08 val PER: 0.3884
2026-01-04 21:55:21,632: t15.2023.10.13 val PER: 0.3856
2026-01-04 21:55:21,632: t15.2023.10.15 val PER: 0.2940
2026-01-04 21:55:21,632: t15.2023.10.20 val PER: 0.2785
2026-01-04 21:55:21,632: t15.2023.10.22 val PER: 0.2506
2026-01-04 21:55:21,632: t15.2023.11.03 val PER: 0.3148
2026-01-04 21:55:21,632: t15.2023.11.04 val PER: 0.0853
2026-01-04 21:55:21,632: t15.2023.11.17 val PER: 0.1757
2026-01-04 21:55:21,633: t15.2023.11.19 val PER: 0.1437
2026-01-04 21:55:21,633: t15.2023.11.26 val PER: 0.3703
2026-01-04 21:55:21,633: t15.2023.12.03 val PER: 0.2994
2026-01-04 21:55:21,633: t15.2023.12.08 val PER: 0.2949
2026-01-04 21:55:21,633: t15.2023.12.10 val PER: 0.2589
2026-01-04 21:55:21,633: t15.2023.12.17 val PER: 0.3274
2026-01-04 21:55:21,633: t15.2023.12.29 val PER: 0.3240
2026-01-04 21:55:21,633: t15.2024.02.25 val PER: 0.2711
2026-01-04 21:55:21,633: t15.2024.03.08 val PER: 0.3983
2026-01-04 21:55:21,633: t15.2024.03.15 val PER: 0.3571
2026-01-04 21:55:21,633: t15.2024.03.17 val PER: 0.3389
2026-01-04 21:55:21,634: t15.2024.05.10 val PER: 0.3314
2026-01-04 21:55:21,634: t15.2024.06.14 val PER: 0.3486
2026-01-04 21:55:21,634: t15.2024.07.19 val PER: 0.4687
2026-01-04 21:55:21,634: t15.2024.07.21 val PER: 0.2924
2026-01-04 21:55:21,634: t15.2024.07.28 val PER: 0.3250
2026-01-04 21:55:21,634: t15.2025.01.10 val PER: 0.5399
2026-01-04 21:55:21,634: t15.2025.01.12 val PER: 0.3764
2026-01-04 21:55:21,634: t15.2025.03.14 val PER: 0.5266
2026-01-04 21:55:21,634: t15.2025.03.16 val PER: 0.3927
2026-01-04 21:55:21,634: t15.2025.03.30 val PER: 0.5448
2026-01-04 21:55:21,634: t15.2025.04.13 val PER: 0.4023
2026-01-04 21:55:21,635: New best val WER(1gram) 74.37% --> 68.78%
2026-01-04 21:55:21,635: Checkpointing model
2026-01-04 21:55:22,260: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:55:22,526: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_2000
2026-01-04 21:55:40,060: Train batch 2200: loss: 28.63 grad norm: 72.42 time: 0.060
2026-01-04 21:55:57,804: Train batch 2400: loss: 28.20 grad norm: 61.14 time: 0.051
2026-01-04 21:56:06,902: Running test after training batch: 2500
2026-01-04 21:56:07,006: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:56:11,883: WER debug example
  GT : you can see the code at this point as well
  PR : yule end e the code at this point is will
2026-01-04 21:56:11,910: WER debug example
  GT : how does it keep the cost down
  PR : aue des it hipp the wass it
2026-01-04 21:56:13,502: Val batch 2500: PER (avg): 0.3001 CTC Loss (avg): 29.8947 WER(1gram): 66.75% (n=64) time: 6.599
2026-01-04 21:56:13,502: WER lens: avg_true_words=6.16 avg_pred_words=5.64 max_pred_words=11
2026-01-04 21:56:13,502: t15.2023.08.13 val PER: 0.2921
2026-01-04 21:56:13,502: t15.2023.08.18 val PER: 0.2297
2026-01-04 21:56:13,502: t15.2023.08.20 val PER: 0.2423
2026-01-04 21:56:13,503: t15.2023.08.25 val PER: 0.2018
2026-01-04 21:56:13,503: t15.2023.08.27 val PER: 0.3135
2026-01-04 21:56:13,503: t15.2023.09.01 val PER: 0.2094
2026-01-04 21:56:13,503: t15.2023.09.03 val PER: 0.2838
2026-01-04 21:56:13,503: t15.2023.09.24 val PER: 0.2318
2026-01-04 21:56:13,503: t15.2023.09.29 val PER: 0.2514
2026-01-04 21:56:13,503: t15.2023.10.01 val PER: 0.3065
2026-01-04 21:56:13,503: t15.2023.10.06 val PER: 0.2056
2026-01-04 21:56:13,503: t15.2023.10.08 val PER: 0.3599
2026-01-04 21:56:13,503: t15.2023.10.13 val PER: 0.3514
2026-01-04 21:56:13,503: t15.2023.10.15 val PER: 0.2854
2026-01-04 21:56:13,503: t15.2023.10.20 val PER: 0.2852
2026-01-04 21:56:13,503: t15.2023.10.22 val PER: 0.2305
2026-01-04 21:56:13,503: t15.2023.11.03 val PER: 0.2992
2026-01-04 21:56:13,504: t15.2023.11.04 val PER: 0.0785
2026-01-04 21:56:13,504: t15.2023.11.17 val PER: 0.1415
2026-01-04 21:56:13,504: t15.2023.11.19 val PER: 0.1158
2026-01-04 21:56:13,504: t15.2023.11.26 val PER: 0.3406
2026-01-04 21:56:13,504: t15.2023.12.03 val PER: 0.2868
2026-01-04 21:56:13,504: t15.2023.12.08 val PER: 0.2710
2026-01-04 21:56:13,504: t15.2023.12.10 val PER: 0.2418
2026-01-04 21:56:13,504: t15.2023.12.17 val PER: 0.2921
2026-01-04 21:56:13,504: t15.2023.12.29 val PER: 0.2965
2026-01-04 21:56:13,504: t15.2024.02.25 val PER: 0.2514
2026-01-04 21:56:13,504: t15.2024.03.08 val PER: 0.3585
2026-01-04 21:56:13,504: t15.2024.03.15 val PER: 0.3415
2026-01-04 21:56:13,504: t15.2024.03.17 val PER: 0.3131
2026-01-04 21:56:13,504: t15.2024.05.10 val PER: 0.3195
2026-01-04 21:56:13,504: t15.2024.06.14 val PER: 0.3155
2026-01-04 21:56:13,504: t15.2024.07.19 val PER: 0.4364
2026-01-04 21:56:13,505: t15.2024.07.21 val PER: 0.2572
2026-01-04 21:56:13,505: t15.2024.07.28 val PER: 0.2919
2026-01-04 21:56:13,505: t15.2025.01.10 val PER: 0.4876
2026-01-04 21:56:13,505: t15.2025.01.12 val PER: 0.3533
2026-01-04 21:56:13,505: t15.2025.03.14 val PER: 0.4926
2026-01-04 21:56:13,505: t15.2025.03.16 val PER: 0.3586
2026-01-04 21:56:13,505: t15.2025.03.30 val PER: 0.4931
2026-01-04 21:56:13,505: t15.2025.04.13 val PER: 0.3866
2026-01-04 21:56:13,506: New best val WER(1gram) 68.78% --> 66.75%
2026-01-04 21:56:13,506: Checkpointing model
2026-01-04 21:56:14,114: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:56:14,379: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_2500
2026-01-04 21:56:23,281: Train batch 2600: loss: 34.65 grad norm: 85.83 time: 0.055
2026-01-04 21:56:41,241: Train batch 2800: loss: 25.56 grad norm: 70.16 time: 0.081
2026-01-04 21:56:59,456: Train batch 3000: loss: 30.44 grad norm: 70.64 time: 0.082
2026-01-04 21:56:59,456: Running test after training batch: 3000
2026-01-04 21:56:59,575: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:57:04,404: WER debug example
  GT : you can see the code at this point as well
  PR : yule end sze the code at this point is will
2026-01-04 21:57:04,432: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp the cost nett
2026-01-04 21:57:06,029: Val batch 3000: PER (avg): 0.2778 CTC Loss (avg): 27.4381 WER(1gram): 67.01% (n=64) time: 6.572
2026-01-04 21:57:06,029: WER lens: avg_true_words=6.16 avg_pred_words=5.83 max_pred_words=11
2026-01-04 21:57:06,029: t15.2023.08.13 val PER: 0.2588
2026-01-04 21:57:06,029: t15.2023.08.18 val PER: 0.2104
2026-01-04 21:57:06,030: t15.2023.08.20 val PER: 0.2153
2026-01-04 21:57:06,030: t15.2023.08.25 val PER: 0.1943
2026-01-04 21:57:06,030: t15.2023.08.27 val PER: 0.2942
2026-01-04 21:57:06,030: t15.2023.09.01 val PER: 0.1940
2026-01-04 21:57:06,030: t15.2023.09.03 val PER: 0.2791
2026-01-04 21:57:06,030: t15.2023.09.24 val PER: 0.2136
2026-01-04 21:57:06,030: t15.2023.09.29 val PER: 0.2317
2026-01-04 21:57:06,030: t15.2023.10.01 val PER: 0.2893
2026-01-04 21:57:06,030: t15.2023.10.06 val PER: 0.1959
2026-01-04 21:57:06,030: t15.2023.10.08 val PER: 0.3478
2026-01-04 21:57:06,031: t15.2023.10.13 val PER: 0.3313
2026-01-04 21:57:06,031: t15.2023.10.15 val PER: 0.2637
2026-01-04 21:57:06,031: t15.2023.10.20 val PER: 0.2718
2026-01-04 21:57:06,031: t15.2023.10.22 val PER: 0.1982
2026-01-04 21:57:06,031: t15.2023.11.03 val PER: 0.2653
2026-01-04 21:57:06,031: t15.2023.11.04 val PER: 0.0751
2026-01-04 21:57:06,031: t15.2023.11.17 val PER: 0.1166
2026-01-04 21:57:06,031: t15.2023.11.19 val PER: 0.1078
2026-01-04 21:57:06,031: t15.2023.11.26 val PER: 0.2971
2026-01-04 21:57:06,031: t15.2023.12.03 val PER: 0.2574
2026-01-04 21:57:06,031: t15.2023.12.08 val PER: 0.2530
2026-01-04 21:57:06,032: t15.2023.12.10 val PER: 0.1997
2026-01-04 21:57:06,032: t15.2023.12.17 val PER: 0.2775
2026-01-04 21:57:06,032: t15.2023.12.29 val PER: 0.2841
2026-01-04 21:57:06,032: t15.2024.02.25 val PER: 0.2402
2026-01-04 21:57:06,032: t15.2024.03.08 val PER: 0.3556
2026-01-04 21:57:06,032: t15.2024.03.15 val PER: 0.3346
2026-01-04 21:57:06,032: t15.2024.03.17 val PER: 0.2852
2026-01-04 21:57:06,032: t15.2024.05.10 val PER: 0.2868
2026-01-04 21:57:06,032: t15.2024.06.14 val PER: 0.3028
2026-01-04 21:57:06,032: t15.2024.07.19 val PER: 0.3935
2026-01-04 21:57:06,032: t15.2024.07.21 val PER: 0.2283
2026-01-04 21:57:06,032: t15.2024.07.28 val PER: 0.2743
2026-01-04 21:57:06,032: t15.2025.01.10 val PER: 0.4862
2026-01-04 21:57:06,033: t15.2025.01.12 val PER: 0.3156
2026-01-04 21:57:06,033: t15.2025.03.14 val PER: 0.4497
2026-01-04 21:57:06,033: t15.2025.03.16 val PER: 0.3233
2026-01-04 21:57:06,033: t15.2025.03.30 val PER: 0.4736
2026-01-04 21:57:06,033: t15.2025.04.13 val PER: 0.3595
2026-01-04 21:57:06,310: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_3000
2026-01-04 21:57:24,280: Train batch 3200: loss: 26.33 grad norm: 68.05 time: 0.075
2026-01-04 21:57:42,193: Train batch 3400: loss: 17.54 grad norm: 54.13 time: 0.048
2026-01-04 21:57:51,293: Running test after training batch: 3500
2026-01-04 21:57:51,455: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:57:56,377: WER debug example
  GT : you can see the code at this point as well
  PR : yule end sci the code at this point is will
2026-01-04 21:57:56,405: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp thus cussed nit
2026-01-04 21:57:57,977: Val batch 3500: PER (avg): 0.2670 CTC Loss (avg): 26.4233 WER(1gram): 66.50% (n=64) time: 6.684
2026-01-04 21:57:57,977: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-04 21:57:57,977: t15.2023.08.13 val PER: 0.2422
2026-01-04 21:57:57,977: t15.2023.08.18 val PER: 0.2045
2026-01-04 21:57:57,977: t15.2023.08.20 val PER: 0.2208
2026-01-04 21:57:57,977: t15.2023.08.25 val PER: 0.1822
2026-01-04 21:57:57,977: t15.2023.08.27 val PER: 0.2669
2026-01-04 21:57:57,977: t15.2023.09.01 val PER: 0.1745
2026-01-04 21:57:57,978: t15.2023.09.03 val PER: 0.2637
2026-01-04 21:57:57,978: t15.2023.09.24 val PER: 0.2087
2026-01-04 21:57:57,978: t15.2023.09.29 val PER: 0.2208
2026-01-04 21:57:57,978: t15.2023.10.01 val PER: 0.2847
2026-01-04 21:57:57,978: t15.2023.10.06 val PER: 0.1841
2026-01-04 21:57:57,978: t15.2023.10.08 val PER: 0.3424
2026-01-04 21:57:57,978: t15.2023.10.13 val PER: 0.3142
2026-01-04 21:57:57,978: t15.2023.10.15 val PER: 0.2518
2026-01-04 21:57:57,978: t15.2023.10.20 val PER: 0.2517
2026-01-04 21:57:57,978: t15.2023.10.22 val PER: 0.2038
2026-01-04 21:57:57,978: t15.2023.11.03 val PER: 0.2741
2026-01-04 21:57:57,978: t15.2023.11.04 val PER: 0.0887
2026-01-04 21:57:57,978: t15.2023.11.17 val PER: 0.1135
2026-01-04 21:57:57,978: t15.2023.11.19 val PER: 0.1038
2026-01-04 21:57:57,978: t15.2023.11.26 val PER: 0.2841
2026-01-04 21:57:57,979: t15.2023.12.03 val PER: 0.2405
2026-01-04 21:57:57,979: t15.2023.12.08 val PER: 0.2430
2026-01-04 21:57:57,979: t15.2023.12.10 val PER: 0.2011
2026-01-04 21:57:57,979: t15.2023.12.17 val PER: 0.2578
2026-01-04 21:57:57,979: t15.2023.12.29 val PER: 0.2629
2026-01-04 21:57:57,979: t15.2024.02.25 val PER: 0.2121
2026-01-04 21:57:57,979: t15.2024.03.08 val PER: 0.3457
2026-01-04 21:57:57,979: t15.2024.03.15 val PER: 0.3196
2026-01-04 21:57:57,979: t15.2024.03.17 val PER: 0.2762
2026-01-04 21:57:57,979: t15.2024.05.10 val PER: 0.2689
2026-01-04 21:57:57,979: t15.2024.06.14 val PER: 0.2823
2026-01-04 21:57:57,979: t15.2024.07.19 val PER: 0.3869
2026-01-04 21:57:57,979: t15.2024.07.21 val PER: 0.2207
2026-01-04 21:57:57,980: t15.2024.07.28 val PER: 0.2772
2026-01-04 21:57:57,980: t15.2025.01.10 val PER: 0.4683
2026-01-04 21:57:57,980: t15.2025.01.12 val PER: 0.2856
2026-01-04 21:57:57,980: t15.2025.03.14 val PER: 0.4453
2026-01-04 21:57:57,980: t15.2025.03.16 val PER: 0.3128
2026-01-04 21:57:57,980: t15.2025.03.30 val PER: 0.4575
2026-01-04 21:57:57,980: t15.2025.04.13 val PER: 0.3381
2026-01-04 21:57:57,982: New best val WER(1gram) 66.75% --> 66.50%
2026-01-04 21:57:57,982: Checkpointing model
2026-01-04 21:57:58,603: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:57:58,871: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_3500
2026-01-04 21:58:07,613: Train batch 3600: loss: 22.30 grad norm: 59.32 time: 0.066
2026-01-04 21:58:25,027: Train batch 3800: loss: 25.53 grad norm: 69.74 time: 0.066
2026-01-04 21:58:42,640: Train batch 4000: loss: 18.83 grad norm: 55.53 time: 0.055
2026-01-04 21:58:42,641: Running test after training batch: 4000
2026-01-04 21:58:42,747: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:58:47,662: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-04 21:58:47,688: WER debug example
  GT : how does it keep the cost down
  PR : aue des it keep the cussed it
2026-01-04 21:58:49,243: Val batch 4000: PER (avg): 0.2462 CTC Loss (avg): 24.1742 WER(1gram): 63.20% (n=64) time: 6.602
2026-01-04 21:58:49,243: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-04 21:58:49,243: t15.2023.08.13 val PER: 0.2193
2026-01-04 21:58:49,244: t15.2023.08.18 val PER: 0.1861
2026-01-04 21:58:49,244: t15.2023.08.20 val PER: 0.1994
2026-01-04 21:58:49,244: t15.2023.08.25 val PER: 0.1627
2026-01-04 21:58:49,244: t15.2023.08.27 val PER: 0.2733
2026-01-04 21:58:49,244: t15.2023.09.01 val PER: 0.1502
2026-01-04 21:58:49,244: t15.2023.09.03 val PER: 0.2363
2026-01-04 21:58:49,244: t15.2023.09.24 val PER: 0.1869
2026-01-04 21:58:49,244: t15.2023.09.29 val PER: 0.1972
2026-01-04 21:58:49,244: t15.2023.10.01 val PER: 0.2530
2026-01-04 21:58:49,244: t15.2023.10.06 val PER: 0.1604
2026-01-04 21:58:49,244: t15.2023.10.08 val PER: 0.3126
2026-01-04 21:58:49,244: t15.2023.10.13 val PER: 0.3088
2026-01-04 21:58:49,244: t15.2023.10.15 val PER: 0.2327
2026-01-04 21:58:49,244: t15.2023.10.20 val PER: 0.2282
2026-01-04 21:58:49,244: t15.2023.10.22 val PER: 0.1860
2026-01-04 21:58:49,244: t15.2023.11.03 val PER: 0.2442
2026-01-04 21:58:49,244: t15.2023.11.04 val PER: 0.0648
2026-01-04 21:58:49,245: t15.2023.11.17 val PER: 0.1073
2026-01-04 21:58:49,245: t15.2023.11.19 val PER: 0.1038
2026-01-04 21:58:49,245: t15.2023.11.26 val PER: 0.2630
2026-01-04 21:58:49,245: t15.2023.12.03 val PER: 0.2185
2026-01-04 21:58:49,245: t15.2023.12.08 val PER: 0.2197
2026-01-04 21:58:49,245: t15.2023.12.10 val PER: 0.1827
2026-01-04 21:58:49,245: t15.2023.12.17 val PER: 0.2380
2026-01-04 21:58:49,245: t15.2023.12.29 val PER: 0.2546
2026-01-04 21:58:49,245: t15.2024.02.25 val PER: 0.2177
2026-01-04 21:58:49,245: t15.2024.03.08 val PER: 0.3215
2026-01-04 21:58:49,245: t15.2024.03.15 val PER: 0.2996
2026-01-04 21:58:49,245: t15.2024.03.17 val PER: 0.2573
2026-01-04 21:58:49,245: t15.2024.05.10 val PER: 0.2645
2026-01-04 21:58:49,245: t15.2024.06.14 val PER: 0.2729
2026-01-04 21:58:49,245: t15.2024.07.19 val PER: 0.3632
2026-01-04 21:58:49,245: t15.2024.07.21 val PER: 0.1848
2026-01-04 21:58:49,246: t15.2024.07.28 val PER: 0.2324
2026-01-04 21:58:49,246: t15.2025.01.10 val PER: 0.4229
2026-01-04 21:58:49,246: t15.2025.01.12 val PER: 0.2856
2026-01-04 21:58:49,246: t15.2025.03.14 val PER: 0.4216
2026-01-04 21:58:49,246: t15.2025.03.16 val PER: 0.3037
2026-01-04 21:58:49,246: t15.2025.03.30 val PER: 0.4080
2026-01-04 21:58:49,246: t15.2025.04.13 val PER: 0.3238
2026-01-04 21:58:49,247: New best val WER(1gram) 66.50% --> 63.20%
2026-01-04 21:58:49,247: Checkpointing model
2026-01-04 21:58:49,856: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:58:50,137: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_4000
2026-01-04 21:59:08,184: Train batch 4200: loss: 21.88 grad norm: 65.72 time: 0.078
2026-01-04 21:59:26,335: Train batch 4400: loss: 16.91 grad norm: 55.89 time: 0.067
2026-01-04 21:59:35,472: Running test after training batch: 4500
2026-01-04 21:59:35,674: WER debug GT example: You can see the code at this point as well.
2026-01-04 21:59:40,699: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 21:59:40,727: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it yip the cost get
2026-01-04 21:59:42,270: Val batch 4500: PER (avg): 0.2349 CTC Loss (avg): 23.0154 WER(1gram): 60.66% (n=64) time: 6.798
2026-01-04 21:59:42,270: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-04 21:59:42,271: t15.2023.08.13 val PER: 0.2183
2026-01-04 21:59:42,271: t15.2023.08.18 val PER: 0.1836
2026-01-04 21:59:42,271: t15.2023.08.20 val PER: 0.1827
2026-01-04 21:59:42,271: t15.2023.08.25 val PER: 0.1401
2026-01-04 21:59:42,271: t15.2023.08.27 val PER: 0.2572
2026-01-04 21:59:42,271: t15.2023.09.01 val PER: 0.1494
2026-01-04 21:59:42,271: t15.2023.09.03 val PER: 0.2423
2026-01-04 21:59:42,271: t15.2023.09.24 val PER: 0.1881
2026-01-04 21:59:42,271: t15.2023.09.29 val PER: 0.1927
2026-01-04 21:59:42,271: t15.2023.10.01 val PER: 0.2563
2026-01-04 21:59:42,271: t15.2023.10.06 val PER: 0.1518
2026-01-04 21:59:42,271: t15.2023.10.08 val PER: 0.3180
2026-01-04 21:59:42,271: t15.2023.10.13 val PER: 0.2979
2026-01-04 21:59:42,272: t15.2023.10.15 val PER: 0.2268
2026-01-04 21:59:42,272: t15.2023.10.20 val PER: 0.2215
2026-01-04 21:59:42,272: t15.2023.10.22 val PER: 0.1893
2026-01-04 21:59:42,272: t15.2023.11.03 val PER: 0.2436
2026-01-04 21:59:42,272: t15.2023.11.04 val PER: 0.0819
2026-01-04 21:59:42,272: t15.2023.11.17 val PER: 0.1073
2026-01-04 21:59:42,272: t15.2023.11.19 val PER: 0.0898
2026-01-04 21:59:42,272: t15.2023.11.26 val PER: 0.2616
2026-01-04 21:59:42,272: t15.2023.12.03 val PER: 0.1985
2026-01-04 21:59:42,272: t15.2023.12.08 val PER: 0.2071
2026-01-04 21:59:42,272: t15.2023.12.10 val PER: 0.1735
2026-01-04 21:59:42,272: t15.2023.12.17 val PER: 0.2183
2026-01-04 21:59:42,272: t15.2023.12.29 val PER: 0.2423
2026-01-04 21:59:42,272: t15.2024.02.25 val PER: 0.1994
2026-01-04 21:59:42,272: t15.2024.03.08 val PER: 0.3101
2026-01-04 21:59:42,273: t15.2024.03.15 val PER: 0.2858
2026-01-04 21:59:42,273: t15.2024.03.17 val PER: 0.2378
2026-01-04 21:59:42,273: t15.2024.05.10 val PER: 0.2452
2026-01-04 21:59:42,273: t15.2024.06.14 val PER: 0.2461
2026-01-04 21:59:42,273: t15.2024.07.19 val PER: 0.3336
2026-01-04 21:59:42,273: t15.2024.07.21 val PER: 0.1648
2026-01-04 21:59:42,273: t15.2024.07.28 val PER: 0.2125
2026-01-04 21:59:42,273: t15.2025.01.10 val PER: 0.4118
2026-01-04 21:59:42,273: t15.2025.01.12 val PER: 0.2679
2026-01-04 21:59:42,273: t15.2025.03.14 val PER: 0.3846
2026-01-04 21:59:42,273: t15.2025.03.16 val PER: 0.2984
2026-01-04 21:59:42,273: t15.2025.03.30 val PER: 0.3885
2026-01-04 21:59:42,273: t15.2025.04.13 val PER: 0.2867
2026-01-04 21:59:42,275: New best val WER(1gram) 63.20% --> 60.66%
2026-01-04 21:59:42,275: Checkpointing model
2026-01-04 21:59:42,893: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 21:59:43,158: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_4500
2026-01-04 21:59:52,137: Train batch 4600: loss: 19.64 grad norm: 69.66 time: 0.061
2026-01-04 22:00:10,427: Train batch 4800: loss: 13.66 grad norm: 55.18 time: 0.062
2026-01-04 22:00:28,464: Train batch 5000: loss: 31.24 grad norm: 86.81 time: 0.063
2026-01-04 22:00:28,464: Running test after training batch: 5000
2026-01-04 22:00:28,607: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:00:33,479: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as wheel
2026-01-04 22:00:33,508: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-04 22:00:35,062: Val batch 5000: PER (avg): 0.2235 CTC Loss (avg): 21.8799 WER(1gram): 61.93% (n=64) time: 6.597
2026-01-04 22:00:35,062: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 22:00:35,062: t15.2023.08.13 val PER: 0.1840
2026-01-04 22:00:35,062: t15.2023.08.18 val PER: 0.1601
2026-01-04 22:00:35,062: t15.2023.08.20 val PER: 0.1763
2026-01-04 22:00:35,062: t15.2023.08.25 val PER: 0.1250
2026-01-04 22:00:35,062: t15.2023.08.27 val PER: 0.2299
2026-01-04 22:00:35,063: t15.2023.09.01 val PER: 0.1323
2026-01-04 22:00:35,063: t15.2023.09.03 val PER: 0.2280
2026-01-04 22:00:35,063: t15.2023.09.24 val PER: 0.1796
2026-01-04 22:00:35,063: t15.2023.09.29 val PER: 0.1793
2026-01-04 22:00:35,063: t15.2023.10.01 val PER: 0.2384
2026-01-04 22:00:35,063: t15.2023.10.06 val PER: 0.1356
2026-01-04 22:00:35,063: t15.2023.10.08 val PER: 0.2991
2026-01-04 22:00:35,063: t15.2023.10.13 val PER: 0.2964
2026-01-04 22:00:35,063: t15.2023.10.15 val PER: 0.2235
2026-01-04 22:00:35,063: t15.2023.10.20 val PER: 0.2282
2026-01-04 22:00:35,063: t15.2023.10.22 val PER: 0.1592
2026-01-04 22:00:35,063: t15.2023.11.03 val PER: 0.2151
2026-01-04 22:00:35,063: t15.2023.11.04 val PER: 0.0444
2026-01-04 22:00:35,063: t15.2023.11.17 val PER: 0.0949
2026-01-04 22:00:35,064: t15.2023.11.19 val PER: 0.0739
2026-01-04 22:00:35,064: t15.2023.11.26 val PER: 0.2355
2026-01-04 22:00:35,064: t15.2023.12.03 val PER: 0.1996
2026-01-04 22:00:35,064: t15.2023.12.08 val PER: 0.1931
2026-01-04 22:00:35,064: t15.2023.12.10 val PER: 0.1656
2026-01-04 22:00:35,064: t15.2023.12.17 val PER: 0.2245
2026-01-04 22:00:35,064: t15.2023.12.29 val PER: 0.2279
2026-01-04 22:00:35,064: t15.2024.02.25 val PER: 0.1952
2026-01-04 22:00:35,064: t15.2024.03.08 val PER: 0.3044
2026-01-04 22:00:35,064: t15.2024.03.15 val PER: 0.2858
2026-01-04 22:00:35,064: t15.2024.03.17 val PER: 0.2336
2026-01-04 22:00:35,064: t15.2024.05.10 val PER: 0.2452
2026-01-04 22:00:35,064: t15.2024.06.14 val PER: 0.2461
2026-01-04 22:00:35,064: t15.2024.07.19 val PER: 0.3289
2026-01-04 22:00:35,064: t15.2024.07.21 val PER: 0.1766
2026-01-04 22:00:35,064: t15.2024.07.28 val PER: 0.2015
2026-01-04 22:00:35,065: t15.2025.01.10 val PER: 0.3760
2026-01-04 22:00:35,065: t15.2025.01.12 val PER: 0.2471
2026-01-04 22:00:35,065: t15.2025.03.14 val PER: 0.3891
2026-01-04 22:00:35,065: t15.2025.03.16 val PER: 0.2657
2026-01-04 22:00:35,065: t15.2025.03.30 val PER: 0.3874
2026-01-04 22:00:35,065: t15.2025.04.13 val PER: 0.3010
2026-01-04 22:00:35,356: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_5000
2026-01-04 22:00:53,184: Train batch 5200: loss: 16.40 grad norm: 61.88 time: 0.051
2026-01-04 22:01:10,825: Train batch 5400: loss: 17.42 grad norm: 60.32 time: 0.067
2026-01-04 22:01:19,657: Running test after training batch: 5500
2026-01-04 22:01:19,768: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:01:25,309: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-04 22:01:25,340: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost tet
2026-01-04 22:01:26,927: Val batch 5500: PER (avg): 0.2152 CTC Loss (avg): 20.9472 WER(1gram): 57.36% (n=64) time: 7.270
2026-01-04 22:01:26,928: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-04 22:01:26,928: t15.2023.08.13 val PER: 0.1881
2026-01-04 22:01:26,928: t15.2023.08.18 val PER: 0.1626
2026-01-04 22:01:26,928: t15.2023.08.20 val PER: 0.1708
2026-01-04 22:01:26,928: t15.2023.08.25 val PER: 0.1220
2026-01-04 22:01:26,928: t15.2023.08.27 val PER: 0.2347
2026-01-04 22:01:26,928: t15.2023.09.01 val PER: 0.1299
2026-01-04 22:01:26,929: t15.2023.09.03 val PER: 0.2280
2026-01-04 22:01:26,929: t15.2023.09.24 val PER: 0.1687
2026-01-04 22:01:26,929: t15.2023.09.29 val PER: 0.1691
2026-01-04 22:01:26,929: t15.2023.10.01 val PER: 0.2279
2026-01-04 22:01:26,929: t15.2023.10.06 val PER: 0.1367
2026-01-04 22:01:26,929: t15.2023.10.08 val PER: 0.2896
2026-01-04 22:01:26,930: t15.2023.10.13 val PER: 0.2739
2026-01-04 22:01:26,931: t15.2023.10.15 val PER: 0.2057
2026-01-04 22:01:26,931: t15.2023.10.20 val PER: 0.2416
2026-01-04 22:01:26,931: t15.2023.10.22 val PER: 0.1670
2026-01-04 22:01:26,931: t15.2023.11.03 val PER: 0.2232
2026-01-04 22:01:26,931: t15.2023.11.04 val PER: 0.0648
2026-01-04 22:01:26,931: t15.2023.11.17 val PER: 0.0886
2026-01-04 22:01:26,931: t15.2023.11.19 val PER: 0.0659
2026-01-04 22:01:26,931: t15.2023.11.26 val PER: 0.2188
2026-01-04 22:01:26,931: t15.2023.12.03 val PER: 0.1838
2026-01-04 22:01:26,931: t15.2023.12.08 val PER: 0.1851
2026-01-04 22:01:26,931: t15.2023.12.10 val PER: 0.1459
2026-01-04 22:01:26,932: t15.2023.12.17 val PER: 0.2235
2026-01-04 22:01:26,932: t15.2023.12.29 val PER: 0.2155
2026-01-04 22:01:26,932: t15.2024.02.25 val PER: 0.1882
2026-01-04 22:01:26,932: t15.2024.03.08 val PER: 0.3073
2026-01-04 22:01:26,932: t15.2024.03.15 val PER: 0.2583
2026-01-04 22:01:26,932: t15.2024.03.17 val PER: 0.2266
2026-01-04 22:01:26,932: t15.2024.05.10 val PER: 0.2288
2026-01-04 22:01:26,932: t15.2024.06.14 val PER: 0.2413
2026-01-04 22:01:26,932: t15.2024.07.19 val PER: 0.3204
2026-01-04 22:01:26,932: t15.2024.07.21 val PER: 0.1621
2026-01-04 22:01:26,932: t15.2024.07.28 val PER: 0.2140
2026-01-04 22:01:26,932: t15.2025.01.10 val PER: 0.3774
2026-01-04 22:01:26,932: t15.2025.01.12 val PER: 0.2279
2026-01-04 22:01:26,932: t15.2025.03.14 val PER: 0.3595
2026-01-04 22:01:26,932: t15.2025.03.16 val PER: 0.2526
2026-01-04 22:01:26,932: t15.2025.03.30 val PER: 0.3598
2026-01-04 22:01:26,932: t15.2025.04.13 val PER: 0.2896
2026-01-04 22:01:26,933: New best val WER(1gram) 60.66% --> 57.36%
2026-01-04 22:01:26,933: Checkpointing model
2026-01-04 22:01:27,561: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:01:27,826: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_5500
2026-01-04 22:01:36,763: Train batch 5600: loss: 18.98 grad norm: 65.77 time: 0.062
2026-01-04 22:01:54,703: Train batch 5800: loss: 13.25 grad norm: 57.46 time: 0.081
2026-01-04 22:02:12,219: Train batch 6000: loss: 14.19 grad norm: 58.54 time: 0.049
2026-01-04 22:02:12,220: Running test after training batch: 6000
2026-01-04 22:02:12,331: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:02:17,152: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-04 22:02:17,183: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-04 22:02:18,780: Val batch 6000: PER (avg): 0.2094 CTC Loss (avg): 20.5419 WER(1gram): 56.35% (n=64) time: 6.560
2026-01-04 22:02:18,780: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 22:02:18,781: t15.2023.08.13 val PER: 0.1819
2026-01-04 22:02:18,781: t15.2023.08.18 val PER: 0.1618
2026-01-04 22:02:18,781: t15.2023.08.20 val PER: 0.1628
2026-01-04 22:02:18,781: t15.2023.08.25 val PER: 0.1114
2026-01-04 22:02:18,781: t15.2023.08.27 val PER: 0.2331
2026-01-04 22:02:18,781: t15.2023.09.01 val PER: 0.1315
2026-01-04 22:02:18,781: t15.2023.09.03 val PER: 0.2138
2026-01-04 22:02:18,781: t15.2023.09.24 val PER: 0.1541
2026-01-04 22:02:18,781: t15.2023.09.29 val PER: 0.1672
2026-01-04 22:02:18,781: t15.2023.10.01 val PER: 0.2199
2026-01-04 22:02:18,781: t15.2023.10.06 val PER: 0.1313
2026-01-04 22:02:18,782: t15.2023.10.08 val PER: 0.2977
2026-01-04 22:02:18,782: t15.2023.10.13 val PER: 0.2692
2026-01-04 22:02:18,782: t15.2023.10.15 val PER: 0.2063
2026-01-04 22:02:18,782: t15.2023.10.20 val PER: 0.2114
2026-01-04 22:02:18,782: t15.2023.10.22 val PER: 0.1637
2026-01-04 22:02:18,782: t15.2023.11.03 val PER: 0.2273
2026-01-04 22:02:18,782: t15.2023.11.04 val PER: 0.0683
2026-01-04 22:02:18,782: t15.2023.11.17 val PER: 0.0762
2026-01-04 22:02:18,782: t15.2023.11.19 val PER: 0.0778
2026-01-04 22:02:18,782: t15.2023.11.26 val PER: 0.2232
2026-01-04 22:02:18,782: t15.2023.12.03 val PER: 0.1733
2026-01-04 22:02:18,782: t15.2023.12.08 val PER: 0.1738
2026-01-04 22:02:18,783: t15.2023.12.10 val PER: 0.1432
2026-01-04 22:02:18,783: t15.2023.12.17 val PER: 0.2048
2026-01-04 22:02:18,783: t15.2023.12.29 val PER: 0.2155
2026-01-04 22:02:18,783: t15.2024.02.25 val PER: 0.1489
2026-01-04 22:02:18,783: t15.2024.03.08 val PER: 0.2902
2026-01-04 22:02:18,783: t15.2024.03.15 val PER: 0.2683
2026-01-04 22:02:18,783: t15.2024.03.17 val PER: 0.2134
2026-01-04 22:02:18,783: t15.2024.05.10 val PER: 0.2303
2026-01-04 22:02:18,783: t15.2024.06.14 val PER: 0.2177
2026-01-04 22:02:18,783: t15.2024.07.19 val PER: 0.3118
2026-01-04 22:02:18,783: t15.2024.07.21 val PER: 0.1510
2026-01-04 22:02:18,783: t15.2024.07.28 val PER: 0.1985
2026-01-04 22:02:18,783: t15.2025.01.10 val PER: 0.3609
2026-01-04 22:02:18,783: t15.2025.01.12 val PER: 0.2202
2026-01-04 22:02:18,783: t15.2025.03.14 val PER: 0.3757
2026-01-04 22:02:18,783: t15.2025.03.16 val PER: 0.2552
2026-01-04 22:02:18,784: t15.2025.03.30 val PER: 0.3632
2026-01-04 22:02:18,784: t15.2025.04.13 val PER: 0.2710
2026-01-04 22:02:18,784: New best val WER(1gram) 57.36% --> 56.35%
2026-01-04 22:02:18,784: Checkpointing model
2026-01-04 22:02:19,403: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:02:19,671: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_6000
2026-01-04 22:02:37,709: Train batch 6200: loss: 15.93 grad norm: 59.77 time: 0.069
2026-01-04 22:02:55,745: Train batch 6400: loss: 19.11 grad norm: 66.12 time: 0.063
2026-01-04 22:03:04,715: Running test after training batch: 6500
2026-01-04 22:03:04,885: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:03:10,053: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 22:03:10,082: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ged
2026-01-04 22:03:11,676: Val batch 6500: PER (avg): 0.2025 CTC Loss (avg): 19.9344 WER(1gram): 52.03% (n=64) time: 6.960
2026-01-04 22:03:11,676: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 22:03:11,676: t15.2023.08.13 val PER: 0.1684
2026-01-04 22:03:11,676: t15.2023.08.18 val PER: 0.1450
2026-01-04 22:03:11,676: t15.2023.08.20 val PER: 0.1557
2026-01-04 22:03:11,676: t15.2023.08.25 val PER: 0.1160
2026-01-04 22:03:11,677: t15.2023.08.27 val PER: 0.2219
2026-01-04 22:03:11,677: t15.2023.09.01 val PER: 0.1177
2026-01-04 22:03:11,677: t15.2023.09.03 val PER: 0.2078
2026-01-04 22:03:11,677: t15.2023.09.24 val PER: 0.1614
2026-01-04 22:03:11,677: t15.2023.09.29 val PER: 0.1621
2026-01-04 22:03:11,677: t15.2023.10.01 val PER: 0.2199
2026-01-04 22:03:11,677: t15.2023.10.06 val PER: 0.1216
2026-01-04 22:03:11,677: t15.2023.10.08 val PER: 0.2936
2026-01-04 22:03:11,677: t15.2023.10.13 val PER: 0.2700
2026-01-04 22:03:11,677: t15.2023.10.15 val PER: 0.2076
2026-01-04 22:03:11,677: t15.2023.10.20 val PER: 0.1980
2026-01-04 22:03:11,677: t15.2023.10.22 val PER: 0.1604
2026-01-04 22:03:11,677: t15.2023.11.03 val PER: 0.2103
2026-01-04 22:03:11,677: t15.2023.11.04 val PER: 0.0580
2026-01-04 22:03:11,678: t15.2023.11.17 val PER: 0.0591
2026-01-04 22:03:11,678: t15.2023.11.19 val PER: 0.0719
2026-01-04 22:03:11,678: t15.2023.11.26 val PER: 0.2014
2026-01-04 22:03:11,678: t15.2023.12.03 val PER: 0.1660
2026-01-04 22:03:11,678: t15.2023.12.08 val PER: 0.1664
2026-01-04 22:03:11,678: t15.2023.12.10 val PER: 0.1367
2026-01-04 22:03:11,678: t15.2023.12.17 val PER: 0.1954
2026-01-04 22:03:11,678: t15.2023.12.29 val PER: 0.2073
2026-01-04 22:03:11,678: t15.2024.02.25 val PER: 0.1559
2026-01-04 22:03:11,678: t15.2024.03.08 val PER: 0.3001
2026-01-04 22:03:11,678: t15.2024.03.15 val PER: 0.2620
2026-01-04 22:03:11,678: t15.2024.03.17 val PER: 0.2106
2026-01-04 22:03:11,678: t15.2024.05.10 val PER: 0.2214
2026-01-04 22:03:11,678: t15.2024.06.14 val PER: 0.2066
2026-01-04 22:03:11,679: t15.2024.07.19 val PER: 0.2933
2026-01-04 22:03:11,679: t15.2024.07.21 val PER: 0.1421
2026-01-04 22:03:11,679: t15.2024.07.28 val PER: 0.1868
2026-01-04 22:03:11,679: t15.2025.01.10 val PER: 0.3471
2026-01-04 22:03:11,679: t15.2025.01.12 val PER: 0.2156
2026-01-04 22:03:11,679: t15.2025.03.14 val PER: 0.4024
2026-01-04 22:03:11,679: t15.2025.03.16 val PER: 0.2435
2026-01-04 22:03:11,679: t15.2025.03.30 val PER: 0.3517
2026-01-04 22:03:11,679: t15.2025.04.13 val PER: 0.2753
2026-01-04 22:03:11,680: New best val WER(1gram) 56.35% --> 52.03%
2026-01-04 22:03:11,680: Checkpointing model
2026-01-04 22:03:12,311: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:03:12,578: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_6500
2026-01-04 22:03:21,331: Train batch 6600: loss: 11.83 grad norm: 50.87 time: 0.045
2026-01-04 22:03:39,091: Train batch 6800: loss: 15.07 grad norm: 59.19 time: 0.048
2026-01-04 22:03:57,076: Train batch 7000: loss: 16.61 grad norm: 59.84 time: 0.061
2026-01-04 22:03:57,076: Running test after training batch: 7000
2026-01-04 22:03:57,230: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:04:02,103: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 22:04:02,132: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 22:04:03,761: Val batch 7000: PER (avg): 0.1952 CTC Loss (avg): 19.0691 WER(1gram): 54.57% (n=64) time: 6.685
2026-01-04 22:04:03,761: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 22:04:03,761: t15.2023.08.13 val PER: 0.1642
2026-01-04 22:04:03,762: t15.2023.08.18 val PER: 0.1433
2026-01-04 22:04:03,762: t15.2023.08.20 val PER: 0.1493
2026-01-04 22:04:03,762: t15.2023.08.25 val PER: 0.0964
2026-01-04 22:04:03,762: t15.2023.08.27 val PER: 0.2074
2026-01-04 22:04:03,762: t15.2023.09.01 val PER: 0.1120
2026-01-04 22:04:03,762: t15.2023.09.03 val PER: 0.1995
2026-01-04 22:04:03,762: t15.2023.09.24 val PER: 0.1541
2026-01-04 22:04:03,762: t15.2023.09.29 val PER: 0.1691
2026-01-04 22:04:03,762: t15.2023.10.01 val PER: 0.2114
2026-01-04 22:04:03,762: t15.2023.10.06 val PER: 0.1076
2026-01-04 22:04:03,762: t15.2023.10.08 val PER: 0.2869
2026-01-04 22:04:03,762: t15.2023.10.13 val PER: 0.2638
2026-01-04 22:04:03,762: t15.2023.10.15 val PER: 0.1958
2026-01-04 22:04:03,763: t15.2023.10.20 val PER: 0.2181
2026-01-04 22:04:03,763: t15.2023.10.22 val PER: 0.1403
2026-01-04 22:04:03,763: t15.2023.11.03 val PER: 0.2062
2026-01-04 22:04:03,763: t15.2023.11.04 val PER: 0.0375
2026-01-04 22:04:03,763: t15.2023.11.17 val PER: 0.0684
2026-01-04 22:04:03,763: t15.2023.11.19 val PER: 0.0559
2026-01-04 22:04:03,763: t15.2023.11.26 val PER: 0.1928
2026-01-04 22:04:03,763: t15.2023.12.03 val PER: 0.1534
2026-01-04 22:04:03,763: t15.2023.12.08 val PER: 0.1578
2026-01-04 22:04:03,763: t15.2023.12.10 val PER: 0.1419
2026-01-04 22:04:03,763: t15.2023.12.17 val PER: 0.1778
2026-01-04 22:04:03,763: t15.2023.12.29 val PER: 0.1908
2026-01-04 22:04:03,763: t15.2024.02.25 val PER: 0.1461
2026-01-04 22:04:03,763: t15.2024.03.08 val PER: 0.2902
2026-01-04 22:04:03,763: t15.2024.03.15 val PER: 0.2376
2026-01-04 22:04:03,763: t15.2024.03.17 val PER: 0.1980
2026-01-04 22:04:03,763: t15.2024.05.10 val PER: 0.2125
2026-01-04 22:04:03,764: t15.2024.06.14 val PER: 0.2098
2026-01-04 22:04:03,764: t15.2024.07.19 val PER: 0.3013
2026-01-04 22:04:03,764: t15.2024.07.21 val PER: 0.1386
2026-01-04 22:04:03,764: t15.2024.07.28 val PER: 0.1853
2026-01-04 22:04:03,764: t15.2025.01.10 val PER: 0.3499
2026-01-04 22:04:03,764: t15.2025.01.12 val PER: 0.2048
2026-01-04 22:04:03,764: t15.2025.03.14 val PER: 0.3713
2026-01-04 22:04:03,764: t15.2025.03.16 val PER: 0.2644
2026-01-04 22:04:03,765: t15.2025.03.30 val PER: 0.3460
2026-01-04 22:04:03,765: t15.2025.04.13 val PER: 0.2653
2026-01-04 22:04:04,020: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_7000
2026-01-04 22:04:21,938: Train batch 7200: loss: 13.85 grad norm: 56.96 time: 0.078
2026-01-04 22:04:39,541: Train batch 7400: loss: 12.80 grad norm: 52.83 time: 0.075
2026-01-04 22:04:48,324: Running test after training batch: 7500
2026-01-04 22:04:48,521: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:04:53,653: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 22:04:53,682: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 22:04:55,300: Val batch 7500: PER (avg): 0.1886 CTC Loss (avg): 18.5587 WER(1gram): 54.06% (n=64) time: 6.976
2026-01-04 22:04:55,301: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 22:04:55,301: t15.2023.08.13 val PER: 0.1466
2026-01-04 22:04:55,301: t15.2023.08.18 val PER: 0.1358
2026-01-04 22:04:55,301: t15.2023.08.20 val PER: 0.1366
2026-01-04 22:04:55,301: t15.2023.08.25 val PER: 0.1039
2026-01-04 22:04:55,301: t15.2023.08.27 val PER: 0.2042
2026-01-04 22:04:55,301: t15.2023.09.01 val PER: 0.1153
2026-01-04 22:04:55,301: t15.2023.09.03 val PER: 0.1746
2026-01-04 22:04:55,301: t15.2023.09.24 val PER: 0.1529
2026-01-04 22:04:55,301: t15.2023.09.29 val PER: 0.1576
2026-01-04 22:04:55,302: t15.2023.10.01 val PER: 0.2107
2026-01-04 22:04:55,302: t15.2023.10.06 val PER: 0.1152
2026-01-04 22:04:55,302: t15.2023.10.08 val PER: 0.2882
2026-01-04 22:04:55,302: t15.2023.10.13 val PER: 0.2521
2026-01-04 22:04:55,302: t15.2023.10.15 val PER: 0.1892
2026-01-04 22:04:55,302: t15.2023.10.20 val PER: 0.1846
2026-01-04 22:04:55,302: t15.2023.10.22 val PER: 0.1392
2026-01-04 22:04:55,302: t15.2023.11.03 val PER: 0.2035
2026-01-04 22:04:55,302: t15.2023.11.04 val PER: 0.0444
2026-01-04 22:04:55,302: t15.2023.11.17 val PER: 0.0622
2026-01-04 22:04:55,302: t15.2023.11.19 val PER: 0.0579
2026-01-04 22:04:55,302: t15.2023.11.26 val PER: 0.1855
2026-01-04 22:04:55,302: t15.2023.12.03 val PER: 0.1481
2026-01-04 22:04:55,302: t15.2023.12.08 val PER: 0.1545
2026-01-04 22:04:55,302: t15.2023.12.10 val PER: 0.1235
2026-01-04 22:04:55,302: t15.2023.12.17 val PER: 0.1778
2026-01-04 22:04:55,303: t15.2023.12.29 val PER: 0.1867
2026-01-04 22:04:55,303: t15.2024.02.25 val PER: 0.1545
2026-01-04 22:04:55,303: t15.2024.03.08 val PER: 0.2688
2026-01-04 22:04:55,303: t15.2024.03.15 val PER: 0.2408
2026-01-04 22:04:55,303: t15.2024.03.17 val PER: 0.1841
2026-01-04 22:04:55,303: t15.2024.05.10 val PER: 0.2184
2026-01-04 22:04:55,303: t15.2024.06.14 val PER: 0.2098
2026-01-04 22:04:55,303: t15.2024.07.19 val PER: 0.2861
2026-01-04 22:04:55,303: t15.2024.07.21 val PER: 0.1366
2026-01-04 22:04:55,303: t15.2024.07.28 val PER: 0.1743
2026-01-04 22:04:55,303: t15.2025.01.10 val PER: 0.3499
2026-01-04 22:04:55,303: t15.2025.01.12 val PER: 0.1925
2026-01-04 22:04:55,303: t15.2025.03.14 val PER: 0.3669
2026-01-04 22:04:55,303: t15.2025.03.16 val PER: 0.2421
2026-01-04 22:04:55,303: t15.2025.03.30 val PER: 0.3345
2026-01-04 22:04:55,303: t15.2025.04.13 val PER: 0.2439
2026-01-04 22:04:55,562: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_7500
2026-01-04 22:05:04,299: Train batch 7600: loss: 15.69 grad norm: 62.63 time: 0.068
2026-01-04 22:05:21,866: Train batch 7800: loss: 13.81 grad norm: 60.06 time: 0.055
2026-01-04 22:05:39,874: Train batch 8000: loss: 10.86 grad norm: 51.61 time: 0.072
2026-01-04 22:05:39,874: Running test after training batch: 8000
2026-01-04 22:05:39,971: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:05:44,783: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-04 22:05:44,812: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-04 22:05:46,428: Val batch 8000: PER (avg): 0.1830 CTC Loss (avg): 18.1120 WER(1gram): 53.05% (n=64) time: 6.554
2026-01-04 22:05:46,428: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 22:05:46,429: t15.2023.08.13 val PER: 0.1362
2026-01-04 22:05:46,429: t15.2023.08.18 val PER: 0.1274
2026-01-04 22:05:46,429: t15.2023.08.20 val PER: 0.1422
2026-01-04 22:05:46,429: t15.2023.08.25 val PER: 0.1054
2026-01-04 22:05:46,429: t15.2023.08.27 val PER: 0.2122
2026-01-04 22:05:46,429: t15.2023.09.01 val PER: 0.1006
2026-01-04 22:05:46,429: t15.2023.09.03 val PER: 0.1924
2026-01-04 22:05:46,429: t15.2023.09.24 val PER: 0.1517
2026-01-04 22:05:46,429: t15.2023.09.29 val PER: 0.1493
2026-01-04 22:05:46,429: t15.2023.10.01 val PER: 0.2028
2026-01-04 22:05:46,429: t15.2023.10.06 val PER: 0.1087
2026-01-04 22:05:46,429: t15.2023.10.08 val PER: 0.2801
2026-01-04 22:05:46,429: t15.2023.10.13 val PER: 0.2459
2026-01-04 22:05:46,430: t15.2023.10.15 val PER: 0.1793
2026-01-04 22:05:46,430: t15.2023.10.20 val PER: 0.1913
2026-01-04 22:05:46,430: t15.2023.10.22 val PER: 0.1336
2026-01-04 22:05:46,430: t15.2023.11.03 val PER: 0.2062
2026-01-04 22:05:46,430: t15.2023.11.04 val PER: 0.0375
2026-01-04 22:05:46,430: t15.2023.11.17 val PER: 0.0653
2026-01-04 22:05:46,430: t15.2023.11.19 val PER: 0.0579
2026-01-04 22:05:46,430: t15.2023.11.26 val PER: 0.1877
2026-01-04 22:05:46,430: t15.2023.12.03 val PER: 0.1565
2026-01-04 22:05:46,430: t15.2023.12.08 val PER: 0.1425
2026-01-04 22:05:46,430: t15.2023.12.10 val PER: 0.1275
2026-01-04 22:05:46,430: t15.2023.12.17 val PER: 0.1736
2026-01-04 22:05:46,430: t15.2023.12.29 val PER: 0.1730
2026-01-04 22:05:46,430: t15.2024.02.25 val PER: 0.1376
2026-01-04 22:05:46,430: t15.2024.03.08 val PER: 0.2560
2026-01-04 22:05:46,431: t15.2024.03.15 val PER: 0.2351
2026-01-04 22:05:46,431: t15.2024.03.17 val PER: 0.1688
2026-01-04 22:05:46,431: t15.2024.05.10 val PER: 0.1976
2026-01-04 22:05:46,431: t15.2024.06.14 val PER: 0.2145
2026-01-04 22:05:46,431: t15.2024.07.19 val PER: 0.2821
2026-01-04 22:05:46,431: t15.2024.07.21 val PER: 0.1200
2026-01-04 22:05:46,431: t15.2024.07.28 val PER: 0.1625
2026-01-04 22:05:46,431: t15.2025.01.10 val PER: 0.3333
2026-01-04 22:05:46,431: t15.2025.01.12 val PER: 0.1824
2026-01-04 22:05:46,431: t15.2025.03.14 val PER: 0.3506
2026-01-04 22:05:46,431: t15.2025.03.16 val PER: 0.2317
2026-01-04 22:05:46,432: t15.2025.03.30 val PER: 0.3437
2026-01-04 22:05:46,432: t15.2025.04.13 val PER: 0.2568
2026-01-04 22:05:46,688: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_8000
2026-01-04 22:06:04,452: Train batch 8200: loss: 9.47 grad norm: 46.86 time: 0.054
2026-01-04 22:06:22,334: Train batch 8400: loss: 9.93 grad norm: 46.50 time: 0.063
2026-01-04 22:06:31,496: Running test after training batch: 8500
2026-01-04 22:06:31,610: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:06:36,751: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 22:06:36,781: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ent
2026-01-04 22:06:38,439: Val batch 8500: PER (avg): 0.1761 CTC Loss (avg): 17.5601 WER(1gram): 52.03% (n=64) time: 6.943
2026-01-04 22:06:38,440: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 22:06:38,440: t15.2023.08.13 val PER: 0.1445
2026-01-04 22:06:38,440: t15.2023.08.18 val PER: 0.1324
2026-01-04 22:06:38,440: t15.2023.08.20 val PER: 0.1358
2026-01-04 22:06:38,440: t15.2023.08.25 val PER: 0.1114
2026-01-04 22:06:38,440: t15.2023.08.27 val PER: 0.1929
2026-01-04 22:06:38,440: t15.2023.09.01 val PER: 0.0990
2026-01-04 22:06:38,440: t15.2023.09.03 val PER: 0.1853
2026-01-04 22:06:38,441: t15.2023.09.24 val PER: 0.1541
2026-01-04 22:06:38,441: t15.2023.09.29 val PER: 0.1461
2026-01-04 22:06:38,441: t15.2023.10.01 val PER: 0.1962
2026-01-04 22:06:38,441: t15.2023.10.06 val PER: 0.1055
2026-01-04 22:06:38,441: t15.2023.10.08 val PER: 0.2666
2026-01-04 22:06:38,441: t15.2023.10.13 val PER: 0.2335
2026-01-04 22:06:38,441: t15.2023.10.15 val PER: 0.1668
2026-01-04 22:06:38,441: t15.2023.10.20 val PER: 0.1846
2026-01-04 22:06:38,441: t15.2023.10.22 val PER: 0.1437
2026-01-04 22:06:38,441: t15.2023.11.03 val PER: 0.1961
2026-01-04 22:06:38,441: t15.2023.11.04 val PER: 0.0478
2026-01-04 22:06:38,441: t15.2023.11.17 val PER: 0.0544
2026-01-04 22:06:38,441: t15.2023.11.19 val PER: 0.0539
2026-01-04 22:06:38,441: t15.2023.11.26 val PER: 0.1746
2026-01-04 22:06:38,441: t15.2023.12.03 val PER: 0.1376
2026-01-04 22:06:38,441: t15.2023.12.08 val PER: 0.1305
2026-01-04 22:06:38,442: t15.2023.12.10 val PER: 0.1012
2026-01-04 22:06:38,442: t15.2023.12.17 val PER: 0.1705
2026-01-04 22:06:38,442: t15.2023.12.29 val PER: 0.1647
2026-01-04 22:06:38,442: t15.2024.02.25 val PER: 0.1433
2026-01-04 22:06:38,442: t15.2024.03.08 val PER: 0.2518
2026-01-04 22:06:38,442: t15.2024.03.15 val PER: 0.2295
2026-01-04 22:06:38,442: t15.2024.03.17 val PER: 0.1709
2026-01-04 22:06:38,442: t15.2024.05.10 val PER: 0.1813
2026-01-04 22:06:38,442: t15.2024.06.14 val PER: 0.1924
2026-01-04 22:06:38,442: t15.2024.07.19 val PER: 0.2630
2026-01-04 22:06:38,442: t15.2024.07.21 val PER: 0.1131
2026-01-04 22:06:38,443: t15.2024.07.28 val PER: 0.1676
2026-01-04 22:06:38,443: t15.2025.01.10 val PER: 0.3264
2026-01-04 22:06:38,443: t15.2025.01.12 val PER: 0.1794
2026-01-04 22:06:38,443: t15.2025.03.14 val PER: 0.3698
2026-01-04 22:06:38,443: t15.2025.03.16 val PER: 0.2107
2026-01-04 22:06:38,443: t15.2025.03.30 val PER: 0.3161
2026-01-04 22:06:38,443: t15.2025.04.13 val PER: 0.2368
2026-01-04 22:06:38,695: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_8500
2026-01-04 22:06:47,586: Train batch 8600: loss: 14.91 grad norm: 57.42 time: 0.054
2026-01-04 22:07:05,300: Train batch 8800: loss: 15.02 grad norm: 59.37 time: 0.060
2026-01-04 22:07:23,258: Train batch 9000: loss: 15.57 grad norm: 65.72 time: 0.072
2026-01-04 22:07:23,258: Running test after training batch: 9000
2026-01-04 22:07:23,358: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:07:28,183: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 22:07:28,213: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 22:07:29,853: Val batch 9000: PER (avg): 0.1746 CTC Loss (avg): 17.3577 WER(1gram): 53.05% (n=64) time: 6.594
2026-01-04 22:07:29,853: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-04 22:07:29,854: t15.2023.08.13 val PER: 0.1341
2026-01-04 22:07:29,854: t15.2023.08.18 val PER: 0.1257
2026-01-04 22:07:29,854: t15.2023.08.20 val PER: 0.1326
2026-01-04 22:07:29,854: t15.2023.08.25 val PER: 0.0964
2026-01-04 22:07:29,854: t15.2023.08.27 val PER: 0.1961
2026-01-04 22:07:29,855: t15.2023.09.01 val PER: 0.0869
2026-01-04 22:07:29,855: t15.2023.09.03 val PER: 0.1853
2026-01-04 22:07:29,855: t15.2023.09.24 val PER: 0.1468
2026-01-04 22:07:29,855: t15.2023.09.29 val PER: 0.1423
2026-01-04 22:07:29,855: t15.2023.10.01 val PER: 0.1863
2026-01-04 22:07:29,855: t15.2023.10.06 val PER: 0.1012
2026-01-04 22:07:29,856: t15.2023.10.08 val PER: 0.2652
2026-01-04 22:07:29,856: t15.2023.10.13 val PER: 0.2327
2026-01-04 22:07:29,856: t15.2023.10.15 val PER: 0.1714
2026-01-04 22:07:29,856: t15.2023.10.20 val PER: 0.2047
2026-01-04 22:07:29,856: t15.2023.10.22 val PER: 0.1336
2026-01-04 22:07:29,856: t15.2023.11.03 val PER: 0.2062
2026-01-04 22:07:29,856: t15.2023.11.04 val PER: 0.0375
2026-01-04 22:07:29,856: t15.2023.11.17 val PER: 0.0638
2026-01-04 22:07:29,856: t15.2023.11.19 val PER: 0.0499
2026-01-04 22:07:29,857: t15.2023.11.26 val PER: 0.1717
2026-01-04 22:07:29,857: t15.2023.12.03 val PER: 0.1429
2026-01-04 22:07:29,857: t15.2023.12.08 val PER: 0.1312
2026-01-04 22:07:29,857: t15.2023.12.10 val PER: 0.1091
2026-01-04 22:07:29,857: t15.2023.12.17 val PER: 0.1590
2026-01-04 22:07:29,857: t15.2023.12.29 val PER: 0.1640
2026-01-04 22:07:29,857: t15.2024.02.25 val PER: 0.1348
2026-01-04 22:07:29,857: t15.2024.03.08 val PER: 0.2603
2026-01-04 22:07:29,857: t15.2024.03.15 val PER: 0.2289
2026-01-04 22:07:29,857: t15.2024.03.17 val PER: 0.1715
2026-01-04 22:07:29,857: t15.2024.05.10 val PER: 0.1961
2026-01-04 22:07:29,857: t15.2024.06.14 val PER: 0.1924
2026-01-04 22:07:29,858: t15.2024.07.19 val PER: 0.2703
2026-01-04 22:07:29,858: t15.2024.07.21 val PER: 0.1145
2026-01-04 22:07:29,858: t15.2024.07.28 val PER: 0.1507
2026-01-04 22:07:29,858: t15.2025.01.10 val PER: 0.3168
2026-01-04 22:07:29,858: t15.2025.01.12 val PER: 0.1740
2026-01-04 22:07:29,858: t15.2025.03.14 val PER: 0.3595
2026-01-04 22:07:29,858: t15.2025.03.16 val PER: 0.2277
2026-01-04 22:07:29,858: t15.2025.03.30 val PER: 0.3241
2026-01-04 22:07:29,858: t15.2025.04.13 val PER: 0.2397
2026-01-04 22:07:30,131: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_9000
2026-01-04 22:07:48,712: Train batch 9200: loss: 10.55 grad norm: 49.18 time: 0.056
2026-01-04 22:08:07,087: Train batch 9400: loss: 7.67 grad norm: 47.94 time: 0.067
2026-01-04 22:08:16,177: Running test after training batch: 9500
2026-01-04 22:08:16,282: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:08:21,269: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:08:21,299: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 22:08:22,931: Val batch 9500: PER (avg): 0.1719 CTC Loss (avg): 17.2265 WER(1gram): 49.24% (n=64) time: 6.754
2026-01-04 22:08:22,932: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 22:08:22,932: t15.2023.08.13 val PER: 0.1289
2026-01-04 22:08:22,932: t15.2023.08.18 val PER: 0.1257
2026-01-04 22:08:22,932: t15.2023.08.20 val PER: 0.1279
2026-01-04 22:08:22,932: t15.2023.08.25 val PER: 0.1054
2026-01-04 22:08:22,932: t15.2023.08.27 val PER: 0.2138
2026-01-04 22:08:22,932: t15.2023.09.01 val PER: 0.0917
2026-01-04 22:08:22,932: t15.2023.09.03 val PER: 0.1722
2026-01-04 22:08:22,932: t15.2023.09.24 val PER: 0.1347
2026-01-04 22:08:22,933: t15.2023.09.29 val PER: 0.1468
2026-01-04 22:08:22,933: t15.2023.10.01 val PER: 0.1935
2026-01-04 22:08:22,933: t15.2023.10.06 val PER: 0.0990
2026-01-04 22:08:22,933: t15.2023.10.08 val PER: 0.2612
2026-01-04 22:08:22,933: t15.2023.10.13 val PER: 0.2281
2026-01-04 22:08:22,933: t15.2023.10.15 val PER: 0.1760
2026-01-04 22:08:22,933: t15.2023.10.20 val PER: 0.1779
2026-01-04 22:08:22,933: t15.2023.10.22 val PER: 0.1325
2026-01-04 22:08:22,933: t15.2023.11.03 val PER: 0.1954
2026-01-04 22:08:22,933: t15.2023.11.04 val PER: 0.0375
2026-01-04 22:08:22,933: t15.2023.11.17 val PER: 0.0467
2026-01-04 22:08:22,933: t15.2023.11.19 val PER: 0.0539
2026-01-04 22:08:22,933: t15.2023.11.26 val PER: 0.1572
2026-01-04 22:08:22,933: t15.2023.12.03 val PER: 0.1418
2026-01-04 22:08:22,933: t15.2023.12.08 val PER: 0.1338
2026-01-04 22:08:22,934: t15.2023.12.10 val PER: 0.1078
2026-01-04 22:08:22,934: t15.2023.12.17 val PER: 0.1570
2026-01-04 22:08:22,934: t15.2023.12.29 val PER: 0.1531
2026-01-04 22:08:22,934: t15.2024.02.25 val PER: 0.1362
2026-01-04 22:08:22,934: t15.2024.03.08 val PER: 0.2546
2026-01-04 22:08:22,934: t15.2024.03.15 val PER: 0.2245
2026-01-04 22:08:22,934: t15.2024.03.17 val PER: 0.1646
2026-01-04 22:08:22,934: t15.2024.05.10 val PER: 0.1902
2026-01-04 22:08:22,934: t15.2024.06.14 val PER: 0.1845
2026-01-04 22:08:22,934: t15.2024.07.19 val PER: 0.2643
2026-01-04 22:08:22,934: t15.2024.07.21 val PER: 0.1193
2026-01-04 22:08:22,934: t15.2024.07.28 val PER: 0.1463
2026-01-04 22:08:22,934: t15.2025.01.10 val PER: 0.3140
2026-01-04 22:08:22,934: t15.2025.01.12 val PER: 0.1801
2026-01-04 22:08:22,934: t15.2025.03.14 val PER: 0.3757
2026-01-04 22:08:22,935: t15.2025.03.16 val PER: 0.2107
2026-01-04 22:08:22,935: t15.2025.03.30 val PER: 0.3115
2026-01-04 22:08:22,935: t15.2025.04.13 val PER: 0.2382
2026-01-04 22:08:22,936: New best val WER(1gram) 52.03% --> 49.24%
2026-01-04 22:08:22,936: Checkpointing model
2026-01-04 22:08:23,548: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:08:23,836: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_9500
2026-01-04 22:08:32,882: Train batch 9600: loss: 8.19 grad norm: 46.72 time: 0.074
2026-01-04 22:08:51,302: Train batch 9800: loss: 12.30 grad norm: 59.79 time: 0.063
2026-01-04 22:09:09,282: Train batch 10000: loss: 5.08 grad norm: 35.41 time: 0.061
2026-01-04 22:09:09,282: Running test after training batch: 10000
2026-01-04 22:09:09,419: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:09:14,217: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 22:09:14,249: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sit
2026-01-04 22:09:15,935: Val batch 10000: PER (avg): 0.1695 CTC Loss (avg): 16.7876 WER(1gram): 52.79% (n=64) time: 6.653
2026-01-04 22:09:15,936: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 22:09:15,936: t15.2023.08.13 val PER: 0.1289
2026-01-04 22:09:15,936: t15.2023.08.18 val PER: 0.1241
2026-01-04 22:09:15,936: t15.2023.08.20 val PER: 0.1271
2026-01-04 22:09:15,936: t15.2023.08.25 val PER: 0.1039
2026-01-04 22:09:15,936: t15.2023.08.27 val PER: 0.2058
2026-01-04 22:09:15,936: t15.2023.09.01 val PER: 0.0860
2026-01-04 22:09:15,936: t15.2023.09.03 val PER: 0.1817
2026-01-04 22:09:15,936: t15.2023.09.24 val PER: 0.1408
2026-01-04 22:09:15,936: t15.2023.09.29 val PER: 0.1512
2026-01-04 22:09:15,940: t15.2023.10.01 val PER: 0.1823
2026-01-04 22:09:15,940: t15.2023.10.06 val PER: 0.0990
2026-01-04 22:09:15,940: t15.2023.10.08 val PER: 0.2558
2026-01-04 22:09:15,940: t15.2023.10.13 val PER: 0.2227
2026-01-04 22:09:15,940: t15.2023.10.15 val PER: 0.1641
2026-01-04 22:09:15,940: t15.2023.10.20 val PER: 0.1846
2026-01-04 22:09:15,940: t15.2023.10.22 val PER: 0.1236
2026-01-04 22:09:15,940: t15.2023.11.03 val PER: 0.1913
2026-01-04 22:09:15,940: t15.2023.11.04 val PER: 0.0307
2026-01-04 22:09:15,941: t15.2023.11.17 val PER: 0.0529
2026-01-04 22:09:15,941: t15.2023.11.19 val PER: 0.0439
2026-01-04 22:09:15,941: t15.2023.11.26 val PER: 0.1478
2026-01-04 22:09:15,941: t15.2023.12.03 val PER: 0.1387
2026-01-04 22:09:15,941: t15.2023.12.08 val PER: 0.1252
2026-01-04 22:09:15,941: t15.2023.12.10 val PER: 0.1170
2026-01-04 22:09:15,941: t15.2023.12.17 val PER: 0.1590
2026-01-04 22:09:15,941: t15.2023.12.29 val PER: 0.1544
2026-01-04 22:09:15,941: t15.2024.02.25 val PER: 0.1390
2026-01-04 22:09:15,941: t15.2024.03.08 val PER: 0.2504
2026-01-04 22:09:15,941: t15.2024.03.15 val PER: 0.2245
2026-01-04 22:09:15,941: t15.2024.03.17 val PER: 0.1604
2026-01-04 22:09:15,942: t15.2024.05.10 val PER: 0.1798
2026-01-04 22:09:15,942: t15.2024.06.14 val PER: 0.1877
2026-01-04 22:09:15,942: t15.2024.07.19 val PER: 0.2597
2026-01-04 22:09:15,942: t15.2024.07.21 val PER: 0.1179
2026-01-04 22:09:15,942: t15.2024.07.28 val PER: 0.1588
2026-01-04 22:09:15,942: t15.2025.01.10 val PER: 0.3099
2026-01-04 22:09:15,942: t15.2025.01.12 val PER: 0.1763
2026-01-04 22:09:15,942: t15.2025.03.14 val PER: 0.3506
2026-01-04 22:09:15,942: t15.2025.03.16 val PER: 0.2107
2026-01-04 22:09:15,942: t15.2025.03.30 val PER: 0.3184
2026-01-04 22:09:15,942: t15.2025.04.13 val PER: 0.2368
2026-01-04 22:09:16,216: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_10000
2026-01-04 22:09:34,076: Train batch 10200: loss: 5.93 grad norm: 38.53 time: 0.050
2026-01-04 22:09:52,439: Train batch 10400: loss: 9.06 grad norm: 52.13 time: 0.073
2026-01-04 22:10:01,642: Running test after training batch: 10500
2026-01-04 22:10:01,743: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:10:06,607: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:10:06,636: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-04 22:10:08,331: Val batch 10500: PER (avg): 0.1661 CTC Loss (avg): 16.6530 WER(1gram): 50.00% (n=64) time: 6.689
2026-01-04 22:10:08,332: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 22:10:08,332: t15.2023.08.13 val PER: 0.1216
2026-01-04 22:10:08,332: t15.2023.08.18 val PER: 0.1140
2026-01-04 22:10:08,332: t15.2023.08.20 val PER: 0.1279
2026-01-04 22:10:08,332: t15.2023.08.25 val PER: 0.1009
2026-01-04 22:10:08,332: t15.2023.08.27 val PER: 0.2074
2026-01-04 22:10:08,333: t15.2023.09.01 val PER: 0.0869
2026-01-04 22:10:08,333: t15.2023.09.03 val PER: 0.1924
2026-01-04 22:10:08,333: t15.2023.09.24 val PER: 0.1420
2026-01-04 22:10:08,333: t15.2023.09.29 val PER: 0.1436
2026-01-04 22:10:08,333: t15.2023.10.01 val PER: 0.1882
2026-01-04 22:10:08,333: t15.2023.10.06 val PER: 0.0958
2026-01-04 22:10:08,333: t15.2023.10.08 val PER: 0.2355
2026-01-04 22:10:08,333: t15.2023.10.13 val PER: 0.2141
2026-01-04 22:10:08,333: t15.2023.10.15 val PER: 0.1813
2026-01-04 22:10:08,333: t15.2023.10.20 val PER: 0.1812
2026-01-04 22:10:08,333: t15.2023.10.22 val PER: 0.1158
2026-01-04 22:10:08,334: t15.2023.11.03 val PER: 0.1947
2026-01-04 22:10:08,334: t15.2023.11.04 val PER: 0.0341
2026-01-04 22:10:08,334: t15.2023.11.17 val PER: 0.0529
2026-01-04 22:10:08,334: t15.2023.11.19 val PER: 0.0539
2026-01-04 22:10:08,334: t15.2023.11.26 val PER: 0.1406
2026-01-04 22:10:08,334: t15.2023.12.03 val PER: 0.1376
2026-01-04 22:10:08,334: t15.2023.12.08 val PER: 0.1198
2026-01-04 22:10:08,334: t15.2023.12.10 val PER: 0.1078
2026-01-04 22:10:08,334: t15.2023.12.17 val PER: 0.1507
2026-01-04 22:10:08,334: t15.2023.12.29 val PER: 0.1524
2026-01-04 22:10:08,334: t15.2024.02.25 val PER: 0.1292
2026-01-04 22:10:08,334: t15.2024.03.08 val PER: 0.2461
2026-01-04 22:10:08,334: t15.2024.03.15 val PER: 0.2226
2026-01-04 22:10:08,334: t15.2024.03.17 val PER: 0.1562
2026-01-04 22:10:08,334: t15.2024.05.10 val PER: 0.1828
2026-01-04 22:10:08,334: t15.2024.06.14 val PER: 0.1782
2026-01-04 22:10:08,335: t15.2024.07.19 val PER: 0.2498
2026-01-04 22:10:08,335: t15.2024.07.21 val PER: 0.1083
2026-01-04 22:10:08,335: t15.2024.07.28 val PER: 0.1434
2026-01-04 22:10:08,335: t15.2025.01.10 val PER: 0.3127
2026-01-04 22:10:08,335: t15.2025.01.12 val PER: 0.1671
2026-01-04 22:10:08,335: t15.2025.03.14 val PER: 0.3609
2026-01-04 22:10:08,335: t15.2025.03.16 val PER: 0.2016
2026-01-04 22:10:08,335: t15.2025.03.30 val PER: 0.3103
2026-01-04 22:10:08,335: t15.2025.04.13 val PER: 0.2297
2026-01-04 22:10:08,616: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_10500
2026-01-04 22:10:18,064: Train batch 10600: loss: 8.59 grad norm: 54.33 time: 0.073
2026-01-04 22:10:36,205: Train batch 10800: loss: 14.31 grad norm: 64.61 time: 0.064
2026-01-04 22:10:54,490: Train batch 11000: loss: 13.61 grad norm: 59.71 time: 0.057
2026-01-04 22:10:54,491: Running test after training batch: 11000
2026-01-04 22:10:54,597: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:10:59,468: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:10:59,498: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-04 22:11:01,203: Val batch 11000: PER (avg): 0.1621 CTC Loss (avg): 16.2989 WER(1gram): 50.51% (n=64) time: 6.712
2026-01-04 22:11:01,203: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=11
2026-01-04 22:11:01,203: t15.2023.08.13 val PER: 0.1185
2026-01-04 22:11:01,203: t15.2023.08.18 val PER: 0.1266
2026-01-04 22:11:01,204: t15.2023.08.20 val PER: 0.1239
2026-01-04 22:11:01,204: t15.2023.08.25 val PER: 0.0904
2026-01-04 22:11:01,204: t15.2023.08.27 val PER: 0.1929
2026-01-04 22:11:01,204: t15.2023.09.01 val PER: 0.0795
2026-01-04 22:11:01,204: t15.2023.09.03 val PER: 0.1853
2026-01-04 22:11:01,204: t15.2023.09.24 val PER: 0.1456
2026-01-04 22:11:01,204: t15.2023.09.29 val PER: 0.1429
2026-01-04 22:11:01,204: t15.2023.10.01 val PER: 0.1942
2026-01-04 22:11:01,204: t15.2023.10.06 val PER: 0.0904
2026-01-04 22:11:01,204: t15.2023.10.08 val PER: 0.2463
2026-01-04 22:11:01,205: t15.2023.10.13 val PER: 0.2180
2026-01-04 22:11:01,205: t15.2023.10.15 val PER: 0.1628
2026-01-04 22:11:01,205: t15.2023.10.20 val PER: 0.1913
2026-01-04 22:11:01,205: t15.2023.10.22 val PER: 0.1180
2026-01-04 22:11:01,205: t15.2023.11.03 val PER: 0.1927
2026-01-04 22:11:01,205: t15.2023.11.04 val PER: 0.0410
2026-01-04 22:11:01,205: t15.2023.11.17 val PER: 0.0498
2026-01-04 22:11:01,205: t15.2023.11.19 val PER: 0.0419
2026-01-04 22:11:01,205: t15.2023.11.26 val PER: 0.1399
2026-01-04 22:11:01,205: t15.2023.12.03 val PER: 0.1303
2026-01-04 22:11:01,206: t15.2023.12.08 val PER: 0.1165
2026-01-04 22:11:01,206: t15.2023.12.10 val PER: 0.0972
2026-01-04 22:11:01,206: t15.2023.12.17 val PER: 0.1455
2026-01-04 22:11:01,206: t15.2023.12.29 val PER: 0.1400
2026-01-04 22:11:01,206: t15.2024.02.25 val PER: 0.1306
2026-01-04 22:11:01,206: t15.2024.03.08 val PER: 0.2304
2026-01-04 22:11:01,206: t15.2024.03.15 val PER: 0.2133
2026-01-04 22:11:01,206: t15.2024.03.17 val PER: 0.1541
2026-01-04 22:11:01,206: t15.2024.05.10 val PER: 0.1724
2026-01-04 22:11:01,206: t15.2024.06.14 val PER: 0.1845
2026-01-04 22:11:01,206: t15.2024.07.19 val PER: 0.2452
2026-01-04 22:11:01,206: t15.2024.07.21 val PER: 0.1097
2026-01-04 22:11:01,206: t15.2024.07.28 val PER: 0.1353
2026-01-04 22:11:01,206: t15.2025.01.10 val PER: 0.3113
2026-01-04 22:11:01,206: t15.2025.01.12 val PER: 0.1617
2026-01-04 22:11:01,206: t15.2025.03.14 val PER: 0.3462
2026-01-04 22:11:01,207: t15.2025.03.16 val PER: 0.1924
2026-01-04 22:11:01,207: t15.2025.03.30 val PER: 0.3023
2026-01-04 22:11:01,207: t15.2025.04.13 val PER: 0.2197
2026-01-04 22:11:01,490: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_11000
2026-01-04 22:11:19,255: Train batch 11200: loss: 10.43 grad norm: 53.50 time: 0.071
2026-01-04 22:11:36,966: Train batch 11400: loss: 9.34 grad norm: 53.14 time: 0.057
2026-01-04 22:11:45,951: Running test after training batch: 11500
2026-01-04 22:11:46,062: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:11:50,858: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 22:11:50,889: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-04 22:11:52,584: Val batch 11500: PER (avg): 0.1589 CTC Loss (avg): 16.2676 WER(1gram): 49.49% (n=64) time: 6.633
2026-01-04 22:11:52,584: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 22:11:52,584: t15.2023.08.13 val PER: 0.1237
2026-01-04 22:11:52,585: t15.2023.08.18 val PER: 0.1123
2026-01-04 22:11:52,585: t15.2023.08.20 val PER: 0.1168
2026-01-04 22:11:52,585: t15.2023.08.25 val PER: 0.0994
2026-01-04 22:11:52,585: t15.2023.08.27 val PER: 0.1977
2026-01-04 22:11:52,585: t15.2023.09.01 val PER: 0.0812
2026-01-04 22:11:52,585: t15.2023.09.03 val PER: 0.1770
2026-01-04 22:11:52,585: t15.2023.09.24 val PER: 0.1347
2026-01-04 22:11:52,585: t15.2023.09.29 val PER: 0.1423
2026-01-04 22:11:52,585: t15.2023.10.01 val PER: 0.1869
2026-01-04 22:11:52,585: t15.2023.10.06 val PER: 0.0840
2026-01-04 22:11:52,586: t15.2023.10.08 val PER: 0.2585
2026-01-04 22:11:52,586: t15.2023.10.13 val PER: 0.2141
2026-01-04 22:11:52,586: t15.2023.10.15 val PER: 0.1608
2026-01-04 22:11:52,586: t15.2023.10.20 val PER: 0.1879
2026-01-04 22:11:52,586: t15.2023.10.22 val PER: 0.1225
2026-01-04 22:11:52,586: t15.2023.11.03 val PER: 0.1811
2026-01-04 22:11:52,586: t15.2023.11.04 val PER: 0.0307
2026-01-04 22:11:52,586: t15.2023.11.17 val PER: 0.0451
2026-01-04 22:11:52,586: t15.2023.11.19 val PER: 0.0479
2026-01-04 22:11:52,586: t15.2023.11.26 val PER: 0.1290
2026-01-04 22:11:52,586: t15.2023.12.03 val PER: 0.1208
2026-01-04 22:11:52,586: t15.2023.12.08 val PER: 0.1099
2026-01-04 22:11:52,586: t15.2023.12.10 val PER: 0.0999
2026-01-04 22:11:52,586: t15.2023.12.17 val PER: 0.1476
2026-01-04 22:11:52,586: t15.2023.12.29 val PER: 0.1359
2026-01-04 22:11:52,586: t15.2024.02.25 val PER: 0.1166
2026-01-04 22:11:52,587: t15.2024.03.08 val PER: 0.2333
2026-01-04 22:11:52,587: t15.2024.03.15 val PER: 0.2145
2026-01-04 22:11:52,587: t15.2024.03.17 val PER: 0.1513
2026-01-04 22:11:52,587: t15.2024.05.10 val PER: 0.1709
2026-01-04 22:11:52,587: t15.2024.06.14 val PER: 0.1751
2026-01-04 22:11:52,587: t15.2024.07.19 val PER: 0.2446
2026-01-04 22:11:52,587: t15.2024.07.21 val PER: 0.0986
2026-01-04 22:11:52,587: t15.2024.07.28 val PER: 0.1404
2026-01-04 22:11:52,587: t15.2025.01.10 val PER: 0.3154
2026-01-04 22:11:52,587: t15.2025.01.12 val PER: 0.1563
2026-01-04 22:11:52,587: t15.2025.03.14 val PER: 0.3373
2026-01-04 22:11:52,587: t15.2025.03.16 val PER: 0.2029
2026-01-04 22:11:52,587: t15.2025.03.30 val PER: 0.3046
2026-01-04 22:11:52,587: t15.2025.04.13 val PER: 0.2111
2026-01-04 22:11:52,869: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_11500
2026-01-04 22:12:01,764: Train batch 11600: loss: 10.62 grad norm: 50.05 time: 0.060
2026-01-04 22:12:20,069: Train batch 11800: loss: 6.31 grad norm: 41.70 time: 0.044
2026-01-04 22:12:38,312: Train batch 12000: loss: 12.94 grad norm: 55.82 time: 0.072
2026-01-04 22:12:38,312: Running test after training batch: 12000
2026-01-04 22:12:38,420: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:12:43,222: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 22:12:43,253: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 22:12:44,970: Val batch 12000: PER (avg): 0.1585 CTC Loss (avg): 16.1203 WER(1gram): 52.79% (n=64) time: 6.658
2026-01-04 22:12:44,970: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 22:12:44,971: t15.2023.08.13 val PER: 0.1258
2026-01-04 22:12:44,971: t15.2023.08.18 val PER: 0.1098
2026-01-04 22:12:44,971: t15.2023.08.20 val PER: 0.1136
2026-01-04 22:12:44,971: t15.2023.08.25 val PER: 0.0979
2026-01-04 22:12:44,971: t15.2023.08.27 val PER: 0.1833
2026-01-04 22:12:44,971: t15.2023.09.01 val PER: 0.0860
2026-01-04 22:12:44,971: t15.2023.09.03 val PER: 0.1675
2026-01-04 22:12:44,971: t15.2023.09.24 val PER: 0.1359
2026-01-04 22:12:44,971: t15.2023.09.29 val PER: 0.1398
2026-01-04 22:12:44,971: t15.2023.10.01 val PER: 0.1797
2026-01-04 22:12:44,971: t15.2023.10.06 val PER: 0.0829
2026-01-04 22:12:44,971: t15.2023.10.08 val PER: 0.2585
2026-01-04 22:12:44,971: t15.2023.10.13 val PER: 0.2149
2026-01-04 22:12:44,971: t15.2023.10.15 val PER: 0.1635
2026-01-04 22:12:44,971: t15.2023.10.20 val PER: 0.1779
2026-01-04 22:12:44,972: t15.2023.10.22 val PER: 0.1147
2026-01-04 22:12:44,972: t15.2023.11.03 val PER: 0.1845
2026-01-04 22:12:44,972: t15.2023.11.04 val PER: 0.0478
2026-01-04 22:12:44,972: t15.2023.11.17 val PER: 0.0373
2026-01-04 22:12:44,972: t15.2023.11.19 val PER: 0.0419
2026-01-04 22:12:44,972: t15.2023.11.26 val PER: 0.1326
2026-01-04 22:12:44,972: t15.2023.12.03 val PER: 0.1250
2026-01-04 22:12:44,973: t15.2023.12.08 val PER: 0.1085
2026-01-04 22:12:44,973: t15.2023.12.10 val PER: 0.0959
2026-01-04 22:12:44,973: t15.2023.12.17 val PER: 0.1445
2026-01-04 22:12:44,973: t15.2023.12.29 val PER: 0.1386
2026-01-04 22:12:44,973: t15.2024.02.25 val PER: 0.1236
2026-01-04 22:12:44,973: t15.2024.03.08 val PER: 0.2518
2026-01-04 22:12:44,973: t15.2024.03.15 val PER: 0.2195
2026-01-04 22:12:44,973: t15.2024.03.17 val PER: 0.1409
2026-01-04 22:12:44,973: t15.2024.05.10 val PER: 0.1842
2026-01-04 22:12:44,973: t15.2024.06.14 val PER: 0.1845
2026-01-04 22:12:44,973: t15.2024.07.19 val PER: 0.2419
2026-01-04 22:12:44,973: t15.2024.07.21 val PER: 0.1069
2026-01-04 22:12:44,973: t15.2024.07.28 val PER: 0.1353
2026-01-04 22:12:44,973: t15.2025.01.10 val PER: 0.3085
2026-01-04 22:12:44,973: t15.2025.01.12 val PER: 0.1555
2026-01-04 22:12:44,973: t15.2025.03.14 val PER: 0.3476
2026-01-04 22:12:44,973: t15.2025.03.16 val PER: 0.1950
2026-01-04 22:12:44,974: t15.2025.03.30 val PER: 0.2943
2026-01-04 22:12:44,974: t15.2025.04.13 val PER: 0.2068
2026-01-04 22:12:45,252: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_12000
2026-01-04 22:13:03,443: Train batch 12200: loss: 5.79 grad norm: 41.84 time: 0.065
2026-01-04 22:13:21,978: Train batch 12400: loss: 4.67 grad norm: 35.93 time: 0.040
2026-01-04 22:13:31,288: Running test after training batch: 12500
2026-01-04 22:13:31,392: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:13:36,290: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 22:13:36,324: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-04 22:13:38,093: Val batch 12500: PER (avg): 0.1552 CTC Loss (avg): 15.9428 WER(1gram): 48.98% (n=64) time: 6.804
2026-01-04 22:13:38,093: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 22:13:38,093: t15.2023.08.13 val PER: 0.1227
2026-01-04 22:13:38,094: t15.2023.08.18 val PER: 0.1140
2026-01-04 22:13:38,094: t15.2023.08.20 val PER: 0.1160
2026-01-04 22:13:38,094: t15.2023.08.25 val PER: 0.0949
2026-01-04 22:13:38,094: t15.2023.08.27 val PER: 0.1913
2026-01-04 22:13:38,094: t15.2023.09.01 val PER: 0.0869
2026-01-04 22:13:38,094: t15.2023.09.03 val PER: 0.1580
2026-01-04 22:13:38,094: t15.2023.09.24 val PER: 0.1323
2026-01-04 22:13:38,094: t15.2023.09.29 val PER: 0.1315
2026-01-04 22:13:38,095: t15.2023.10.01 val PER: 0.1757
2026-01-04 22:13:38,095: t15.2023.10.06 val PER: 0.0764
2026-01-04 22:13:38,095: t15.2023.10.08 val PER: 0.2530
2026-01-04 22:13:38,095: t15.2023.10.13 val PER: 0.2033
2026-01-04 22:13:38,095: t15.2023.10.15 val PER: 0.1589
2026-01-04 22:13:38,095: t15.2023.10.20 val PER: 0.1846
2026-01-04 22:13:38,095: t15.2023.10.22 val PER: 0.1091
2026-01-04 22:13:38,095: t15.2023.11.03 val PER: 0.1784
2026-01-04 22:13:38,095: t15.2023.11.04 val PER: 0.0341
2026-01-04 22:13:38,095: t15.2023.11.17 val PER: 0.0482
2026-01-04 22:13:38,095: t15.2023.11.19 val PER: 0.0319
2026-01-04 22:13:38,095: t15.2023.11.26 val PER: 0.1319
2026-01-04 22:13:38,096: t15.2023.12.03 val PER: 0.1145
2026-01-04 22:13:38,096: t15.2023.12.08 val PER: 0.0965
2026-01-04 22:13:38,096: t15.2023.12.10 val PER: 0.0880
2026-01-04 22:13:38,096: t15.2023.12.17 val PER: 0.1435
2026-01-04 22:13:38,096: t15.2023.12.29 val PER: 0.1448
2026-01-04 22:13:38,096: t15.2024.02.25 val PER: 0.1166
2026-01-04 22:13:38,096: t15.2024.03.08 val PER: 0.2347
2026-01-04 22:13:38,096: t15.2024.03.15 val PER: 0.2095
2026-01-04 22:13:38,096: t15.2024.03.17 val PER: 0.1360
2026-01-04 22:13:38,096: t15.2024.05.10 val PER: 0.1798
2026-01-04 22:13:38,096: t15.2024.06.14 val PER: 0.1703
2026-01-04 22:13:38,096: t15.2024.07.19 val PER: 0.2393
2026-01-04 22:13:38,097: t15.2024.07.21 val PER: 0.0952
2026-01-04 22:13:38,097: t15.2024.07.28 val PER: 0.1397
2026-01-04 22:13:38,097: t15.2025.01.10 val PER: 0.3099
2026-01-04 22:13:38,097: t15.2025.01.12 val PER: 0.1586
2026-01-04 22:13:38,097: t15.2025.03.14 val PER: 0.3609
2026-01-04 22:13:38,097: t15.2025.03.16 val PER: 0.1950
2026-01-04 22:13:38,097: t15.2025.03.30 val PER: 0.3057
2026-01-04 22:13:38,097: t15.2025.04.13 val PER: 0.2126
2026-01-04 22:13:38,098: New best val WER(1gram) 49.24% --> 48.98%
2026-01-04 22:13:38,098: Checkpointing model
2026-01-04 22:13:38,715: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:13:39,006: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_12500
2026-01-04 22:13:48,272: Train batch 12600: loss: 7.51 grad norm: 40.64 time: 0.057
2026-01-04 22:14:06,980: Train batch 12800: loss: 5.98 grad norm: 40.92 time: 0.052
2026-01-04 22:14:25,499: Train batch 13000: loss: 5.90 grad norm: 41.43 time: 0.067
2026-01-04 22:14:25,500: Running test after training batch: 13000
2026-01-04 22:14:25,623: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:14:30,446: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:14:30,480: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-04 22:14:32,252: Val batch 13000: PER (avg): 0.1535 CTC Loss (avg): 15.6974 WER(1gram): 48.73% (n=64) time: 6.752
2026-01-04 22:14:32,253: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 22:14:32,253: t15.2023.08.13 val PER: 0.1143
2026-01-04 22:14:32,253: t15.2023.08.18 val PER: 0.1115
2026-01-04 22:14:32,253: t15.2023.08.20 val PER: 0.1041
2026-01-04 22:14:32,253: t15.2023.08.25 val PER: 0.0843
2026-01-04 22:14:32,253: t15.2023.08.27 val PER: 0.1801
2026-01-04 22:14:32,253: t15.2023.09.01 val PER: 0.0787
2026-01-04 22:14:32,253: t15.2023.09.03 val PER: 0.1639
2026-01-04 22:14:32,253: t15.2023.09.24 val PER: 0.1323
2026-01-04 22:14:32,253: t15.2023.09.29 val PER: 0.1302
2026-01-04 22:14:32,253: t15.2023.10.01 val PER: 0.1724
2026-01-04 22:14:32,253: t15.2023.10.06 val PER: 0.0840
2026-01-04 22:14:32,254: t15.2023.10.08 val PER: 0.2571
2026-01-04 22:14:32,254: t15.2023.10.13 val PER: 0.1955
2026-01-04 22:14:32,254: t15.2023.10.15 val PER: 0.1582
2026-01-04 22:14:32,254: t15.2023.10.20 val PER: 0.1678
2026-01-04 22:14:32,254: t15.2023.10.22 val PER: 0.1147
2026-01-04 22:14:32,254: t15.2023.11.03 val PER: 0.1798
2026-01-04 22:14:32,254: t15.2023.11.04 val PER: 0.0341
2026-01-04 22:14:32,254: t15.2023.11.17 val PER: 0.0404
2026-01-04 22:14:32,254: t15.2023.11.19 val PER: 0.0499
2026-01-04 22:14:32,254: t15.2023.11.26 val PER: 0.1261
2026-01-04 22:14:32,254: t15.2023.12.03 val PER: 0.1176
2026-01-04 22:14:32,254: t15.2023.12.08 val PER: 0.1059
2026-01-04 22:14:32,254: t15.2023.12.10 val PER: 0.0972
2026-01-04 22:14:32,254: t15.2023.12.17 val PER: 0.1414
2026-01-04 22:14:32,255: t15.2023.12.29 val PER: 0.1386
2026-01-04 22:14:32,255: t15.2024.02.25 val PER: 0.1166
2026-01-04 22:14:32,255: t15.2024.03.08 val PER: 0.2304
2026-01-04 22:14:32,255: t15.2024.03.15 val PER: 0.2114
2026-01-04 22:14:32,255: t15.2024.03.17 val PER: 0.1464
2026-01-04 22:14:32,255: t15.2024.05.10 val PER: 0.1753
2026-01-04 22:14:32,255: t15.2024.06.14 val PER: 0.1703
2026-01-04 22:14:32,255: t15.2024.07.19 val PER: 0.2347
2026-01-04 22:14:32,255: t15.2024.07.21 val PER: 0.0966
2026-01-04 22:14:32,255: t15.2024.07.28 val PER: 0.1368
2026-01-04 22:14:32,255: t15.2025.01.10 val PER: 0.3154
2026-01-04 22:14:32,255: t15.2025.01.12 val PER: 0.1501
2026-01-04 22:14:32,255: t15.2025.03.14 val PER: 0.3343
2026-01-04 22:14:32,255: t15.2025.03.16 val PER: 0.1911
2026-01-04 22:14:32,255: t15.2025.03.30 val PER: 0.2943
2026-01-04 22:14:32,255: t15.2025.04.13 val PER: 0.2197
2026-01-04 22:14:32,257: New best val WER(1gram) 48.98% --> 48.73%
2026-01-04 22:14:32,257: Checkpointing model
2026-01-04 22:14:32,910: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:14:33,202: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_13000
2026-01-04 22:14:51,345: Train batch 13200: loss: 12.02 grad norm: 60.30 time: 0.054
2026-01-04 22:15:09,784: Train batch 13400: loss: 8.61 grad norm: 50.42 time: 0.062
2026-01-04 22:15:18,932: Running test after training batch: 13500
2026-01-04 22:15:19,036: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:15:23,932: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 22:15:23,965: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 22:15:25,753: Val batch 13500: PER (avg): 0.1523 CTC Loss (avg): 15.5266 WER(1gram): 50.00% (n=64) time: 6.821
2026-01-04 22:15:25,754: WER lens: avg_true_words=6.16 avg_pred_words=6.27 max_pred_words=12
2026-01-04 22:15:25,754: t15.2023.08.13 val PER: 0.1237
2026-01-04 22:15:25,754: t15.2023.08.18 val PER: 0.1090
2026-01-04 22:15:25,754: t15.2023.08.20 val PER: 0.1096
2026-01-04 22:15:25,754: t15.2023.08.25 val PER: 0.0904
2026-01-04 22:15:25,754: t15.2023.08.27 val PER: 0.1849
2026-01-04 22:15:25,754: t15.2023.09.01 val PER: 0.0779
2026-01-04 22:15:25,754: t15.2023.09.03 val PER: 0.1698
2026-01-04 22:15:25,755: t15.2023.09.24 val PER: 0.1323
2026-01-04 22:15:25,755: t15.2023.09.29 val PER: 0.1315
2026-01-04 22:15:25,755: t15.2023.10.01 val PER: 0.1724
2026-01-04 22:15:25,755: t15.2023.10.06 val PER: 0.0893
2026-01-04 22:15:25,755: t15.2023.10.08 val PER: 0.2530
2026-01-04 22:15:25,755: t15.2023.10.13 val PER: 0.2071
2026-01-04 22:15:25,755: t15.2023.10.15 val PER: 0.1543
2026-01-04 22:15:25,755: t15.2023.10.20 val PER: 0.1745
2026-01-04 22:15:25,756: t15.2023.10.22 val PER: 0.1125
2026-01-04 22:15:25,756: t15.2023.11.03 val PER: 0.1798
2026-01-04 22:15:25,756: t15.2023.11.04 val PER: 0.0444
2026-01-04 22:15:25,756: t15.2023.11.17 val PER: 0.0467
2026-01-04 22:15:25,756: t15.2023.11.19 val PER: 0.0359
2026-01-04 22:15:25,756: t15.2023.11.26 val PER: 0.1254
2026-01-04 22:15:25,756: t15.2023.12.03 val PER: 0.1134
2026-01-04 22:15:25,756: t15.2023.12.08 val PER: 0.1065
2026-01-04 22:15:25,756: t15.2023.12.10 val PER: 0.0959
2026-01-04 22:15:25,756: t15.2023.12.17 val PER: 0.1362
2026-01-04 22:15:25,756: t15.2023.12.29 val PER: 0.1290
2026-01-04 22:15:25,756: t15.2024.02.25 val PER: 0.1138
2026-01-04 22:15:25,756: t15.2024.03.08 val PER: 0.2404
2026-01-04 22:15:25,756: t15.2024.03.15 val PER: 0.2051
2026-01-04 22:15:25,756: t15.2024.03.17 val PER: 0.1346
2026-01-04 22:15:25,756: t15.2024.05.10 val PER: 0.1709
2026-01-04 22:15:25,757: t15.2024.06.14 val PER: 0.1577
2026-01-04 22:15:25,757: t15.2024.07.19 val PER: 0.2294
2026-01-04 22:15:25,757: t15.2024.07.21 val PER: 0.0952
2026-01-04 22:15:25,757: t15.2024.07.28 val PER: 0.1324
2026-01-04 22:15:25,757: t15.2025.01.10 val PER: 0.2961
2026-01-04 22:15:25,757: t15.2025.01.12 val PER: 0.1509
2026-01-04 22:15:25,757: t15.2025.03.14 val PER: 0.3328
2026-01-04 22:15:25,757: t15.2025.03.16 val PER: 0.1846
2026-01-04 22:15:25,757: t15.2025.03.30 val PER: 0.3034
2026-01-04 22:15:25,757: t15.2025.04.13 val PER: 0.2140
2026-01-04 22:15:26,048: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_13500
2026-01-04 22:15:35,182: Train batch 13600: loss: 12.32 grad norm: 66.61 time: 0.062
2026-01-04 22:15:53,883: Train batch 13800: loss: 8.41 grad norm: 56.08 time: 0.055
2026-01-04 22:16:12,212: Train batch 14000: loss: 11.37 grad norm: 58.32 time: 0.050
2026-01-04 22:16:12,212: Running test after training batch: 14000
2026-01-04 22:16:12,330: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:16:17,160: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:16:17,195: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-04 22:16:19,008: Val batch 14000: PER (avg): 0.1503 CTC Loss (avg): 15.4372 WER(1gram): 46.45% (n=64) time: 6.795
2026-01-04 22:16:19,008: WER lens: avg_true_words=6.16 avg_pred_words=6.25 max_pred_words=12
2026-01-04 22:16:19,008: t15.2023.08.13 val PER: 0.1164
2026-01-04 22:16:19,008: t15.2023.08.18 val PER: 0.0989
2026-01-04 22:16:19,008: t15.2023.08.20 val PER: 0.0977
2026-01-04 22:16:19,008: t15.2023.08.25 val PER: 0.0949
2026-01-04 22:16:19,008: t15.2023.08.27 val PER: 0.1801
2026-01-04 22:16:19,008: t15.2023.09.01 val PER: 0.0812
2026-01-04 22:16:19,008: t15.2023.09.03 val PER: 0.1758
2026-01-04 22:16:19,008: t15.2023.09.24 val PER: 0.1311
2026-01-04 22:16:19,008: t15.2023.09.29 val PER: 0.1295
2026-01-04 22:16:19,009: t15.2023.10.01 val PER: 0.1750
2026-01-04 22:16:19,009: t15.2023.10.06 val PER: 0.0829
2026-01-04 22:16:19,009: t15.2023.10.08 val PER: 0.2517
2026-01-04 22:16:19,009: t15.2023.10.13 val PER: 0.2056
2026-01-04 22:16:19,009: t15.2023.10.15 val PER: 0.1602
2026-01-04 22:16:19,009: t15.2023.10.20 val PER: 0.1779
2026-01-04 22:16:19,009: t15.2023.10.22 val PER: 0.1125
2026-01-04 22:16:19,009: t15.2023.11.03 val PER: 0.1744
2026-01-04 22:16:19,009: t15.2023.11.04 val PER: 0.0410
2026-01-04 22:16:19,010: t15.2023.11.17 val PER: 0.0451
2026-01-04 22:16:19,010: t15.2023.11.19 val PER: 0.0319
2026-01-04 22:16:19,010: t15.2023.11.26 val PER: 0.1268
2026-01-04 22:16:19,010: t15.2023.12.03 val PER: 0.1218
2026-01-04 22:16:19,010: t15.2023.12.08 val PER: 0.0999
2026-01-04 22:16:19,010: t15.2023.12.10 val PER: 0.0920
2026-01-04 22:16:19,010: t15.2023.12.17 val PER: 0.1331
2026-01-04 22:16:19,010: t15.2023.12.29 val PER: 0.1229
2026-01-04 22:16:19,010: t15.2024.02.25 val PER: 0.1138
2026-01-04 22:16:19,010: t15.2024.03.08 val PER: 0.2276
2026-01-04 22:16:19,010: t15.2024.03.15 val PER: 0.2008
2026-01-04 22:16:19,010: t15.2024.03.17 val PER: 0.1395
2026-01-04 22:16:19,010: t15.2024.05.10 val PER: 0.1664
2026-01-04 22:16:19,010: t15.2024.06.14 val PER: 0.1609
2026-01-04 22:16:19,010: t15.2024.07.19 val PER: 0.2248
2026-01-04 22:16:19,010: t15.2024.07.21 val PER: 0.0897
2026-01-04 22:16:19,010: t15.2024.07.28 val PER: 0.1353
2026-01-04 22:16:19,011: t15.2025.01.10 val PER: 0.2865
2026-01-04 22:16:19,011: t15.2025.01.12 val PER: 0.1532
2026-01-04 22:16:19,011: t15.2025.03.14 val PER: 0.3402
2026-01-04 22:16:19,011: t15.2025.03.16 val PER: 0.1819
2026-01-04 22:16:19,011: t15.2025.03.30 val PER: 0.2862
2026-01-04 22:16:19,011: t15.2025.04.13 val PER: 0.2211
2026-01-04 22:16:19,015: New best val WER(1gram) 48.73% --> 46.45%
2026-01-04 22:16:19,015: Checkpointing model
2026-01-04 22:16:19,630: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:16:19,936: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_14000
2026-01-04 22:16:38,070: Train batch 14200: loss: 7.98 grad norm: 50.68 time: 0.055
2026-01-04 22:16:56,500: Train batch 14400: loss: 5.58 grad norm: 38.62 time: 0.063
2026-01-04 22:17:06,041: Running test after training batch: 14500
2026-01-04 22:17:06,151: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:17:11,089: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:17:11,125: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 22:17:12,942: Val batch 14500: PER (avg): 0.1502 CTC Loss (avg): 15.5140 WER(1gram): 48.48% (n=64) time: 6.900
2026-01-04 22:17:12,942: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-04 22:17:12,943: t15.2023.08.13 val PER: 0.1091
2026-01-04 22:17:12,943: t15.2023.08.18 val PER: 0.1048
2026-01-04 22:17:12,943: t15.2023.08.20 val PER: 0.1041
2026-01-04 22:17:12,943: t15.2023.08.25 val PER: 0.0949
2026-01-04 22:17:12,943: t15.2023.08.27 val PER: 0.1913
2026-01-04 22:17:12,943: t15.2023.09.01 val PER: 0.0828
2026-01-04 22:17:12,943: t15.2023.09.03 val PER: 0.1639
2026-01-04 22:17:12,943: t15.2023.09.24 val PER: 0.1408
2026-01-04 22:17:12,943: t15.2023.09.29 val PER: 0.1251
2026-01-04 22:17:12,943: t15.2023.10.01 val PER: 0.1744
2026-01-04 22:17:12,943: t15.2023.10.06 val PER: 0.0883
2026-01-04 22:17:12,943: t15.2023.10.08 val PER: 0.2490
2026-01-04 22:17:12,943: t15.2023.10.13 val PER: 0.2033
2026-01-04 22:17:12,943: t15.2023.10.15 val PER: 0.1523
2026-01-04 22:17:12,943: t15.2023.10.20 val PER: 0.1779
2026-01-04 22:17:12,944: t15.2023.10.22 val PER: 0.1058
2026-01-04 22:17:12,944: t15.2023.11.03 val PER: 0.1737
2026-01-04 22:17:12,944: t15.2023.11.04 val PER: 0.0375
2026-01-04 22:17:12,944: t15.2023.11.17 val PER: 0.0404
2026-01-04 22:17:12,944: t15.2023.11.19 val PER: 0.0379
2026-01-04 22:17:12,944: t15.2023.11.26 val PER: 0.1254
2026-01-04 22:17:12,944: t15.2023.12.03 val PER: 0.1103
2026-01-04 22:17:12,944: t15.2023.12.08 val PER: 0.0999
2026-01-04 22:17:12,944: t15.2023.12.10 val PER: 0.0828
2026-01-04 22:17:12,944: t15.2023.12.17 val PER: 0.1424
2026-01-04 22:17:12,944: t15.2023.12.29 val PER: 0.1270
2026-01-04 22:17:12,944: t15.2024.02.25 val PER: 0.1194
2026-01-04 22:17:12,944: t15.2024.03.08 val PER: 0.2319
2026-01-04 22:17:12,944: t15.2024.03.15 val PER: 0.2026
2026-01-04 22:17:12,945: t15.2024.03.17 val PER: 0.1402
2026-01-04 22:17:12,945: t15.2024.05.10 val PER: 0.1575
2026-01-04 22:17:12,945: t15.2024.06.14 val PER: 0.1577
2026-01-04 22:17:12,945: t15.2024.07.19 val PER: 0.2281
2026-01-04 22:17:12,945: t15.2024.07.21 val PER: 0.0924
2026-01-04 22:17:12,945: t15.2024.07.28 val PER: 0.1316
2026-01-04 22:17:12,945: t15.2025.01.10 val PER: 0.2879
2026-01-04 22:17:12,945: t15.2025.01.12 val PER: 0.1424
2026-01-04 22:17:12,945: t15.2025.03.14 val PER: 0.3521
2026-01-04 22:17:12,945: t15.2025.03.16 val PER: 0.2003
2026-01-04 22:17:12,945: t15.2025.03.30 val PER: 0.2874
2026-01-04 22:17:12,945: t15.2025.04.13 val PER: 0.2183
2026-01-04 22:17:13,237: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_14500
2026-01-04 22:17:22,534: Train batch 14600: loss: 11.98 grad norm: 58.20 time: 0.058
2026-01-04 22:17:40,820: Train batch 14800: loss: 5.41 grad norm: 43.79 time: 0.051
2026-01-04 22:17:59,379: Train batch 15000: loss: 8.63 grad norm: 51.55 time: 0.052
2026-01-04 22:17:59,379: Running test after training batch: 15000
2026-01-04 22:17:59,485: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:18:04,326: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 22:18:04,359: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-04 22:18:06,219: Val batch 15000: PER (avg): 0.1482 CTC Loss (avg): 15.2914 WER(1gram): 46.70% (n=64) time: 6.839
2026-01-04 22:18:06,219: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 22:18:06,219: t15.2023.08.13 val PER: 0.1102
2026-01-04 22:18:06,219: t15.2023.08.18 val PER: 0.1039
2026-01-04 22:18:06,219: t15.2023.08.20 val PER: 0.1096
2026-01-04 22:18:06,219: t15.2023.08.25 val PER: 0.0904
2026-01-04 22:18:06,219: t15.2023.08.27 val PER: 0.1801
2026-01-04 22:18:06,220: t15.2023.09.01 val PER: 0.0771
2026-01-04 22:18:06,220: t15.2023.09.03 val PER: 0.1520
2026-01-04 22:18:06,220: t15.2023.09.24 val PER: 0.1299
2026-01-04 22:18:06,220: t15.2023.09.29 val PER: 0.1244
2026-01-04 22:18:06,220: t15.2023.10.01 val PER: 0.1724
2026-01-04 22:18:06,220: t15.2023.10.06 val PER: 0.0764
2026-01-04 22:18:06,220: t15.2023.10.08 val PER: 0.2409
2026-01-04 22:18:06,220: t15.2023.10.13 val PER: 0.1963
2026-01-04 22:18:06,220: t15.2023.10.15 val PER: 0.1536
2026-01-04 22:18:06,220: t15.2023.10.20 val PER: 0.1846
2026-01-04 22:18:06,220: t15.2023.10.22 val PER: 0.1125
2026-01-04 22:18:06,220: t15.2023.11.03 val PER: 0.1771
2026-01-04 22:18:06,220: t15.2023.11.04 val PER: 0.0341
2026-01-04 22:18:06,220: t15.2023.11.17 val PER: 0.0435
2026-01-04 22:18:06,221: t15.2023.11.19 val PER: 0.0339
2026-01-04 22:18:06,221: t15.2023.11.26 val PER: 0.1174
2026-01-04 22:18:06,221: t15.2023.12.03 val PER: 0.1050
2026-01-04 22:18:06,221: t15.2023.12.08 val PER: 0.1012
2026-01-04 22:18:06,221: t15.2023.12.10 val PER: 0.0920
2026-01-04 22:18:06,221: t15.2023.12.17 val PER: 0.1351
2026-01-04 22:18:06,221: t15.2023.12.29 val PER: 0.1297
2026-01-04 22:18:06,221: t15.2024.02.25 val PER: 0.1039
2026-01-04 22:18:06,221: t15.2024.03.08 val PER: 0.2304
2026-01-04 22:18:06,221: t15.2024.03.15 val PER: 0.2020
2026-01-04 22:18:06,221: t15.2024.03.17 val PER: 0.1346
2026-01-04 22:18:06,221: t15.2024.05.10 val PER: 0.1753
2026-01-04 22:18:06,222: t15.2024.06.14 val PER: 0.1672
2026-01-04 22:18:06,222: t15.2024.07.19 val PER: 0.2129
2026-01-04 22:18:06,222: t15.2024.07.21 val PER: 0.0903
2026-01-04 22:18:06,222: t15.2024.07.28 val PER: 0.1272
2026-01-04 22:18:06,222: t15.2025.01.10 val PER: 0.2879
2026-01-04 22:18:06,222: t15.2025.01.12 val PER: 0.1432
2026-01-04 22:18:06,222: t15.2025.03.14 val PER: 0.3624
2026-01-04 22:18:06,222: t15.2025.03.16 val PER: 0.1937
2026-01-04 22:18:06,222: t15.2025.03.30 val PER: 0.2943
2026-01-04 22:18:06,222: t15.2025.04.13 val PER: 0.2183
2026-01-04 22:18:06,518: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_15000
2026-01-04 22:18:24,909: Train batch 15200: loss: 4.88 grad norm: 39.89 time: 0.057
2026-01-04 22:18:42,978: Train batch 15400: loss: 10.98 grad norm: 57.18 time: 0.049
2026-01-04 22:18:52,387: Running test after training batch: 15500
2026-01-04 22:18:52,495: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:18:57,368: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:18:57,401: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 22:18:59,240: Val batch 15500: PER (avg): 0.1467 CTC Loss (avg): 15.2362 WER(1gram): 45.69% (n=64) time: 6.852
2026-01-04 22:18:59,241: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 22:18:59,241: t15.2023.08.13 val PER: 0.1091
2026-01-04 22:18:59,241: t15.2023.08.18 val PER: 0.1031
2026-01-04 22:18:59,241: t15.2023.08.20 val PER: 0.1041
2026-01-04 22:18:59,241: t15.2023.08.25 val PER: 0.0873
2026-01-04 22:18:59,241: t15.2023.08.27 val PER: 0.1833
2026-01-04 22:18:59,241: t15.2023.09.01 val PER: 0.0755
2026-01-04 22:18:59,241: t15.2023.09.03 val PER: 0.1603
2026-01-04 22:18:59,242: t15.2023.09.24 val PER: 0.1214
2026-01-04 22:18:59,242: t15.2023.09.29 val PER: 0.1276
2026-01-04 22:18:59,242: t15.2023.10.01 val PER: 0.1711
2026-01-04 22:18:59,242: t15.2023.10.06 val PER: 0.0861
2026-01-04 22:18:59,242: t15.2023.10.08 val PER: 0.2382
2026-01-04 22:18:59,242: t15.2023.10.13 val PER: 0.1994
2026-01-04 22:18:59,242: t15.2023.10.15 val PER: 0.1556
2026-01-04 22:18:59,242: t15.2023.10.20 val PER: 0.1644
2026-01-04 22:18:59,242: t15.2023.10.22 val PER: 0.1136
2026-01-04 22:18:59,242: t15.2023.11.03 val PER: 0.1750
2026-01-04 22:18:59,242: t15.2023.11.04 val PER: 0.0410
2026-01-04 22:18:59,242: t15.2023.11.17 val PER: 0.0435
2026-01-04 22:18:59,242: t15.2023.11.19 val PER: 0.0339
2026-01-04 22:18:59,243: t15.2023.11.26 val PER: 0.1167
2026-01-04 22:18:59,243: t15.2023.12.03 val PER: 0.1050
2026-01-04 22:18:59,243: t15.2023.12.08 val PER: 0.0972
2026-01-04 22:18:59,243: t15.2023.12.10 val PER: 0.0815
2026-01-04 22:18:59,243: t15.2023.12.17 val PER: 0.1362
2026-01-04 22:18:59,243: t15.2023.12.29 val PER: 0.1277
2026-01-04 22:18:59,243: t15.2024.02.25 val PER: 0.1025
2026-01-04 22:18:59,243: t15.2024.03.08 val PER: 0.2333
2026-01-04 22:18:59,243: t15.2024.03.15 val PER: 0.1995
2026-01-04 22:18:59,243: t15.2024.03.17 val PER: 0.1283
2026-01-04 22:18:59,243: t15.2024.05.10 val PER: 0.1664
2026-01-04 22:18:59,243: t15.2024.06.14 val PER: 0.1514
2026-01-04 22:18:59,243: t15.2024.07.19 val PER: 0.2090
2026-01-04 22:18:59,243: t15.2024.07.21 val PER: 0.0938
2026-01-04 22:18:59,243: t15.2024.07.28 val PER: 0.1309
2026-01-04 22:18:59,243: t15.2025.01.10 val PER: 0.2879
2026-01-04 22:18:59,243: t15.2025.01.12 val PER: 0.1501
2026-01-04 22:18:59,244: t15.2025.03.14 val PER: 0.3491
2026-01-04 22:18:59,244: t15.2025.03.16 val PER: 0.1819
2026-01-04 22:18:59,244: t15.2025.03.30 val PER: 0.2874
2026-01-04 22:18:59,244: t15.2025.04.13 val PER: 0.2097
2026-01-04 22:18:59,245: New best val WER(1gram) 46.45% --> 45.69%
2026-01-04 22:18:59,246: Checkpointing model
2026-01-04 22:18:59,860: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:19:00,158: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_15500
2026-01-04 22:19:09,102: Train batch 15600: loss: 10.58 grad norm: 56.23 time: 0.062
2026-01-04 22:19:26,823: Train batch 15800: loss: 12.65 grad norm: 62.00 time: 0.067
2026-01-04 22:19:44,732: Train batch 16000: loss: 8.40 grad norm: 48.01 time: 0.055
2026-01-04 22:19:44,732: Running test after training batch: 16000
2026-01-04 22:19:44,828: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:19:49,708: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:19:49,742: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 22:19:51,573: Val batch 16000: PER (avg): 0.1475 CTC Loss (avg): 15.3588 WER(1gram): 45.43% (n=64) time: 6.841
2026-01-04 22:19:51,574: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-04 22:19:51,574: t15.2023.08.13 val PER: 0.1133
2026-01-04 22:19:51,574: t15.2023.08.18 val PER: 0.1039
2026-01-04 22:19:51,574: t15.2023.08.20 val PER: 0.0993
2026-01-04 22:19:51,574: t15.2023.08.25 val PER: 0.0949
2026-01-04 22:19:51,574: t15.2023.08.27 val PER: 0.1849
2026-01-04 22:19:51,574: t15.2023.09.01 val PER: 0.0836
2026-01-04 22:19:51,574: t15.2023.09.03 val PER: 0.1544
2026-01-04 22:19:51,574: t15.2023.09.24 val PER: 0.1311
2026-01-04 22:19:51,575: t15.2023.09.29 val PER: 0.1257
2026-01-04 22:19:51,575: t15.2023.10.01 val PER: 0.1737
2026-01-04 22:19:51,575: t15.2023.10.06 val PER: 0.0850
2026-01-04 22:19:51,575: t15.2023.10.08 val PER: 0.2436
2026-01-04 22:19:51,575: t15.2023.10.13 val PER: 0.1971
2026-01-04 22:19:51,575: t15.2023.10.15 val PER: 0.1536
2026-01-04 22:19:51,575: t15.2023.10.20 val PER: 0.1745
2026-01-04 22:19:51,575: t15.2023.10.22 val PER: 0.1069
2026-01-04 22:19:51,575: t15.2023.11.03 val PER: 0.1764
2026-01-04 22:19:51,575: t15.2023.11.04 val PER: 0.0444
2026-01-04 22:19:51,575: t15.2023.11.17 val PER: 0.0358
2026-01-04 22:19:51,575: t15.2023.11.19 val PER: 0.0379
2026-01-04 22:19:51,575: t15.2023.11.26 val PER: 0.1167
2026-01-04 22:19:51,575: t15.2023.12.03 val PER: 0.1113
2026-01-04 22:19:51,576: t15.2023.12.08 val PER: 0.0932
2026-01-04 22:19:51,576: t15.2023.12.10 val PER: 0.0894
2026-01-04 22:19:51,576: t15.2023.12.17 val PER: 0.1289
2026-01-04 22:19:51,577: t15.2023.12.29 val PER: 0.1270
2026-01-04 22:19:51,577: t15.2024.02.25 val PER: 0.1053
2026-01-04 22:19:51,577: t15.2024.03.08 val PER: 0.2219
2026-01-04 22:19:51,577: t15.2024.03.15 val PER: 0.1920
2026-01-04 22:19:51,577: t15.2024.03.17 val PER: 0.1311
2026-01-04 22:19:51,577: t15.2024.05.10 val PER: 0.1634
2026-01-04 22:19:51,577: t15.2024.06.14 val PER: 0.1640
2026-01-04 22:19:51,577: t15.2024.07.19 val PER: 0.2202
2026-01-04 22:19:51,577: t15.2024.07.21 val PER: 0.0924
2026-01-04 22:19:51,577: t15.2024.07.28 val PER: 0.1382
2026-01-04 22:19:51,577: t15.2025.01.10 val PER: 0.2837
2026-01-04 22:19:51,578: t15.2025.01.12 val PER: 0.1401
2026-01-04 22:19:51,578: t15.2025.03.14 val PER: 0.3447
2026-01-04 22:19:51,578: t15.2025.03.16 val PER: 0.1950
2026-01-04 22:19:51,578: t15.2025.03.30 val PER: 0.2989
2026-01-04 22:19:51,578: t15.2025.04.13 val PER: 0.2140
2026-01-04 22:19:51,578: New best val WER(1gram) 45.69% --> 45.43%
2026-01-04 22:19:51,578: Checkpointing model
2026-01-04 22:19:52,214: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:19:52,515: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_16000
2026-01-04 22:20:11,278: Train batch 16200: loss: 6.17 grad norm: 43.83 time: 0.056
2026-01-04 22:20:29,993: Train batch 16400: loss: 10.29 grad norm: 62.05 time: 0.058
2026-01-04 22:20:39,526: Running test after training batch: 16500
2026-01-04 22:20:39,938: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:20:44,787: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:20:44,820: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 22:20:46,657: Val batch 16500: PER (avg): 0.1472 CTC Loss (avg): 15.1930 WER(1gram): 44.67% (n=64) time: 7.131
2026-01-04 22:20:46,658: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 22:20:46,658: t15.2023.08.13 val PER: 0.1133
2026-01-04 22:20:46,658: t15.2023.08.18 val PER: 0.1056
2026-01-04 22:20:46,658: t15.2023.08.20 val PER: 0.0961
2026-01-04 22:20:46,658: t15.2023.08.25 val PER: 0.0858
2026-01-04 22:20:46,658: t15.2023.08.27 val PER: 0.1849
2026-01-04 22:20:46,658: t15.2023.09.01 val PER: 0.0779
2026-01-04 22:20:46,658: t15.2023.09.03 val PER: 0.1627
2026-01-04 22:20:46,658: t15.2023.09.24 val PER: 0.1323
2026-01-04 22:20:46,659: t15.2023.09.29 val PER: 0.1232
2026-01-04 22:20:46,659: t15.2023.10.01 val PER: 0.1731
2026-01-04 22:20:46,659: t15.2023.10.06 val PER: 0.0850
2026-01-04 22:20:46,659: t15.2023.10.08 val PER: 0.2341
2026-01-04 22:20:46,659: t15.2023.10.13 val PER: 0.1986
2026-01-04 22:20:46,659: t15.2023.10.15 val PER: 0.1549
2026-01-04 22:20:46,659: t15.2023.10.20 val PER: 0.1678
2026-01-04 22:20:46,659: t15.2023.10.22 val PER: 0.1080
2026-01-04 22:20:46,659: t15.2023.11.03 val PER: 0.1771
2026-01-04 22:20:46,659: t15.2023.11.04 val PER: 0.0307
2026-01-04 22:20:46,659: t15.2023.11.17 val PER: 0.0389
2026-01-04 22:20:46,659: t15.2023.11.19 val PER: 0.0319
2026-01-04 22:20:46,659: t15.2023.11.26 val PER: 0.1145
2026-01-04 22:20:46,659: t15.2023.12.03 val PER: 0.1061
2026-01-04 22:20:46,660: t15.2023.12.08 val PER: 0.0945
2026-01-04 22:20:46,660: t15.2023.12.10 val PER: 0.0854
2026-01-04 22:20:46,660: t15.2023.12.17 val PER: 0.1331
2026-01-04 22:20:46,660: t15.2023.12.29 val PER: 0.1194
2026-01-04 22:20:46,660: t15.2024.02.25 val PER: 0.1025
2026-01-04 22:20:46,660: t15.2024.03.08 val PER: 0.2333
2026-01-04 22:20:46,660: t15.2024.03.15 val PER: 0.2026
2026-01-04 22:20:46,660: t15.2024.03.17 val PER: 0.1346
2026-01-04 22:20:46,660: t15.2024.05.10 val PER: 0.1590
2026-01-04 22:20:46,660: t15.2024.06.14 val PER: 0.1593
2026-01-04 22:20:46,660: t15.2024.07.19 val PER: 0.2189
2026-01-04 22:20:46,660: t15.2024.07.21 val PER: 0.0917
2026-01-04 22:20:46,661: t15.2024.07.28 val PER: 0.1338
2026-01-04 22:20:46,661: t15.2025.01.10 val PER: 0.2975
2026-01-04 22:20:46,661: t15.2025.01.12 val PER: 0.1463
2026-01-04 22:20:46,661: t15.2025.03.14 val PER: 0.3417
2026-01-04 22:20:46,661: t15.2025.03.16 val PER: 0.1911
2026-01-04 22:20:46,661: t15.2025.03.30 val PER: 0.2943
2026-01-04 22:20:46,661: t15.2025.04.13 val PER: 0.2140
2026-01-04 22:20:46,662: New best val WER(1gram) 45.43% --> 44.67%
2026-01-04 22:20:46,662: Checkpointing model
2026-01-04 22:20:47,278: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:20:47,575: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_16500
2026-01-04 22:20:56,932: Train batch 16600: loss: 7.72 grad norm: 50.40 time: 0.052
2026-01-04 22:21:15,386: Train batch 16800: loss: 16.30 grad norm: 75.99 time: 0.062
2026-01-04 22:21:33,752: Train batch 17000: loss: 7.92 grad norm: 50.59 time: 0.082
2026-01-04 22:21:33,753: Running test after training batch: 17000
2026-01-04 22:21:33,856: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:21:38,947: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:21:38,980: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-04 22:21:40,816: Val batch 17000: PER (avg): 0.1456 CTC Loss (avg): 15.0541 WER(1gram): 45.94% (n=64) time: 7.063
2026-01-04 22:21:40,816: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 22:21:40,817: t15.2023.08.13 val PER: 0.1133
2026-01-04 22:21:40,817: t15.2023.08.18 val PER: 0.1056
2026-01-04 22:21:40,817: t15.2023.08.20 val PER: 0.1001
2026-01-04 22:21:40,817: t15.2023.08.25 val PER: 0.0919
2026-01-04 22:21:40,817: t15.2023.08.27 val PER: 0.1752
2026-01-04 22:21:40,817: t15.2023.09.01 val PER: 0.0739
2026-01-04 22:21:40,817: t15.2023.09.03 val PER: 0.1591
2026-01-04 22:21:40,817: t15.2023.09.24 val PER: 0.1359
2026-01-04 22:21:40,817: t15.2023.09.29 val PER: 0.1257
2026-01-04 22:21:40,817: t15.2023.10.01 val PER: 0.1684
2026-01-04 22:21:40,817: t15.2023.10.06 val PER: 0.0850
2026-01-04 22:21:40,818: t15.2023.10.08 val PER: 0.2395
2026-01-04 22:21:40,818: t15.2023.10.13 val PER: 0.1986
2026-01-04 22:21:40,818: t15.2023.10.15 val PER: 0.1543
2026-01-04 22:21:40,818: t15.2023.10.20 val PER: 0.1544
2026-01-04 22:21:40,818: t15.2023.10.22 val PER: 0.1158
2026-01-04 22:21:40,818: t15.2023.11.03 val PER: 0.1777
2026-01-04 22:21:40,818: t15.2023.11.04 val PER: 0.0375
2026-01-04 22:21:40,818: t15.2023.11.17 val PER: 0.0389
2026-01-04 22:21:40,818: t15.2023.11.19 val PER: 0.0359
2026-01-04 22:21:40,818: t15.2023.11.26 val PER: 0.1109
2026-01-04 22:21:40,818: t15.2023.12.03 val PER: 0.1092
2026-01-04 22:21:40,818: t15.2023.12.08 val PER: 0.0932
2026-01-04 22:21:40,818: t15.2023.12.10 val PER: 0.0841
2026-01-04 22:21:40,818: t15.2023.12.17 val PER: 0.1289
2026-01-04 22:21:40,818: t15.2023.12.29 val PER: 0.1222
2026-01-04 22:21:40,818: t15.2024.02.25 val PER: 0.1039
2026-01-04 22:21:40,818: t15.2024.03.08 val PER: 0.2248
2026-01-04 22:21:40,818: t15.2024.03.15 val PER: 0.1964
2026-01-04 22:21:40,819: t15.2024.03.17 val PER: 0.1311
2026-01-04 22:21:40,819: t15.2024.05.10 val PER: 0.1620
2026-01-04 22:21:40,819: t15.2024.06.14 val PER: 0.1546
2026-01-04 22:21:40,819: t15.2024.07.19 val PER: 0.2109
2026-01-04 22:21:40,819: t15.2024.07.21 val PER: 0.0890
2026-01-04 22:21:40,819: t15.2024.07.28 val PER: 0.1206
2026-01-04 22:21:40,819: t15.2025.01.10 val PER: 0.2989
2026-01-04 22:21:40,819: t15.2025.01.12 val PER: 0.1440
2026-01-04 22:21:40,819: t15.2025.03.14 val PER: 0.3388
2026-01-04 22:21:40,819: t15.2025.03.16 val PER: 0.1832
2026-01-04 22:21:40,819: t15.2025.03.30 val PER: 0.2908
2026-01-04 22:21:40,819: t15.2025.04.13 val PER: 0.2140
2026-01-04 22:21:41,105: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_17000
2026-01-04 22:21:59,639: Train batch 17200: loss: 8.84 grad norm: 49.66 time: 0.084
2026-01-04 22:22:18,067: Train batch 17400: loss: 10.28 grad norm: 54.21 time: 0.070
2026-01-04 22:22:27,126: Running test after training batch: 17500
2026-01-04 22:22:27,229: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:22:32,067: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:22:32,101: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 22:22:33,956: Val batch 17500: PER (avg): 0.1455 CTC Loss (avg): 15.0284 WER(1gram): 44.92% (n=64) time: 6.829
2026-01-04 22:22:33,956: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 22:22:33,956: t15.2023.08.13 val PER: 0.1091
2026-01-04 22:22:33,957: t15.2023.08.18 val PER: 0.1039
2026-01-04 22:22:33,957: t15.2023.08.20 val PER: 0.0985
2026-01-04 22:22:33,957: t15.2023.08.25 val PER: 0.0873
2026-01-04 22:22:33,957: t15.2023.08.27 val PER: 0.1752
2026-01-04 22:22:33,957: t15.2023.09.01 val PER: 0.0787
2026-01-04 22:22:33,957: t15.2023.09.03 val PER: 0.1580
2026-01-04 22:22:33,957: t15.2023.09.24 val PER: 0.1335
2026-01-04 22:22:33,957: t15.2023.09.29 val PER: 0.1244
2026-01-04 22:22:33,957: t15.2023.10.01 val PER: 0.1717
2026-01-04 22:22:33,958: t15.2023.10.06 val PER: 0.0840
2026-01-04 22:22:33,958: t15.2023.10.08 val PER: 0.2463
2026-01-04 22:22:33,958: t15.2023.10.13 val PER: 0.1955
2026-01-04 22:22:33,958: t15.2023.10.15 val PER: 0.1516
2026-01-04 22:22:33,958: t15.2023.10.20 val PER: 0.1644
2026-01-04 22:22:33,958: t15.2023.10.22 val PER: 0.1069
2026-01-04 22:22:33,958: t15.2023.11.03 val PER: 0.1744
2026-01-04 22:22:33,958: t15.2023.11.04 val PER: 0.0375
2026-01-04 22:22:33,958: t15.2023.11.17 val PER: 0.0373
2026-01-04 22:22:33,958: t15.2023.11.19 val PER: 0.0339
2026-01-04 22:22:33,958: t15.2023.11.26 val PER: 0.1072
2026-01-04 22:22:33,958: t15.2023.12.03 val PER: 0.1061
2026-01-04 22:22:33,958: t15.2023.12.08 val PER: 0.0945
2026-01-04 22:22:33,958: t15.2023.12.10 val PER: 0.0828
2026-01-04 22:22:33,958: t15.2023.12.17 val PER: 0.1299
2026-01-04 22:22:33,958: t15.2023.12.29 val PER: 0.1263
2026-01-04 22:22:33,958: t15.2024.02.25 val PER: 0.1096
2026-01-04 22:22:33,958: t15.2024.03.08 val PER: 0.2205
2026-01-04 22:22:33,959: t15.2024.03.15 val PER: 0.1957
2026-01-04 22:22:33,959: t15.2024.03.17 val PER: 0.1339
2026-01-04 22:22:33,959: t15.2024.05.10 val PER: 0.1575
2026-01-04 22:22:33,959: t15.2024.06.14 val PER: 0.1546
2026-01-04 22:22:33,959: t15.2024.07.19 val PER: 0.2116
2026-01-04 22:22:33,959: t15.2024.07.21 val PER: 0.0883
2026-01-04 22:22:33,959: t15.2024.07.28 val PER: 0.1265
2026-01-04 22:22:33,959: t15.2025.01.10 val PER: 0.2948
2026-01-04 22:22:33,959: t15.2025.01.12 val PER: 0.1455
2026-01-04 22:22:33,959: t15.2025.03.14 val PER: 0.3447
2026-01-04 22:22:33,959: t15.2025.03.16 val PER: 0.1885
2026-01-04 22:22:33,959: t15.2025.03.30 val PER: 0.2897
2026-01-04 22:22:33,959: t15.2025.04.13 val PER: 0.2168
2026-01-04 22:22:34,240: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_17500
2026-01-04 22:22:43,481: Train batch 17600: loss: 9.37 grad norm: 55.95 time: 0.051
2026-01-04 22:23:01,767: Train batch 17800: loss: 6.22 grad norm: 52.50 time: 0.042
2026-01-04 22:23:19,996: Train batch 18000: loss: 10.06 grad norm: 60.94 time: 0.061
2026-01-04 22:23:19,997: Running test after training batch: 18000
2026-01-04 22:23:20,144: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:23:25,085: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:23:25,120: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 22:23:27,030: Val batch 18000: PER (avg): 0.1443 CTC Loss (avg): 15.0429 WER(1gram): 46.19% (n=64) time: 7.033
2026-01-04 22:23:27,030: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 22:23:27,031: t15.2023.08.13 val PER: 0.1133
2026-01-04 22:23:27,031: t15.2023.08.18 val PER: 0.0989
2026-01-04 22:23:27,031: t15.2023.08.20 val PER: 0.0913
2026-01-04 22:23:27,031: t15.2023.08.25 val PER: 0.0904
2026-01-04 22:23:27,031: t15.2023.08.27 val PER: 0.1801
2026-01-04 22:23:27,031: t15.2023.09.01 val PER: 0.0747
2026-01-04 22:23:27,031: t15.2023.09.03 val PER: 0.1627
2026-01-04 22:23:27,031: t15.2023.09.24 val PER: 0.1299
2026-01-04 22:23:27,031: t15.2023.09.29 val PER: 0.1219
2026-01-04 22:23:27,031: t15.2023.10.01 val PER: 0.1678
2026-01-04 22:23:27,031: t15.2023.10.06 val PER: 0.0840
2026-01-04 22:23:27,031: t15.2023.10.08 val PER: 0.2409
2026-01-04 22:23:27,031: t15.2023.10.13 val PER: 0.1955
2026-01-04 22:23:27,031: t15.2023.10.15 val PER: 0.1503
2026-01-04 22:23:27,031: t15.2023.10.20 val PER: 0.1644
2026-01-04 22:23:27,031: t15.2023.10.22 val PER: 0.1136
2026-01-04 22:23:27,032: t15.2023.11.03 val PER: 0.1744
2026-01-04 22:23:27,032: t15.2023.11.04 val PER: 0.0410
2026-01-04 22:23:27,032: t15.2023.11.17 val PER: 0.0373
2026-01-04 22:23:27,032: t15.2023.11.19 val PER: 0.0299
2026-01-04 22:23:27,032: t15.2023.11.26 val PER: 0.1080
2026-01-04 22:23:27,032: t15.2023.12.03 val PER: 0.1040
2026-01-04 22:23:27,032: t15.2023.12.08 val PER: 0.0885
2026-01-04 22:23:27,032: t15.2023.12.10 val PER: 0.0815
2026-01-04 22:23:27,032: t15.2023.12.17 val PER: 0.1310
2026-01-04 22:23:27,032: t15.2023.12.29 val PER: 0.1242
2026-01-04 22:23:27,032: t15.2024.02.25 val PER: 0.0983
2026-01-04 22:23:27,032: t15.2024.03.08 val PER: 0.2176
2026-01-04 22:23:27,032: t15.2024.03.15 val PER: 0.1970
2026-01-04 22:23:27,032: t15.2024.03.17 val PER: 0.1304
2026-01-04 22:23:27,033: t15.2024.05.10 val PER: 0.1545
2026-01-04 22:23:27,033: t15.2024.06.14 val PER: 0.1562
2026-01-04 22:23:27,033: t15.2024.07.19 val PER: 0.2162
2026-01-04 22:23:27,033: t15.2024.07.21 val PER: 0.0883
2026-01-04 22:23:27,033: t15.2024.07.28 val PER: 0.1279
2026-01-04 22:23:27,033: t15.2025.01.10 val PER: 0.2879
2026-01-04 22:23:27,033: t15.2025.01.12 val PER: 0.1447
2026-01-04 22:23:27,033: t15.2025.03.14 val PER: 0.3432
2026-01-04 22:23:27,033: t15.2025.03.16 val PER: 0.1872
2026-01-04 22:23:27,033: t15.2025.03.30 val PER: 0.2862
2026-01-04 22:23:27,033: t15.2025.04.13 val PER: 0.2168
2026-01-04 22:23:27,319: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_18000
2026-01-04 22:23:46,220: Train batch 18200: loss: 7.44 grad norm: 49.00 time: 0.074
2026-01-04 22:24:04,688: Train batch 18400: loss: 4.88 grad norm: 44.14 time: 0.058
2026-01-04 22:24:13,847: Running test after training batch: 18500
2026-01-04 22:24:13,998: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:24:18,847: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:24:18,881: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 22:24:20,744: Val batch 18500: PER (avg): 0.1451 CTC Loss (avg): 14.9783 WER(1gram): 46.45% (n=64) time: 6.896
2026-01-04 22:24:20,745: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 22:24:20,745: t15.2023.08.13 val PER: 0.1102
2026-01-04 22:24:20,745: t15.2023.08.18 val PER: 0.1039
2026-01-04 22:24:20,745: t15.2023.08.20 val PER: 0.0969
2026-01-04 22:24:20,745: t15.2023.08.25 val PER: 0.0904
2026-01-04 22:24:20,745: t15.2023.08.27 val PER: 0.1801
2026-01-04 22:24:20,745: t15.2023.09.01 val PER: 0.0771
2026-01-04 22:24:20,745: t15.2023.09.03 val PER: 0.1603
2026-01-04 22:24:20,745: t15.2023.09.24 val PER: 0.1299
2026-01-04 22:24:20,745: t15.2023.09.29 val PER: 0.1213
2026-01-04 22:24:20,745: t15.2023.10.01 val PER: 0.1704
2026-01-04 22:24:20,746: t15.2023.10.06 val PER: 0.0829
2026-01-04 22:24:20,746: t15.2023.10.08 val PER: 0.2422
2026-01-04 22:24:20,746: t15.2023.10.13 val PER: 0.1994
2026-01-04 22:24:20,746: t15.2023.10.15 val PER: 0.1529
2026-01-04 22:24:20,746: t15.2023.10.20 val PER: 0.1678
2026-01-04 22:24:20,746: t15.2023.10.22 val PER: 0.1102
2026-01-04 22:24:20,746: t15.2023.11.03 val PER: 0.1784
2026-01-04 22:24:20,746: t15.2023.11.04 val PER: 0.0375
2026-01-04 22:24:20,746: t15.2023.11.17 val PER: 0.0420
2026-01-04 22:24:20,746: t15.2023.11.19 val PER: 0.0339
2026-01-04 22:24:20,746: t15.2023.11.26 val PER: 0.1109
2026-01-04 22:24:20,746: t15.2023.12.03 val PER: 0.1061
2026-01-04 22:24:20,746: t15.2023.12.08 val PER: 0.0939
2026-01-04 22:24:20,746: t15.2023.12.10 val PER: 0.0815
2026-01-04 22:24:20,747: t15.2023.12.17 val PER: 0.1310
2026-01-04 22:24:20,747: t15.2023.12.29 val PER: 0.1242
2026-01-04 22:24:20,747: t15.2024.02.25 val PER: 0.1025
2026-01-04 22:24:20,747: t15.2024.03.08 val PER: 0.2262
2026-01-04 22:24:20,747: t15.2024.03.15 val PER: 0.1957
2026-01-04 22:24:20,747: t15.2024.03.17 val PER: 0.1269
2026-01-04 22:24:20,747: t15.2024.05.10 val PER: 0.1575
2026-01-04 22:24:20,747: t15.2024.06.14 val PER: 0.1609
2026-01-04 22:24:20,747: t15.2024.07.19 val PER: 0.2123
2026-01-04 22:24:20,747: t15.2024.07.21 val PER: 0.0869
2026-01-04 22:24:20,747: t15.2024.07.28 val PER: 0.1206
2026-01-04 22:24:20,748: t15.2025.01.10 val PER: 0.2851
2026-01-04 22:24:20,748: t15.2025.01.12 val PER: 0.1463
2026-01-04 22:24:20,748: t15.2025.03.14 val PER: 0.3462
2026-01-04 22:24:20,748: t15.2025.03.16 val PER: 0.1885
2026-01-04 22:24:20,748: t15.2025.03.30 val PER: 0.2874
2026-01-04 22:24:20,748: t15.2025.04.13 val PER: 0.2097
2026-01-04 22:24:21,036: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_18500
2026-01-04 22:24:30,119: Train batch 18600: loss: 11.71 grad norm: 60.76 time: 0.067
2026-01-04 22:24:49,333: Train batch 18800: loss: 8.07 grad norm: 52.97 time: 0.064
2026-01-04 22:25:07,890: Train batch 19000: loss: 7.39 grad norm: 44.31 time: 0.066
2026-01-04 22:25:07,890: Running test after training batch: 19000
2026-01-04 22:25:08,061: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:25:12,893: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:25:12,928: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 22:25:14,802: Val batch 19000: PER (avg): 0.1439 CTC Loss (avg): 15.0484 WER(1gram): 44.67% (n=64) time: 6.911
2026-01-04 22:25:14,802: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 22:25:14,802: t15.2023.08.13 val PER: 0.1081
2026-01-04 22:25:14,802: t15.2023.08.18 val PER: 0.0964
2026-01-04 22:25:14,803: t15.2023.08.20 val PER: 0.0993
2026-01-04 22:25:14,803: t15.2023.08.25 val PER: 0.0904
2026-01-04 22:25:14,803: t15.2023.08.27 val PER: 0.1752
2026-01-04 22:25:14,803: t15.2023.09.01 val PER: 0.0763
2026-01-04 22:25:14,803: t15.2023.09.03 val PER: 0.1651
2026-01-04 22:25:14,803: t15.2023.09.24 val PER: 0.1299
2026-01-04 22:25:14,803: t15.2023.09.29 val PER: 0.1187
2026-01-04 22:25:14,803: t15.2023.10.01 val PER: 0.1671
2026-01-04 22:25:14,803: t15.2023.10.06 val PER: 0.0829
2026-01-04 22:25:14,803: t15.2023.10.08 val PER: 0.2382
2026-01-04 22:25:14,803: t15.2023.10.13 val PER: 0.1978
2026-01-04 22:25:14,803: t15.2023.10.15 val PER: 0.1496
2026-01-04 22:25:14,803: t15.2023.10.20 val PER: 0.1779
2026-01-04 22:25:14,803: t15.2023.10.22 val PER: 0.1125
2026-01-04 22:25:14,803: t15.2023.11.03 val PER: 0.1710
2026-01-04 22:25:14,804: t15.2023.11.04 val PER: 0.0341
2026-01-04 22:25:14,804: t15.2023.11.17 val PER: 0.0358
2026-01-04 22:25:14,804: t15.2023.11.19 val PER: 0.0339
2026-01-04 22:25:14,804: t15.2023.11.26 val PER: 0.1109
2026-01-04 22:25:14,804: t15.2023.12.03 val PER: 0.1050
2026-01-04 22:25:14,804: t15.2023.12.08 val PER: 0.0925
2026-01-04 22:25:14,804: t15.2023.12.10 val PER: 0.0788
2026-01-04 22:25:14,804: t15.2023.12.17 val PER: 0.1310
2026-01-04 22:25:14,804: t15.2023.12.29 val PER: 0.1318
2026-01-04 22:25:14,804: t15.2024.02.25 val PER: 0.0983
2026-01-04 22:25:14,804: t15.2024.03.08 val PER: 0.2162
2026-01-04 22:25:14,804: t15.2024.03.15 val PER: 0.1895
2026-01-04 22:25:14,804: t15.2024.03.17 val PER: 0.1269
2026-01-04 22:25:14,804: t15.2024.05.10 val PER: 0.1560
2026-01-04 22:25:14,804: t15.2024.06.14 val PER: 0.1562
2026-01-04 22:25:14,804: t15.2024.07.19 val PER: 0.2202
2026-01-04 22:25:14,804: t15.2024.07.21 val PER: 0.0869
2026-01-04 22:25:14,804: t15.2024.07.28 val PER: 0.1243
2026-01-04 22:25:14,805: t15.2025.01.10 val PER: 0.2865
2026-01-04 22:25:14,805: t15.2025.01.12 val PER: 0.1447
2026-01-04 22:25:14,805: t15.2025.03.14 val PER: 0.3491
2026-01-04 22:25:14,805: t15.2025.03.16 val PER: 0.1859
2026-01-04 22:25:14,805: t15.2025.03.30 val PER: 0.2793
2026-01-04 22:25:14,805: t15.2025.04.13 val PER: 0.2097
2026-01-04 22:25:15,095: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_19000
2026-01-04 22:25:33,484: Train batch 19200: loss: 5.60 grad norm: 45.59 time: 0.063
2026-01-04 22:25:52,222: Train batch 19400: loss: 4.72 grad norm: 37.22 time: 0.053
2026-01-04 22:26:01,747: Running test after training batch: 19500
2026-01-04 22:26:01,909: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:26:06,805: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:26:06,840: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 22:26:08,728: Val batch 19500: PER (avg): 0.1445 CTC Loss (avg): 14.9763 WER(1gram): 45.18% (n=64) time: 6.980
2026-01-04 22:26:08,728: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 22:26:08,729: t15.2023.08.13 val PER: 0.1112
2026-01-04 22:26:08,729: t15.2023.08.18 val PER: 0.1006
2026-01-04 22:26:08,729: t15.2023.08.20 val PER: 0.0961
2026-01-04 22:26:08,729: t15.2023.08.25 val PER: 0.0858
2026-01-04 22:26:08,729: t15.2023.08.27 val PER: 0.1768
2026-01-04 22:26:08,729: t15.2023.09.01 val PER: 0.0779
2026-01-04 22:26:08,729: t15.2023.09.03 val PER: 0.1580
2026-01-04 22:26:08,729: t15.2023.09.24 val PER: 0.1274
2026-01-04 22:26:08,729: t15.2023.09.29 val PER: 0.1206
2026-01-04 22:26:08,729: t15.2023.10.01 val PER: 0.1764
2026-01-04 22:26:08,729: t15.2023.10.06 val PER: 0.0850
2026-01-04 22:26:08,729: t15.2023.10.08 val PER: 0.2368
2026-01-04 22:26:08,729: t15.2023.10.13 val PER: 0.2009
2026-01-04 22:26:08,729: t15.2023.10.15 val PER: 0.1536
2026-01-04 22:26:08,730: t15.2023.10.20 val PER: 0.1678
2026-01-04 22:26:08,730: t15.2023.10.22 val PER: 0.1080
2026-01-04 22:26:08,730: t15.2023.11.03 val PER: 0.1744
2026-01-04 22:26:08,730: t15.2023.11.04 val PER: 0.0444
2026-01-04 22:26:08,730: t15.2023.11.17 val PER: 0.0373
2026-01-04 22:26:08,730: t15.2023.11.19 val PER: 0.0359
2026-01-04 22:26:08,730: t15.2023.11.26 val PER: 0.1072
2026-01-04 22:26:08,730: t15.2023.12.03 val PER: 0.1029
2026-01-04 22:26:08,730: t15.2023.12.08 val PER: 0.0932
2026-01-04 22:26:08,730: t15.2023.12.10 val PER: 0.0815
2026-01-04 22:26:08,730: t15.2023.12.17 val PER: 0.1258
2026-01-04 22:26:08,730: t15.2023.12.29 val PER: 0.1242
2026-01-04 22:26:08,730: t15.2024.02.25 val PER: 0.1011
2026-01-04 22:26:08,730: t15.2024.03.08 val PER: 0.2205
2026-01-04 22:26:08,730: t15.2024.03.15 val PER: 0.1889
2026-01-04 22:26:08,730: t15.2024.03.17 val PER: 0.1290
2026-01-04 22:26:08,731: t15.2024.05.10 val PER: 0.1575
2026-01-04 22:26:08,731: t15.2024.06.14 val PER: 0.1577
2026-01-04 22:26:08,731: t15.2024.07.19 val PER: 0.2162
2026-01-04 22:26:08,731: t15.2024.07.21 val PER: 0.0876
2026-01-04 22:26:08,731: t15.2024.07.28 val PER: 0.1250
2026-01-04 22:26:08,731: t15.2025.01.10 val PER: 0.2934
2026-01-04 22:26:08,731: t15.2025.01.12 val PER: 0.1424
2026-01-04 22:26:08,731: t15.2025.03.14 val PER: 0.3432
2026-01-04 22:26:08,731: t15.2025.03.16 val PER: 0.1859
2026-01-04 22:26:08,731: t15.2025.03.30 val PER: 0.2897
2026-01-04 22:26:08,731: t15.2025.04.13 val PER: 0.2154
2026-01-04 22:26:09,028: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_19500
2026-01-04 22:26:18,046: Train batch 19600: loss: 7.13 grad norm: 46.88 time: 0.057
2026-01-04 22:26:36,375: Train batch 19800: loss: 6.94 grad norm: 48.60 time: 0.055
2026-01-04 22:26:54,692: Running test after training batch: 19999
2026-01-04 22:26:54,786: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:26:59,528: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 22:26:59,563: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 22:27:01,473: Val batch 19999: PER (avg): 0.1438 CTC Loss (avg): 14.9701 WER(1gram): 44.92% (n=64) time: 6.780
2026-01-04 22:27:01,473: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 22:27:01,473: t15.2023.08.13 val PER: 0.1133
2026-01-04 22:27:01,473: t15.2023.08.18 val PER: 0.0956
2026-01-04 22:27:01,473: t15.2023.08.20 val PER: 0.1009
2026-01-04 22:27:01,474: t15.2023.08.25 val PER: 0.0873
2026-01-04 22:27:01,474: t15.2023.08.27 val PER: 0.1801
2026-01-04 22:27:01,474: t15.2023.09.01 val PER: 0.0844
2026-01-04 22:27:01,474: t15.2023.09.03 val PER: 0.1556
2026-01-04 22:27:01,474: t15.2023.09.24 val PER: 0.1299
2026-01-04 22:27:01,474: t15.2023.09.29 val PER: 0.1219
2026-01-04 22:27:01,474: t15.2023.10.01 val PER: 0.1684
2026-01-04 22:27:01,474: t15.2023.10.06 val PER: 0.0829
2026-01-04 22:27:01,474: t15.2023.10.08 val PER: 0.2355
2026-01-04 22:27:01,474: t15.2023.10.13 val PER: 0.2002
2026-01-04 22:27:01,474: t15.2023.10.15 val PER: 0.1503
2026-01-04 22:27:01,474: t15.2023.10.20 val PER: 0.1711
2026-01-04 22:27:01,474: t15.2023.10.22 val PER: 0.1047
2026-01-04 22:27:01,475: t15.2023.11.03 val PER: 0.1716
2026-01-04 22:27:01,475: t15.2023.11.04 val PER: 0.0375
2026-01-04 22:27:01,475: t15.2023.11.17 val PER: 0.0389
2026-01-04 22:27:01,475: t15.2023.11.19 val PER: 0.0339
2026-01-04 22:27:01,475: t15.2023.11.26 val PER: 0.1080
2026-01-04 22:27:01,475: t15.2023.12.03 val PER: 0.1071
2026-01-04 22:27:01,475: t15.2023.12.08 val PER: 0.0952
2026-01-04 22:27:01,475: t15.2023.12.10 val PER: 0.0788
2026-01-04 22:27:01,475: t15.2023.12.17 val PER: 0.1258
2026-01-04 22:27:01,475: t15.2023.12.29 val PER: 0.1249
2026-01-04 22:27:01,475: t15.2024.02.25 val PER: 0.0997
2026-01-04 22:27:01,475: t15.2024.03.08 val PER: 0.2219
2026-01-04 22:27:01,476: t15.2024.03.15 val PER: 0.1914
2026-01-04 22:27:01,476: t15.2024.03.17 val PER: 0.1276
2026-01-04 22:27:01,476: t15.2024.05.10 val PER: 0.1516
2026-01-04 22:27:01,476: t15.2024.06.14 val PER: 0.1609
2026-01-04 22:27:01,476: t15.2024.07.19 val PER: 0.2109
2026-01-04 22:27:01,476: t15.2024.07.21 val PER: 0.0862
2026-01-04 22:27:01,476: t15.2024.07.28 val PER: 0.1250
2026-01-04 22:27:01,476: t15.2025.01.10 val PER: 0.2851
2026-01-04 22:27:01,476: t15.2025.01.12 val PER: 0.1432
2026-01-04 22:27:01,476: t15.2025.03.14 val PER: 0.3447
2026-01-04 22:27:01,476: t15.2025.03.16 val PER: 0.1859
2026-01-04 22:27:01,476: t15.2025.03.30 val PER: 0.2885
2026-01-04 22:27:01,476: t15.2025.04.13 val PER: 0.2097
2026-01-04 22:27:01,749: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id05_wd1e-5/checkpoint/checkpoint_batch_19999
2026-01-04 22:27:01,778: Best avg val PER achieved: 0.14718
2026-01-04 22:27:01,778: Total training time: 35.86 minutes

=== RUN id10_wd1e-5.yaml ===
2026-01-04 22:27:07,407: Using device: cuda:0
2026-01-04 22:27:09,049: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-04 22:27:09,070: Using 45 sessions after filtering (from 45).
2026-01-04 22:27:09,481: Using torch.compile (if available)
2026-01-04 22:27:09,481: torch.compile not available (torch<2.0). Skipping.
2026-01-04 22:27:09,481: Initialized RNN decoding model
2026-01-04 22:27:09,481: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.1, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-04 22:27:09,482: Model has 44,907,305 parameters
2026-01-04 22:27:09,482: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-04 22:27:10,755: Successfully initialized datasets
2026-01-04 22:27:10,756: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-04 22:27:11,757: Train batch 0: loss: 581.36 grad norm: 1417.85 time: 0.187
2026-01-04 22:27:11,758: Running test after training batch: 0
2026-01-04 22:27:11,872: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:27:17,225: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-04 22:27:17,938: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-04 22:27:51,649: Val batch 0: PER (avg): 1.4300 CTC Loss (avg): 633.1626 WER(1gram): 100.00% (n=64) time: 39.892
2026-01-04 22:27:51,650: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-04 22:27:51,650: t15.2023.08.13 val PER: 1.3035
2026-01-04 22:27:51,650: t15.2023.08.18 val PER: 1.4250
2026-01-04 22:27:51,650: t15.2023.08.20 val PER: 1.2986
2026-01-04 22:27:51,650: t15.2023.08.25 val PER: 1.3434
2026-01-04 22:27:51,651: t15.2023.08.27 val PER: 1.2524
2026-01-04 22:27:51,651: t15.2023.09.01 val PER: 1.4562
2026-01-04 22:27:51,651: t15.2023.09.03 val PER: 1.3171
2026-01-04 22:27:51,651: t15.2023.09.24 val PER: 1.5413
2026-01-04 22:27:51,651: t15.2023.09.29 val PER: 1.4716
2026-01-04 22:27:51,651: t15.2023.10.01 val PER: 1.2094
2026-01-04 22:27:51,651: t15.2023.10.06 val PER: 1.4919
2026-01-04 22:27:51,651: t15.2023.10.08 val PER: 1.1894
2026-01-04 22:27:51,651: t15.2023.10.13 val PER: 1.3910
2026-01-04 22:27:51,651: t15.2023.10.15 val PER: 1.3896
2026-01-04 22:27:51,651: t15.2023.10.20 val PER: 1.4933
2026-01-04 22:27:51,651: t15.2023.10.22 val PER: 1.3942
2026-01-04 22:27:51,651: t15.2023.11.03 val PER: 1.5936
2026-01-04 22:27:51,651: t15.2023.11.04 val PER: 2.0273
2026-01-04 22:27:51,651: t15.2023.11.17 val PER: 1.9580
2026-01-04 22:27:51,651: t15.2023.11.19 val PER: 1.6826
2026-01-04 22:27:51,652: t15.2023.11.26 val PER: 1.5391
2026-01-04 22:27:51,652: t15.2023.12.03 val PER: 1.4338
2026-01-04 22:27:51,652: t15.2023.12.08 val PER: 1.4534
2026-01-04 22:27:51,652: t15.2023.12.10 val PER: 1.7017
2026-01-04 22:27:51,652: t15.2023.12.17 val PER: 1.3046
2026-01-04 22:27:51,652: t15.2023.12.29 val PER: 1.4111
2026-01-04 22:27:51,652: t15.2024.02.25 val PER: 1.4270
2026-01-04 22:27:51,652: t15.2024.03.08 val PER: 1.3229
2026-01-04 22:27:51,652: t15.2024.03.15 val PER: 1.3164
2026-01-04 22:27:51,652: t15.2024.03.17 val PER: 1.3982
2026-01-04 22:27:51,653: t15.2024.05.10 val PER: 1.3195
2026-01-04 22:27:51,653: t15.2024.06.14 val PER: 1.5284
2026-01-04 22:27:51,653: t15.2024.07.19 val PER: 1.0798
2026-01-04 22:27:51,653: t15.2024.07.21 val PER: 1.6366
2026-01-04 22:27:51,653: t15.2024.07.28 val PER: 1.6529
2026-01-04 22:27:51,653: t15.2025.01.10 val PER: 1.0909
2026-01-04 22:27:51,653: t15.2025.01.12 val PER: 1.7683
2026-01-04 22:27:51,653: t15.2025.03.14 val PER: 1.0399
2026-01-04 22:27:51,653: t15.2025.03.16 val PER: 1.6139
2026-01-04 22:27:51,653: t15.2025.03.30 val PER: 1.2931
2026-01-04 22:27:51,653: t15.2025.04.13 val PER: 1.5849
2026-01-04 22:27:51,654: New best val WER(1gram) inf% --> 100.00%
2026-01-04 22:27:51,654: Checkpointing model
2026-01-04 22:27:51,936: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:27:52,208: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_0
2026-01-04 22:28:10,395: Train batch 200: loss: 78.34 grad norm: 107.02 time: 0.053
2026-01-04 22:28:28,116: Train batch 400: loss: 52.57 grad norm: 80.35 time: 0.062
2026-01-04 22:28:36,992: Running test after training batch: 500
2026-01-04 22:28:37,133: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:28:42,044: WER debug example
  GT : you can see the code at this point as well
  PR : nie
2026-01-04 22:28:42,069: WER debug example
  GT : how does it keep the cost down
  PR : ooohs
2026-01-04 22:28:43,371: Val batch 500: PER (avg): 0.5035 CTC Loss (avg): 52.3974 WER(1gram): 99.75% (n=64) time: 6.379
2026-01-04 22:28:43,371: WER lens: avg_true_words=6.16 avg_pred_words=1.52 max_pred_words=4
2026-01-04 22:28:43,371: t15.2023.08.13 val PER: 0.4595
2026-01-04 22:28:43,371: t15.2023.08.18 val PER: 0.4342
2026-01-04 22:28:43,371: t15.2023.08.20 val PER: 0.4527
2026-01-04 22:28:43,371: t15.2023.08.25 val PER: 0.4202
2026-01-04 22:28:43,371: t15.2023.08.27 val PER: 0.5386
2026-01-04 22:28:43,371: t15.2023.09.01 val PER: 0.4196
2026-01-04 22:28:43,371: t15.2023.09.03 val PER: 0.4988
2026-01-04 22:28:43,371: t15.2023.09.24 val PER: 0.4320
2026-01-04 22:28:43,372: t15.2023.09.29 val PER: 0.4327
2026-01-04 22:28:43,372: t15.2023.10.01 val PER: 0.4980
2026-01-04 22:28:43,372: t15.2023.10.06 val PER: 0.4112
2026-01-04 22:28:43,372: t15.2023.10.08 val PER: 0.5494
2026-01-04 22:28:43,372: t15.2023.10.13 val PER: 0.5337
2026-01-04 22:28:43,372: t15.2023.10.15 val PER: 0.4951
2026-01-04 22:28:43,372: t15.2023.10.20 val PER: 0.4664
2026-01-04 22:28:43,372: t15.2023.10.22 val PER: 0.4443
2026-01-04 22:28:43,372: t15.2023.11.03 val PER: 0.4810
2026-01-04 22:28:43,372: t15.2023.11.04 val PER: 0.2935
2026-01-04 22:28:43,373: t15.2023.11.17 val PER: 0.3499
2026-01-04 22:28:43,373: t15.2023.11.19 val PER: 0.3333
2026-01-04 22:28:43,373: t15.2023.11.26 val PER: 0.5246
2026-01-04 22:28:43,373: t15.2023.12.03 val PER: 0.4874
2026-01-04 22:28:43,373: t15.2023.12.08 val PER: 0.5033
2026-01-04 22:28:43,373: t15.2023.12.10 val PER: 0.4402
2026-01-04 22:28:43,373: t15.2023.12.17 val PER: 0.5593
2026-01-04 22:28:43,373: t15.2023.12.29 val PER: 0.5189
2026-01-04 22:28:43,373: t15.2024.02.25 val PER: 0.4593
2026-01-04 22:28:43,373: t15.2024.03.08 val PER: 0.5733
2026-01-04 22:28:43,373: t15.2024.03.15 val PER: 0.5422
2026-01-04 22:28:43,373: t15.2024.03.17 val PER: 0.4930
2026-01-04 22:28:43,373: t15.2024.05.10 val PER: 0.5215
2026-01-04 22:28:43,373: t15.2024.06.14 val PER: 0.5379
2026-01-04 22:28:43,373: t15.2024.07.19 val PER: 0.6572
2026-01-04 22:28:43,373: t15.2024.07.21 val PER: 0.4703
2026-01-04 22:28:43,373: t15.2024.07.28 val PER: 0.4971
2026-01-04 22:28:43,374: t15.2025.01.10 val PER: 0.7121
2026-01-04 22:28:43,374: t15.2025.01.12 val PER: 0.5273
2026-01-04 22:28:43,374: t15.2025.03.14 val PER: 0.6953
2026-01-04 22:28:43,374: t15.2025.03.16 val PER: 0.5707
2026-01-04 22:28:43,374: t15.2025.03.30 val PER: 0.7069
2026-01-04 22:28:43,374: t15.2025.04.13 val PER: 0.5292
2026-01-04 22:28:43,375: New best val WER(1gram) 100.00% --> 99.75%
2026-01-04 22:28:43,375: Checkpointing model
2026-01-04 22:28:43,995: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:28:44,268: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_500
2026-01-04 22:28:53,134: Train batch 600: loss: 48.00 grad norm: 78.70 time: 0.077
2026-01-04 22:29:10,774: Train batch 800: loss: 40.94 grad norm: 85.70 time: 0.057
2026-01-04 22:29:28,626: Train batch 1000: loss: 43.17 grad norm: 74.14 time: 0.066
2026-01-04 22:29:28,626: Running test after training batch: 1000
2026-01-04 22:29:28,759: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:29:33,666: WER debug example
  GT : you can see the code at this point as well
  PR : ou nie e uhde
2026-01-04 22:29:33,691: WER debug example
  GT : how does it keep the cost down
  PR : sci us
2026-01-04 22:29:34,998: Val batch 1000: PER (avg): 0.4193 CTC Loss (avg): 41.9245 WER(1gram): 99.75% (n=64) time: 6.372
2026-01-04 22:29:34,998: WER lens: avg_true_words=6.16 avg_pred_words=2.97 max_pred_words=7
2026-01-04 22:29:34,998: t15.2023.08.13 val PER: 0.3898
2026-01-04 22:29:34,998: t15.2023.08.18 val PER: 0.3462
2026-01-04 22:29:34,998: t15.2023.08.20 val PER: 0.3630
2026-01-04 22:29:34,998: t15.2023.08.25 val PER: 0.3072
2026-01-04 22:29:34,999: t15.2023.08.27 val PER: 0.4293
2026-01-04 22:29:34,999: t15.2023.09.01 val PER: 0.3133
2026-01-04 22:29:34,999: t15.2023.09.03 val PER: 0.4038
2026-01-04 22:29:34,999: t15.2023.09.24 val PER: 0.3434
2026-01-04 22:29:34,999: t15.2023.09.29 val PER: 0.3791
2026-01-04 22:29:34,999: t15.2023.10.01 val PER: 0.4194
2026-01-04 22:29:34,999: t15.2023.10.06 val PER: 0.3412
2026-01-04 22:29:34,999: t15.2023.10.08 val PER: 0.4655
2026-01-04 22:29:34,999: t15.2023.10.13 val PER: 0.4771
2026-01-04 22:29:34,999: t15.2023.10.15 val PER: 0.3988
2026-01-04 22:29:34,999: t15.2023.10.20 val PER: 0.3658
2026-01-04 22:29:34,999: t15.2023.10.22 val PER: 0.3474
2026-01-04 22:29:34,999: t15.2023.11.03 val PER: 0.3989
2026-01-04 22:29:34,999: t15.2023.11.04 val PER: 0.1741
2026-01-04 22:29:34,999: t15.2023.11.17 val PER: 0.2815
2026-01-04 22:29:35,000: t15.2023.11.19 val PER: 0.2455
2026-01-04 22:29:35,000: t15.2023.11.26 val PER: 0.4384
2026-01-04 22:29:35,000: t15.2023.12.03 val PER: 0.3960
2026-01-04 22:29:35,000: t15.2023.12.08 val PER: 0.4134
2026-01-04 22:29:35,000: t15.2023.12.10 val PER: 0.3732
2026-01-04 22:29:35,000: t15.2023.12.17 val PER: 0.4220
2026-01-04 22:29:35,000: t15.2023.12.29 val PER: 0.4146
2026-01-04 22:29:35,000: t15.2024.02.25 val PER: 0.3469
2026-01-04 22:29:35,000: t15.2024.03.08 val PER: 0.5036
2026-01-04 22:29:35,000: t15.2024.03.15 val PER: 0.4578
2026-01-04 22:29:35,000: t15.2024.03.17 val PER: 0.4142
2026-01-04 22:29:35,000: t15.2024.05.10 val PER: 0.4383
2026-01-04 22:29:35,000: t15.2024.06.14 val PER: 0.4306
2026-01-04 22:29:35,000: t15.2024.07.19 val PER: 0.5577
2026-01-04 22:29:35,000: t15.2024.07.21 val PER: 0.3814
2026-01-04 22:29:35,000: t15.2024.07.28 val PER: 0.4118
2026-01-04 22:29:35,000: t15.2025.01.10 val PER: 0.6419
2026-01-04 22:29:35,001: t15.2025.01.12 val PER: 0.4488
2026-01-04 22:29:35,001: t15.2025.03.14 val PER: 0.6450
2026-01-04 22:29:35,001: t15.2025.03.16 val PER: 0.5157
2026-01-04 22:29:35,001: t15.2025.03.30 val PER: 0.6782
2026-01-04 22:29:35,001: t15.2025.04.13 val PER: 0.4779
2026-01-04 22:29:35,290: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_1000
2026-01-04 22:29:53,386: Train batch 1200: loss: 34.18 grad norm: 75.03 time: 0.068
2026-01-04 22:30:12,124: Train batch 1400: loss: 36.97 grad norm: 80.45 time: 0.061
2026-01-04 22:30:21,406: Running test after training batch: 1500
2026-01-04 22:30:21,518: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:30:26,514: WER debug example
  GT : you can see the code at this point as well
  PR : ou nie e uhde
2026-01-04 22:30:26,539: WER debug example
  GT : how does it keep the cost down
  PR : sci
2026-01-04 22:30:27,896: Val batch 1500: PER (avg): 0.3781 CTC Loss (avg): 36.6252 WER(1gram): 100.00% (n=64) time: 6.490
2026-01-04 22:30:27,897: WER lens: avg_true_words=6.16 avg_pred_words=2.23 max_pred_words=5
2026-01-04 22:30:27,897: t15.2023.08.13 val PER: 0.3430
2026-01-04 22:30:27,897: t15.2023.08.18 val PER: 0.3152
2026-01-04 22:30:27,897: t15.2023.08.20 val PER: 0.3106
2026-01-04 22:30:27,897: t15.2023.08.25 val PER: 0.2756
2026-01-04 22:30:27,897: t15.2023.08.27 val PER: 0.3939
2026-01-04 22:30:27,897: t15.2023.09.01 val PER: 0.2760
2026-01-04 22:30:27,897: t15.2023.09.03 val PER: 0.3705
2026-01-04 22:30:27,898: t15.2023.09.24 val PER: 0.2998
2026-01-04 22:30:27,898: t15.2023.09.29 val PER: 0.3331
2026-01-04 22:30:27,898: t15.2023.10.01 val PER: 0.3857
2026-01-04 22:30:27,898: t15.2023.10.06 val PER: 0.3014
2026-01-04 22:30:27,898: t15.2023.10.08 val PER: 0.4303
2026-01-04 22:30:27,898: t15.2023.10.13 val PER: 0.4399
2026-01-04 22:30:27,898: t15.2023.10.15 val PER: 0.3566
2026-01-04 22:30:27,898: t15.2023.10.20 val PER: 0.3154
2026-01-04 22:30:27,898: t15.2023.10.22 val PER: 0.3140
2026-01-04 22:30:27,898: t15.2023.11.03 val PER: 0.3548
2026-01-04 22:30:27,898: t15.2023.11.04 val PER: 0.1399
2026-01-04 22:30:27,898: t15.2023.11.17 val PER: 0.2302
2026-01-04 22:30:27,899: t15.2023.11.19 val PER: 0.2076
2026-01-04 22:30:27,899: t15.2023.11.26 val PER: 0.4007
2026-01-04 22:30:27,899: t15.2023.12.03 val PER: 0.3666
2026-01-04 22:30:27,899: t15.2023.12.08 val PER: 0.3509
2026-01-04 22:30:27,899: t15.2023.12.10 val PER: 0.3154
2026-01-04 22:30:27,899: t15.2023.12.17 val PER: 0.3732
2026-01-04 22:30:27,899: t15.2023.12.29 val PER: 0.3789
2026-01-04 22:30:27,899: t15.2024.02.25 val PER: 0.3062
2026-01-04 22:30:27,899: t15.2024.03.08 val PER: 0.4637
2026-01-04 22:30:27,899: t15.2024.03.15 val PER: 0.4159
2026-01-04 22:30:27,899: t15.2024.03.17 val PER: 0.3801
2026-01-04 22:30:27,899: t15.2024.05.10 val PER: 0.3938
2026-01-04 22:30:27,899: t15.2024.06.14 val PER: 0.3801
2026-01-04 22:30:27,899: t15.2024.07.19 val PER: 0.5313
2026-01-04 22:30:27,899: t15.2024.07.21 val PER: 0.3455
2026-01-04 22:30:27,899: t15.2024.07.28 val PER: 0.3640
2026-01-04 22:30:27,899: t15.2025.01.10 val PER: 0.6143
2026-01-04 22:30:27,900: t15.2025.01.12 val PER: 0.3965
2026-01-04 22:30:27,900: t15.2025.03.14 val PER: 0.5962
2026-01-04 22:30:27,900: t15.2025.03.16 val PER: 0.4529
2026-01-04 22:30:27,900: t15.2025.03.30 val PER: 0.6460
2026-01-04 22:30:27,900: t15.2025.04.13 val PER: 0.4379
2026-01-04 22:30:28,161: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_1500
2026-01-04 22:30:36,982: Train batch 1600: loss: 35.35 grad norm: 74.04 time: 0.063
2026-01-04 22:30:54,784: Train batch 1800: loss: 35.62 grad norm: 81.03 time: 0.088
2026-01-04 22:31:12,914: Train batch 2000: loss: 33.75 grad norm: 72.86 time: 0.066
2026-01-04 22:31:12,915: Running test after training batch: 2000
2026-01-04 22:31:13,051: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:31:17,976: WER debug example
  GT : you can see the code at this point as well
  PR : ou e a this
2026-01-04 22:31:18,002: WER debug example
  GT : how does it keep the cost down
  PR : us a
2026-01-04 22:31:19,350: Val batch 2000: PER (avg): 0.3375 CTC Loss (avg): 32.7887 WER(1gram): 96.95% (n=64) time: 6.435
2026-01-04 22:31:19,351: WER lens: avg_true_words=6.16 avg_pred_words=2.62 max_pred_words=7
2026-01-04 22:31:19,351: t15.2023.08.13 val PER: 0.2983
2026-01-04 22:31:19,351: t15.2023.08.18 val PER: 0.2741
2026-01-04 22:31:19,351: t15.2023.08.20 val PER: 0.2502
2026-01-04 22:31:19,351: t15.2023.08.25 val PER: 0.2380
2026-01-04 22:31:19,351: t15.2023.08.27 val PER: 0.3360
2026-01-04 22:31:19,351: t15.2023.09.01 val PER: 0.2354
2026-01-04 22:31:19,351: t15.2023.09.03 val PER: 0.3444
2026-01-04 22:31:19,351: t15.2023.09.24 val PER: 0.2658
2026-01-04 22:31:19,352: t15.2023.09.29 val PER: 0.2789
2026-01-04 22:31:19,352: t15.2023.10.01 val PER: 0.3283
2026-01-04 22:31:19,352: t15.2023.10.06 val PER: 0.2637
2026-01-04 22:31:19,352: t15.2023.10.08 val PER: 0.4046
2026-01-04 22:31:19,352: t15.2023.10.13 val PER: 0.3763
2026-01-04 22:31:19,352: t15.2023.10.15 val PER: 0.3072
2026-01-04 22:31:19,352: t15.2023.10.20 val PER: 0.2987
2026-01-04 22:31:19,352: t15.2023.10.22 val PER: 0.2795
2026-01-04 22:31:19,352: t15.2023.11.03 val PER: 0.3263
2026-01-04 22:31:19,352: t15.2023.11.04 val PER: 0.1195
2026-01-04 22:31:19,352: t15.2023.11.17 val PER: 0.1913
2026-01-04 22:31:19,352: t15.2023.11.19 val PER: 0.1677
2026-01-04 22:31:19,352: t15.2023.11.26 val PER: 0.3703
2026-01-04 22:31:19,352: t15.2023.12.03 val PER: 0.3141
2026-01-04 22:31:19,352: t15.2023.12.08 val PER: 0.3262
2026-01-04 22:31:19,353: t15.2023.12.10 val PER: 0.2799
2026-01-04 22:31:19,353: t15.2023.12.17 val PER: 0.3077
2026-01-04 22:31:19,353: t15.2023.12.29 val PER: 0.3459
2026-01-04 22:31:19,353: t15.2024.02.25 val PER: 0.3020
2026-01-04 22:31:19,353: t15.2024.03.08 val PER: 0.4253
2026-01-04 22:31:19,353: t15.2024.03.15 val PER: 0.3734
2026-01-04 22:31:19,353: t15.2024.03.17 val PER: 0.3417
2026-01-04 22:31:19,353: t15.2024.05.10 val PER: 0.3462
2026-01-04 22:31:19,353: t15.2024.06.14 val PER: 0.3596
2026-01-04 22:31:19,353: t15.2024.07.19 val PER: 0.4819
2026-01-04 22:31:19,353: t15.2024.07.21 val PER: 0.3048
2026-01-04 22:31:19,353: t15.2024.07.28 val PER: 0.3449
2026-01-04 22:31:19,353: t15.2025.01.10 val PER: 0.5482
2026-01-04 22:31:19,353: t15.2025.01.12 val PER: 0.3734
2026-01-04 22:31:19,353: t15.2025.03.14 val PER: 0.5503
2026-01-04 22:31:19,353: t15.2025.03.16 val PER: 0.4162
2026-01-04 22:31:19,354: t15.2025.03.30 val PER: 0.5724
2026-01-04 22:31:19,354: t15.2025.04.13 val PER: 0.4194
2026-01-04 22:31:19,355: New best val WER(1gram) 99.75% --> 96.95%
2026-01-04 22:31:19,355: Checkpointing model
2026-01-04 22:31:19,979: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:31:20,283: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_2000
2026-01-04 22:31:38,296: Train batch 2200: loss: 29.23 grad norm: 66.18 time: 0.060
2026-01-04 22:31:56,831: Train batch 2400: loss: 29.29 grad norm: 66.72 time: 0.052
2026-01-04 22:32:05,989: Running test after training batch: 2500
2026-01-04 22:32:06,102: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:32:10,986: WER debug example
  GT : you can see the code at this point as well
  PR : ou
2026-01-04 22:32:11,010: WER debug example
  GT : how does it keep the cost down
  PR : sci
2026-01-04 22:32:12,309: Val batch 2500: PER (avg): 0.3190 CTC Loss (avg): 30.2204 WER(1gram): 96.45% (n=64) time: 6.319
2026-01-04 22:32:12,309: WER lens: avg_true_words=6.16 avg_pred_words=2.03 max_pred_words=8
2026-01-04 22:32:12,310: t15.2023.08.13 val PER: 0.2765
2026-01-04 22:32:12,310: t15.2023.08.18 val PER: 0.2573
2026-01-04 22:32:12,310: t15.2023.08.20 val PER: 0.2431
2026-01-04 22:32:12,310: t15.2023.08.25 val PER: 0.2169
2026-01-04 22:32:12,310: t15.2023.08.27 val PER: 0.3376
2026-01-04 22:32:12,310: t15.2023.09.01 val PER: 0.2224
2026-01-04 22:32:12,310: t15.2023.09.03 val PER: 0.3135
2026-01-04 22:32:12,310: t15.2023.09.24 val PER: 0.2512
2026-01-04 22:32:12,310: t15.2023.09.29 val PER: 0.2693
2026-01-04 22:32:12,310: t15.2023.10.01 val PER: 0.3349
2026-01-04 22:32:12,310: t15.2023.10.06 val PER: 0.2400
2026-01-04 22:32:12,310: t15.2023.10.08 val PER: 0.3829
2026-01-04 22:32:12,310: t15.2023.10.13 val PER: 0.3701
2026-01-04 22:32:12,310: t15.2023.10.15 val PER: 0.2973
2026-01-04 22:32:12,311: t15.2023.10.20 val PER: 0.2886
2026-01-04 22:32:12,311: t15.2023.10.22 val PER: 0.2584
2026-01-04 22:32:12,311: t15.2023.11.03 val PER: 0.2965
2026-01-04 22:32:12,311: t15.2023.11.04 val PER: 0.0922
2026-01-04 22:32:12,311: t15.2023.11.17 val PER: 0.1788
2026-01-04 22:32:12,311: t15.2023.11.19 val PER: 0.1477
2026-01-04 22:32:12,311: t15.2023.11.26 val PER: 0.3312
2026-01-04 22:32:12,311: t15.2023.12.03 val PER: 0.2857
2026-01-04 22:32:12,311: t15.2023.12.08 val PER: 0.2963
2026-01-04 22:32:12,311: t15.2023.12.10 val PER: 0.2497
2026-01-04 22:32:12,311: t15.2023.12.17 val PER: 0.3046
2026-01-04 22:32:12,311: t15.2023.12.29 val PER: 0.3294
2026-01-04 22:32:12,311: t15.2024.02.25 val PER: 0.2711
2026-01-04 22:32:12,311: t15.2024.03.08 val PER: 0.3585
2026-01-04 22:32:12,311: t15.2024.03.15 val PER: 0.3621
2026-01-04 22:32:12,311: t15.2024.03.17 val PER: 0.3229
2026-01-04 22:32:12,311: t15.2024.05.10 val PER: 0.3254
2026-01-04 22:32:12,312: t15.2024.06.14 val PER: 0.3391
2026-01-04 22:32:12,312: t15.2024.07.19 val PER: 0.4634
2026-01-04 22:32:12,312: t15.2024.07.21 val PER: 0.2793
2026-01-04 22:32:12,312: t15.2024.07.28 val PER: 0.3213
2026-01-04 22:32:12,312: t15.2025.01.10 val PER: 0.5041
2026-01-04 22:32:12,312: t15.2025.01.12 val PER: 0.3626
2026-01-04 22:32:12,312: t15.2025.03.14 val PER: 0.5399
2026-01-04 22:32:12,312: t15.2025.03.16 val PER: 0.3887
2026-01-04 22:32:12,312: t15.2025.03.30 val PER: 0.5782
2026-01-04 22:32:12,312: t15.2025.04.13 val PER: 0.4094
2026-01-04 22:32:12,313: New best val WER(1gram) 96.95% --> 96.45%
2026-01-04 22:32:12,313: Checkpointing model
2026-01-04 22:32:12,957: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:32:13,231: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_2500
2026-01-04 22:32:21,930: Train batch 2600: loss: 35.30 grad norm: 79.55 time: 0.055
2026-01-04 22:32:39,568: Train batch 2800: loss: 25.55 grad norm: 77.30 time: 0.081
2026-01-04 22:32:57,328: Train batch 3000: loss: 32.12 grad norm: 78.45 time: 0.083
2026-01-04 22:32:57,328: Running test after training batch: 3000
2026-01-04 22:32:57,432: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:33:02,313: WER debug example
  GT : you can see the code at this point as well
  PR : ou e a this
2026-01-04 22:33:02,338: WER debug example
  GT : how does it keep the cost down
  PR : us the
2026-01-04 22:33:03,716: Val batch 3000: PER (avg): 0.2933 CTC Loss (avg): 27.9553 WER(1gram): 91.12% (n=64) time: 6.388
2026-01-04 22:33:03,716: WER lens: avg_true_words=6.16 avg_pred_words=3.08 max_pred_words=8
2026-01-04 22:33:03,716: t15.2023.08.13 val PER: 0.2651
2026-01-04 22:33:03,716: t15.2023.08.18 val PER: 0.2372
2026-01-04 22:33:03,717: t15.2023.08.20 val PER: 0.2264
2026-01-04 22:33:03,717: t15.2023.08.25 val PER: 0.1928
2026-01-04 22:33:03,717: t15.2023.08.27 val PER: 0.3103
2026-01-04 22:33:03,717: t15.2023.09.01 val PER: 0.1989
2026-01-04 22:33:03,717: t15.2023.09.03 val PER: 0.2933
2026-01-04 22:33:03,717: t15.2023.09.24 val PER: 0.2282
2026-01-04 22:33:03,717: t15.2023.09.29 val PER: 0.2393
2026-01-04 22:33:03,717: t15.2023.10.01 val PER: 0.3012
2026-01-04 22:33:03,717: t15.2023.10.06 val PER: 0.2121
2026-01-04 22:33:03,717: t15.2023.10.08 val PER: 0.3708
2026-01-04 22:33:03,717: t15.2023.10.13 val PER: 0.3382
2026-01-04 22:33:03,717: t15.2023.10.15 val PER: 0.2624
2026-01-04 22:33:03,718: t15.2023.10.20 val PER: 0.2819
2026-01-04 22:33:03,718: t15.2023.10.22 val PER: 0.2316
2026-01-04 22:33:03,718: t15.2023.11.03 val PER: 0.2849
2026-01-04 22:33:03,718: t15.2023.11.04 val PER: 0.1024
2026-01-04 22:33:03,718: t15.2023.11.17 val PER: 0.1446
2026-01-04 22:33:03,718: t15.2023.11.19 val PER: 0.1218
2026-01-04 22:33:03,718: t15.2023.11.26 val PER: 0.3109
2026-01-04 22:33:03,718: t15.2023.12.03 val PER: 0.2752
2026-01-04 22:33:03,718: t15.2023.12.08 val PER: 0.2743
2026-01-04 22:33:03,718: t15.2023.12.10 val PER: 0.2326
2026-01-04 22:33:03,718: t15.2023.12.17 val PER: 0.3015
2026-01-04 22:33:03,719: t15.2023.12.29 val PER: 0.2931
2026-01-04 22:33:03,719: t15.2024.02.25 val PER: 0.2584
2026-01-04 22:33:03,719: t15.2024.03.08 val PER: 0.3542
2026-01-04 22:33:03,719: t15.2024.03.15 val PER: 0.3427
2026-01-04 22:33:03,719: t15.2024.03.17 val PER: 0.2985
2026-01-04 22:33:03,719: t15.2024.05.10 val PER: 0.2972
2026-01-04 22:33:03,719: t15.2024.06.14 val PER: 0.3076
2026-01-04 22:33:03,719: t15.2024.07.19 val PER: 0.4107
2026-01-04 22:33:03,719: t15.2024.07.21 val PER: 0.2503
2026-01-04 22:33:03,719: t15.2024.07.28 val PER: 0.3051
2026-01-04 22:33:03,719: t15.2025.01.10 val PER: 0.4972
2026-01-04 22:33:03,719: t15.2025.01.12 val PER: 0.3326
2026-01-04 22:33:03,719: t15.2025.03.14 val PER: 0.4763
2026-01-04 22:33:03,719: t15.2025.03.16 val PER: 0.3482
2026-01-04 22:33:03,719: t15.2025.03.30 val PER: 0.5069
2026-01-04 22:33:03,720: t15.2025.04.13 val PER: 0.3695
2026-01-04 22:33:03,720: New best val WER(1gram) 96.45% --> 91.12%
2026-01-04 22:33:03,720: Checkpointing model
2026-01-04 22:33:04,341: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:33:04,613: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_3000
2026-01-04 22:33:22,774: Train batch 3200: loss: 26.19 grad norm: 64.38 time: 0.076
2026-01-04 22:33:41,113: Train batch 3400: loss: 19.44 grad norm: 59.39 time: 0.049
2026-01-04 22:33:50,484: Running test after training batch: 3500
2026-01-04 22:33:50,635: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:33:55,570: WER debug example
  GT : you can see the code at this point as well
  PR : yu e a this will
2026-01-04 22:33:55,598: WER debug example
  GT : how does it keep the cost down
  PR : sci t the
2026-01-04 22:33:57,017: Val batch 3500: PER (avg): 0.2809 CTC Loss (avg): 26.7632 WER(1gram): 90.10% (n=64) time: 6.532
2026-01-04 22:33:57,017: WER lens: avg_true_words=6.16 avg_pred_words=3.56 max_pred_words=8
2026-01-04 22:33:57,018: t15.2023.08.13 val PER: 0.2360
2026-01-04 22:33:57,018: t15.2023.08.18 val PER: 0.2246
2026-01-04 22:33:57,018: t15.2023.08.20 val PER: 0.2184
2026-01-04 22:33:57,018: t15.2023.08.25 val PER: 0.1928
2026-01-04 22:33:57,018: t15.2023.08.27 val PER: 0.2926
2026-01-04 22:33:57,018: t15.2023.09.01 val PER: 0.1810
2026-01-04 22:33:57,018: t15.2023.09.03 val PER: 0.2815
2026-01-04 22:33:57,018: t15.2023.09.24 val PER: 0.2221
2026-01-04 22:33:57,018: t15.2023.09.29 val PER: 0.2348
2026-01-04 22:33:57,018: t15.2023.10.01 val PER: 0.2847
2026-01-04 22:33:57,018: t15.2023.10.06 val PER: 0.1981
2026-01-04 22:33:57,018: t15.2023.10.08 val PER: 0.3599
2026-01-04 22:33:57,018: t15.2023.10.13 val PER: 0.3313
2026-01-04 22:33:57,019: t15.2023.10.15 val PER: 0.2544
2026-01-04 22:33:57,019: t15.2023.10.20 val PER: 0.2617
2026-01-04 22:33:57,019: t15.2023.10.22 val PER: 0.2294
2026-01-04 22:33:57,019: t15.2023.11.03 val PER: 0.2700
2026-01-04 22:33:57,019: t15.2023.11.04 val PER: 0.0819
2026-01-04 22:33:57,019: t15.2023.11.17 val PER: 0.1291
2026-01-04 22:33:57,019: t15.2023.11.19 val PER: 0.1098
2026-01-04 22:33:57,019: t15.2023.11.26 val PER: 0.2775
2026-01-04 22:33:57,019: t15.2023.12.03 val PER: 0.2384
2026-01-04 22:33:57,019: t15.2023.12.08 val PER: 0.2703
2026-01-04 22:33:57,019: t15.2023.12.10 val PER: 0.2129
2026-01-04 22:33:57,019: t15.2023.12.17 val PER: 0.2599
2026-01-04 22:33:57,019: t15.2023.12.29 val PER: 0.2739
2026-01-04 22:33:57,019: t15.2024.02.25 val PER: 0.2360
2026-01-04 22:33:57,019: t15.2024.03.08 val PER: 0.3514
2026-01-04 22:33:57,020: t15.2024.03.15 val PER: 0.3290
2026-01-04 22:33:57,020: t15.2024.03.17 val PER: 0.2943
2026-01-04 22:33:57,020: t15.2024.05.10 val PER: 0.2942
2026-01-04 22:33:57,020: t15.2024.06.14 val PER: 0.3060
2026-01-04 22:33:57,020: t15.2024.07.19 val PER: 0.4100
2026-01-04 22:33:57,020: t15.2024.07.21 val PER: 0.2359
2026-01-04 22:33:57,020: t15.2024.07.28 val PER: 0.2875
2026-01-04 22:33:57,020: t15.2025.01.10 val PER: 0.5069
2026-01-04 22:33:57,020: t15.2025.01.12 val PER: 0.3264
2026-01-04 22:33:57,020: t15.2025.03.14 val PER: 0.4837
2026-01-04 22:33:57,020: t15.2025.03.16 val PER: 0.3560
2026-01-04 22:33:57,020: t15.2025.03.30 val PER: 0.4828
2026-01-04 22:33:57,020: t15.2025.04.13 val PER: 0.3581
2026-01-04 22:33:57,021: New best val WER(1gram) 91.12% --> 90.10%
2026-01-04 22:33:57,021: Checkpointing model
2026-01-04 22:33:57,660: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:33:57,931: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_3500
2026-01-04 22:34:06,965: Train batch 3600: loss: 22.82 grad norm: 66.29 time: 0.067
2026-01-04 22:34:24,985: Train batch 3800: loss: 25.63 grad norm: 64.71 time: 0.066
2026-01-04 22:34:43,173: Train batch 4000: loss: 19.37 grad norm: 57.47 time: 0.056
2026-01-04 22:34:43,174: Running test after training batch: 4000
2026-01-04 22:34:43,335: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:34:48,232: WER debug example
  GT : you can see the code at this point as well
  PR : ou e a this will
2026-01-04 22:34:48,260: WER debug example
  GT : how does it keep the cost down
  PR : it pu the
2026-01-04 22:34:49,699: Val batch 4000: PER (avg): 0.2594 CTC Loss (avg): 24.9318 WER(1gram): 82.99% (n=64) time: 6.525
2026-01-04 22:34:49,700: WER lens: avg_true_words=6.16 avg_pred_words=3.72 max_pred_words=8
2026-01-04 22:34:49,700: t15.2023.08.13 val PER: 0.2308
2026-01-04 22:34:49,700: t15.2023.08.18 val PER: 0.2205
2026-01-04 22:34:49,700: t15.2023.08.20 val PER: 0.2033
2026-01-04 22:34:49,700: t15.2023.08.25 val PER: 0.1807
2026-01-04 22:34:49,700: t15.2023.08.27 val PER: 0.2910
2026-01-04 22:34:49,700: t15.2023.09.01 val PER: 0.1648
2026-01-04 22:34:49,700: t15.2023.09.03 val PER: 0.2637
2026-01-04 22:34:49,700: t15.2023.09.24 val PER: 0.2039
2026-01-04 22:34:49,700: t15.2023.09.29 val PER: 0.2183
2026-01-04 22:34:49,700: t15.2023.10.01 val PER: 0.2629
2026-01-04 22:34:49,700: t15.2023.10.06 val PER: 0.1690
2026-01-04 22:34:49,700: t15.2023.10.08 val PER: 0.3383
2026-01-04 22:34:49,701: t15.2023.10.13 val PER: 0.3134
2026-01-04 22:34:49,701: t15.2023.10.15 val PER: 0.2353
2026-01-04 22:34:49,701: t15.2023.10.20 val PER: 0.2685
2026-01-04 22:34:49,701: t15.2023.10.22 val PER: 0.2027
2026-01-04 22:34:49,701: t15.2023.11.03 val PER: 0.2449
2026-01-04 22:34:49,701: t15.2023.11.04 val PER: 0.0853
2026-01-04 22:34:49,701: t15.2023.11.17 val PER: 0.1042
2026-01-04 22:34:49,701: t15.2023.11.19 val PER: 0.1038
2026-01-04 22:34:49,701: t15.2023.11.26 val PER: 0.2601
2026-01-04 22:34:49,701: t15.2023.12.03 val PER: 0.2174
2026-01-04 22:34:49,701: t15.2023.12.08 val PER: 0.2417
2026-01-04 22:34:49,701: t15.2023.12.10 val PER: 0.1984
2026-01-04 22:34:49,701: t15.2023.12.17 val PER: 0.2557
2026-01-04 22:34:49,701: t15.2023.12.29 val PER: 0.2759
2026-01-04 22:34:49,702: t15.2024.02.25 val PER: 0.2163
2026-01-04 22:34:49,702: t15.2024.03.08 val PER: 0.3272
2026-01-04 22:34:49,702: t15.2024.03.15 val PER: 0.3021
2026-01-04 22:34:49,702: t15.2024.03.17 val PER: 0.2608
2026-01-04 22:34:49,702: t15.2024.05.10 val PER: 0.2749
2026-01-04 22:34:49,702: t15.2024.06.14 val PER: 0.2792
2026-01-04 22:34:49,702: t15.2024.07.19 val PER: 0.3850
2026-01-04 22:34:49,702: t15.2024.07.21 val PER: 0.2193
2026-01-04 22:34:49,702: t15.2024.07.28 val PER: 0.2544
2026-01-04 22:34:49,702: t15.2025.01.10 val PER: 0.4366
2026-01-04 22:34:49,702: t15.2025.01.12 val PER: 0.2902
2026-01-04 22:34:49,702: t15.2025.03.14 val PER: 0.4467
2026-01-04 22:34:49,702: t15.2025.03.16 val PER: 0.3089
2026-01-04 22:34:49,702: t15.2025.03.30 val PER: 0.4345
2026-01-04 22:34:49,702: t15.2025.04.13 val PER: 0.3409
2026-01-04 22:34:49,703: New best val WER(1gram) 90.10% --> 82.99%
2026-01-04 22:34:49,703: Checkpointing model
2026-01-04 22:34:50,334: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:34:50,632: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_4000
2026-01-04 22:35:08,734: Train batch 4200: loss: 23.18 grad norm: 68.50 time: 0.079
2026-01-04 22:35:27,020: Train batch 4400: loss: 17.68 grad norm: 62.79 time: 0.066
2026-01-04 22:35:36,179: Running test after training batch: 4500
2026-01-04 22:35:36,341: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:35:41,235: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e a this boye oui
2026-01-04 22:35:41,260: WER debug example
  GT : how does it keep the cost down
  PR : how it the
2026-01-04 22:35:42,641: Val batch 4500: PER (avg): 0.2481 CTC Loss (avg): 23.5010 WER(1gram): 86.29% (n=64) time: 6.461
2026-01-04 22:35:42,641: WER lens: avg_true_words=6.16 avg_pred_words=4.05 max_pred_words=10
2026-01-04 22:35:42,641: t15.2023.08.13 val PER: 0.2121
2026-01-04 22:35:42,641: t15.2023.08.18 val PER: 0.2012
2026-01-04 22:35:42,641: t15.2023.08.20 val PER: 0.1906
2026-01-04 22:35:42,641: t15.2023.08.25 val PER: 0.1431
2026-01-04 22:35:42,641: t15.2023.08.27 val PER: 0.2830
2026-01-04 22:35:42,642: t15.2023.09.01 val PER: 0.1705
2026-01-04 22:35:42,642: t15.2023.09.03 val PER: 0.2482
2026-01-04 22:35:42,642: t15.2023.09.24 val PER: 0.1978
2026-01-04 22:35:42,642: t15.2023.09.29 val PER: 0.2080
2026-01-04 22:35:42,642: t15.2023.10.01 val PER: 0.2675
2026-01-04 22:35:42,642: t15.2023.10.06 val PER: 0.1582
2026-01-04 22:35:42,642: t15.2023.10.08 val PER: 0.3369
2026-01-04 22:35:42,642: t15.2023.10.13 val PER: 0.3018
2026-01-04 22:35:42,642: t15.2023.10.15 val PER: 0.2287
2026-01-04 22:35:42,642: t15.2023.10.20 val PER: 0.2349
2026-01-04 22:35:42,642: t15.2023.10.22 val PER: 0.1938
2026-01-04 22:35:42,642: t15.2023.11.03 val PER: 0.2456
2026-01-04 22:35:42,642: t15.2023.11.04 val PER: 0.0614
2026-01-04 22:35:42,642: t15.2023.11.17 val PER: 0.0918
2026-01-04 22:35:42,642: t15.2023.11.19 val PER: 0.0998
2026-01-04 22:35:42,643: t15.2023.11.26 val PER: 0.2471
2026-01-04 22:35:42,643: t15.2023.12.03 val PER: 0.2195
2026-01-04 22:35:42,643: t15.2023.12.08 val PER: 0.2310
2026-01-04 22:35:42,643: t15.2023.12.10 val PER: 0.1958
2026-01-04 22:35:42,643: t15.2023.12.17 val PER: 0.2370
2026-01-04 22:35:42,643: t15.2023.12.29 val PER: 0.2615
2026-01-04 22:35:42,643: t15.2024.02.25 val PER: 0.2022
2026-01-04 22:35:42,643: t15.2024.03.08 val PER: 0.3115
2026-01-04 22:35:42,643: t15.2024.03.15 val PER: 0.2908
2026-01-04 22:35:42,643: t15.2024.03.17 val PER: 0.2483
2026-01-04 22:35:42,643: t15.2024.05.10 val PER: 0.2689
2026-01-04 22:35:42,643: t15.2024.06.14 val PER: 0.2587
2026-01-04 22:35:42,643: t15.2024.07.19 val PER: 0.3467
2026-01-04 22:35:42,643: t15.2024.07.21 val PER: 0.1945
2026-01-04 22:35:42,643: t15.2024.07.28 val PER: 0.2463
2026-01-04 22:35:42,643: t15.2025.01.10 val PER: 0.4229
2026-01-04 22:35:42,643: t15.2025.01.12 val PER: 0.2756
2026-01-04 22:35:42,644: t15.2025.03.14 val PER: 0.4112
2026-01-04 22:35:42,644: t15.2025.03.16 val PER: 0.3259
2026-01-04 22:35:42,644: t15.2025.03.30 val PER: 0.4414
2026-01-04 22:35:42,644: t15.2025.04.13 val PER: 0.3310
2026-01-04 22:35:42,907: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_4500
2026-01-04 22:35:52,025: Train batch 4600: loss: 20.93 grad norm: 59.51 time: 0.062
2026-01-04 22:36:10,487: Train batch 4800: loss: 15.45 grad norm: 57.40 time: 0.063
2026-01-04 22:36:29,335: Train batch 5000: loss: 33.94 grad norm: 87.33 time: 0.064
2026-01-04 22:36:29,336: Running test after training batch: 5000
2026-01-04 22:36:29,472: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:36:34,331: WER debug example
  GT : you can see the code at this point as well
  PR : yu kenn e the cold this boye will
2026-01-04 22:36:34,359: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the
2026-01-04 22:36:35,830: Val batch 5000: PER (avg): 0.2341 CTC Loss (avg): 22.6894 WER(1gram): 76.14% (n=64) time: 6.494
2026-01-04 22:36:35,830: WER lens: avg_true_words=6.16 avg_pred_words=4.44 max_pred_words=9
2026-01-04 22:36:35,830: t15.2023.08.13 val PER: 0.1996
2026-01-04 22:36:35,830: t15.2023.08.18 val PER: 0.1886
2026-01-04 22:36:35,830: t15.2023.08.20 val PER: 0.1771
2026-01-04 22:36:35,830: t15.2023.08.25 val PER: 0.1596
2026-01-04 22:36:35,830: t15.2023.08.27 val PER: 0.2524
2026-01-04 22:36:35,830: t15.2023.09.01 val PER: 0.1356
2026-01-04 22:36:35,830: t15.2023.09.03 val PER: 0.2435
2026-01-04 22:36:35,831: t15.2023.09.24 val PER: 0.1833
2026-01-04 22:36:35,831: t15.2023.09.29 val PER: 0.1908
2026-01-04 22:36:35,831: t15.2023.10.01 val PER: 0.2279
2026-01-04 22:36:35,831: t15.2023.10.06 val PER: 0.1529
2026-01-04 22:36:35,831: t15.2023.10.08 val PER: 0.3112
2026-01-04 22:36:35,831: t15.2023.10.13 val PER: 0.2863
2026-01-04 22:36:35,831: t15.2023.10.15 val PER: 0.2136
2026-01-04 22:36:35,831: t15.2023.10.20 val PER: 0.2584
2026-01-04 22:36:35,831: t15.2023.10.22 val PER: 0.1860
2026-01-04 22:36:35,831: t15.2023.11.03 val PER: 0.2347
2026-01-04 22:36:35,831: t15.2023.11.04 val PER: 0.0717
2026-01-04 22:36:35,831: t15.2023.11.17 val PER: 0.0902
2026-01-04 22:36:35,831: t15.2023.11.19 val PER: 0.0898
2026-01-04 22:36:35,831: t15.2023.11.26 val PER: 0.2399
2026-01-04 22:36:35,831: t15.2023.12.03 val PER: 0.2090
2026-01-04 22:36:35,832: t15.2023.12.08 val PER: 0.2137
2026-01-04 22:36:35,832: t15.2023.12.10 val PER: 0.1774
2026-01-04 22:36:35,832: t15.2023.12.17 val PER: 0.2225
2026-01-04 22:36:35,832: t15.2023.12.29 val PER: 0.2313
2026-01-04 22:36:35,832: t15.2024.02.25 val PER: 0.1994
2026-01-04 22:36:35,832: t15.2024.03.08 val PER: 0.3030
2026-01-04 22:36:35,832: t15.2024.03.15 val PER: 0.2852
2026-01-04 22:36:35,832: t15.2024.03.17 val PER: 0.2420
2026-01-04 22:36:35,832: t15.2024.05.10 val PER: 0.2496
2026-01-04 22:36:35,832: t15.2024.06.14 val PER: 0.2587
2026-01-04 22:36:35,832: t15.2024.07.19 val PER: 0.3355
2026-01-04 22:36:35,832: t15.2024.07.21 val PER: 0.1952
2026-01-04 22:36:35,832: t15.2024.07.28 val PER: 0.2434
2026-01-04 22:36:35,832: t15.2025.01.10 val PER: 0.4091
2026-01-04 22:36:35,832: t15.2025.01.12 val PER: 0.2479
2026-01-04 22:36:35,832: t15.2025.03.14 val PER: 0.4157
2026-01-04 22:36:35,832: t15.2025.03.16 val PER: 0.2906
2026-01-04 22:36:35,833: t15.2025.03.30 val PER: 0.3989
2026-01-04 22:36:35,833: t15.2025.04.13 val PER: 0.3153
2026-01-04 22:36:35,834: New best val WER(1gram) 82.99% --> 76.14%
2026-01-04 22:36:35,834: Checkpointing model
2026-01-04 22:36:36,467: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:36:36,739: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_5000
2026-01-04 22:36:54,672: Train batch 5200: loss: 17.58 grad norm: 60.42 time: 0.052
2026-01-04 22:37:12,965: Train batch 5400: loss: 18.35 grad norm: 63.08 time: 0.067
2026-01-04 22:37:22,139: Running test after training batch: 5500
2026-01-04 22:37:22,246: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:37:27,140: WER debug example
  GT : you can see the code at this point as well
  PR : yu e a cold this will
2026-01-04 22:37:27,170: WER debug example
  GT : how does it keep the cost down
  PR : aue it the cussed
2026-01-04 22:37:28,683: Val batch 5500: PER (avg): 0.2223 CTC Loss (avg): 21.4027 WER(1gram): 80.46% (n=64) time: 6.543
2026-01-04 22:37:28,683: WER lens: avg_true_words=6.16 avg_pred_words=4.23 max_pred_words=9
2026-01-04 22:37:28,683: t15.2023.08.13 val PER: 0.1913
2026-01-04 22:37:28,683: t15.2023.08.18 val PER: 0.1836
2026-01-04 22:37:28,683: t15.2023.08.20 val PER: 0.1692
2026-01-04 22:37:28,683: t15.2023.08.25 val PER: 0.1536
2026-01-04 22:37:28,683: t15.2023.08.27 val PER: 0.2605
2026-01-04 22:37:28,683: t15.2023.09.01 val PER: 0.1331
2026-01-04 22:37:28,683: t15.2023.09.03 val PER: 0.2197
2026-01-04 22:37:28,684: t15.2023.09.24 val PER: 0.1869
2026-01-04 22:37:28,684: t15.2023.09.29 val PER: 0.1857
2026-01-04 22:37:28,684: t15.2023.10.01 val PER: 0.2318
2026-01-04 22:37:28,684: t15.2023.10.06 val PER: 0.1464
2026-01-04 22:37:28,684: t15.2023.10.08 val PER: 0.2977
2026-01-04 22:37:28,684: t15.2023.10.13 val PER: 0.2801
2026-01-04 22:37:28,684: t15.2023.10.15 val PER: 0.2123
2026-01-04 22:37:28,684: t15.2023.10.20 val PER: 0.2685
2026-01-04 22:37:28,684: t15.2023.10.22 val PER: 0.1704
2026-01-04 22:37:28,684: t15.2023.11.03 val PER: 0.2218
2026-01-04 22:37:28,684: t15.2023.11.04 val PER: 0.0580
2026-01-04 22:37:28,684: t15.2023.11.17 val PER: 0.0855
2026-01-04 22:37:28,684: t15.2023.11.19 val PER: 0.0838
2026-01-04 22:37:28,684: t15.2023.11.26 val PER: 0.2254
2026-01-04 22:37:28,685: t15.2023.12.03 val PER: 0.1912
2026-01-04 22:37:28,685: t15.2023.12.08 val PER: 0.1977
2026-01-04 22:37:28,685: t15.2023.12.10 val PER: 0.1682
2026-01-04 22:37:28,685: t15.2023.12.17 val PER: 0.2121
2026-01-04 22:37:28,685: t15.2023.12.29 val PER: 0.2141
2026-01-04 22:37:28,685: t15.2024.02.25 val PER: 0.1770
2026-01-04 22:37:28,685: t15.2024.03.08 val PER: 0.2888
2026-01-04 22:37:28,685: t15.2024.03.15 val PER: 0.2627
2026-01-04 22:37:28,685: t15.2024.03.17 val PER: 0.2176
2026-01-04 22:37:28,685: t15.2024.05.10 val PER: 0.2333
2026-01-04 22:37:28,685: t15.2024.06.14 val PER: 0.2492
2026-01-04 22:37:28,685: t15.2024.07.19 val PER: 0.3349
2026-01-04 22:37:28,685: t15.2024.07.21 val PER: 0.1648
2026-01-04 22:37:28,685: t15.2024.07.28 val PER: 0.2257
2026-01-04 22:37:28,685: t15.2025.01.10 val PER: 0.3898
2026-01-04 22:37:28,685: t15.2025.01.12 val PER: 0.2402
2026-01-04 22:37:28,686: t15.2025.03.14 val PER: 0.3802
2026-01-04 22:37:28,686: t15.2025.03.16 val PER: 0.2592
2026-01-04 22:37:28,686: t15.2025.03.30 val PER: 0.3851
2026-01-04 22:37:28,686: t15.2025.04.13 val PER: 0.3039
2026-01-04 22:37:28,959: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_5500
2026-01-04 22:37:37,837: Train batch 5600: loss: 21.44 grad norm: 73.41 time: 0.062
2026-01-04 22:37:55,705: Train batch 5800: loss: 14.78 grad norm: 61.17 time: 0.082
2026-01-04 22:38:13,451: Train batch 6000: loss: 14.31 grad norm: 60.55 time: 0.048
2026-01-04 22:38:13,452: Running test after training batch: 6000
2026-01-04 22:38:13,569: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:38:18,537: WER debug example
  GT : you can see the code at this point as well
  PR : ou e the ko this boye will
2026-01-04 22:38:18,564: WER debug example
  GT : how does it keep the cost down
  PR : it the
2026-01-04 22:38:20,040: Val batch 6000: PER (avg): 0.2174 CTC Loss (avg): 20.7846 WER(1gram): 81.22% (n=64) time: 6.588
2026-01-04 22:38:20,040: WER lens: avg_true_words=6.16 avg_pred_words=3.62 max_pred_words=8
2026-01-04 22:38:20,041: t15.2023.08.13 val PER: 0.1861
2026-01-04 22:38:20,041: t15.2023.08.18 val PER: 0.1869
2026-01-04 22:38:20,041: t15.2023.08.20 val PER: 0.1636
2026-01-04 22:38:20,041: t15.2023.08.25 val PER: 0.1446
2026-01-04 22:38:20,041: t15.2023.08.27 val PER: 0.2605
2026-01-04 22:38:20,041: t15.2023.09.01 val PER: 0.1461
2026-01-04 22:38:20,041: t15.2023.09.03 val PER: 0.2292
2026-01-04 22:38:20,041: t15.2023.09.24 val PER: 0.1650
2026-01-04 22:38:20,041: t15.2023.09.29 val PER: 0.1825
2026-01-04 22:38:20,041: t15.2023.10.01 val PER: 0.2226
2026-01-04 22:38:20,042: t15.2023.10.06 val PER: 0.1367
2026-01-04 22:38:20,042: t15.2023.10.08 val PER: 0.3031
2026-01-04 22:38:20,042: t15.2023.10.13 val PER: 0.2708
2026-01-04 22:38:20,042: t15.2023.10.15 val PER: 0.2169
2026-01-04 22:38:20,042: t15.2023.10.20 val PER: 0.2483
2026-01-04 22:38:20,042: t15.2023.10.22 val PER: 0.1748
2026-01-04 22:38:20,042: t15.2023.11.03 val PER: 0.2232
2026-01-04 22:38:20,042: t15.2023.11.04 val PER: 0.0580
2026-01-04 22:38:20,042: t15.2023.11.17 val PER: 0.0809
2026-01-04 22:38:20,042: t15.2023.11.19 val PER: 0.0818
2026-01-04 22:38:20,042: t15.2023.11.26 val PER: 0.2210
2026-01-04 22:38:20,042: t15.2023.12.03 val PER: 0.1744
2026-01-04 22:38:20,043: t15.2023.12.08 val PER: 0.1904
2026-01-04 22:38:20,043: t15.2023.12.10 val PER: 0.1761
2026-01-04 22:38:20,043: t15.2023.12.17 val PER: 0.1965
2026-01-04 22:38:20,043: t15.2023.12.29 val PER: 0.2189
2026-01-04 22:38:20,043: t15.2024.02.25 val PER: 0.1742
2026-01-04 22:38:20,043: t15.2024.03.08 val PER: 0.2774
2026-01-04 22:38:20,043: t15.2024.03.15 val PER: 0.2652
2026-01-04 22:38:20,043: t15.2024.03.17 val PER: 0.2232
2026-01-04 22:38:20,043: t15.2024.05.10 val PER: 0.2214
2026-01-04 22:38:20,043: t15.2024.06.14 val PER: 0.2192
2026-01-04 22:38:20,043: t15.2024.07.19 val PER: 0.3316
2026-01-04 22:38:20,043: t15.2024.07.21 val PER: 0.1614
2026-01-04 22:38:20,043: t15.2024.07.28 val PER: 0.2147
2026-01-04 22:38:20,043: t15.2025.01.10 val PER: 0.3678
2026-01-04 22:38:20,043: t15.2025.01.12 val PER: 0.2302
2026-01-04 22:38:20,043: t15.2025.03.14 val PER: 0.3654
2026-01-04 22:38:20,043: t15.2025.03.16 val PER: 0.2749
2026-01-04 22:38:20,044: t15.2025.03.30 val PER: 0.3540
2026-01-04 22:38:20,044: t15.2025.04.13 val PER: 0.2725
2026-01-04 22:38:20,327: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_6000
2026-01-04 22:38:38,035: Train batch 6200: loss: 16.19 grad norm: 61.16 time: 0.070
2026-01-04 22:38:55,743: Train batch 6400: loss: 21.56 grad norm: 74.27 time: 0.062
2026-01-04 22:39:04,492: Running test after training batch: 6500
2026-01-04 22:39:04,634: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:39:09,575: WER debug example
  GT : you can see the code at this point as well
  PR : yu kenn e the cold this point will
2026-01-04 22:39:09,601: WER debug example
  GT : how does it keep the cost down
  PR : how duh it keep the cost
2026-01-04 22:39:11,080: Val batch 6500: PER (avg): 0.2098 CTC Loss (avg): 20.6074 WER(1gram): 73.10% (n=64) time: 6.588
2026-01-04 22:39:11,080: WER lens: avg_true_words=6.16 avg_pred_words=4.92 max_pred_words=9
2026-01-04 22:39:11,081: t15.2023.08.13 val PER: 0.1694
2026-01-04 22:39:11,081: t15.2023.08.18 val PER: 0.1584
2026-01-04 22:39:11,081: t15.2023.08.20 val PER: 0.1509
2026-01-04 22:39:11,081: t15.2023.08.25 val PER: 0.1280
2026-01-04 22:39:11,081: t15.2023.08.27 val PER: 0.2428
2026-01-04 22:39:11,081: t15.2023.09.01 val PER: 0.1250
2026-01-04 22:39:11,081: t15.2023.09.03 val PER: 0.2185
2026-01-04 22:39:11,081: t15.2023.09.24 val PER: 0.1675
2026-01-04 22:39:11,081: t15.2023.09.29 val PER: 0.1659
2026-01-04 22:39:11,081: t15.2023.10.01 val PER: 0.2246
2026-01-04 22:39:11,081: t15.2023.10.06 val PER: 0.1432
2026-01-04 22:39:11,081: t15.2023.10.08 val PER: 0.2896
2026-01-04 22:39:11,081: t15.2023.10.13 val PER: 0.2684
2026-01-04 22:39:11,081: t15.2023.10.15 val PER: 0.2017
2026-01-04 22:39:11,081: t15.2023.10.20 val PER: 0.2550
2026-01-04 22:39:11,082: t15.2023.10.22 val PER: 0.1648
2026-01-04 22:39:11,082: t15.2023.11.03 val PER: 0.2185
2026-01-04 22:39:11,082: t15.2023.11.04 val PER: 0.0478
2026-01-04 22:39:11,082: t15.2023.11.17 val PER: 0.0762
2026-01-04 22:39:11,082: t15.2023.11.19 val PER: 0.0798
2026-01-04 22:39:11,082: t15.2023.11.26 val PER: 0.2109
2026-01-04 22:39:11,082: t15.2023.12.03 val PER: 0.1796
2026-01-04 22:39:11,082: t15.2023.12.08 val PER: 0.1811
2026-01-04 22:39:11,082: t15.2023.12.10 val PER: 0.1590
2026-01-04 22:39:11,082: t15.2023.12.17 val PER: 0.2027
2026-01-04 22:39:11,082: t15.2023.12.29 val PER: 0.2114
2026-01-04 22:39:11,082: t15.2024.02.25 val PER: 0.1587
2026-01-04 22:39:11,082: t15.2024.03.08 val PER: 0.2831
2026-01-04 22:39:11,082: t15.2024.03.15 val PER: 0.2639
2026-01-04 22:39:11,082: t15.2024.03.17 val PER: 0.2134
2026-01-04 22:39:11,083: t15.2024.05.10 val PER: 0.2169
2026-01-04 22:39:11,083: t15.2024.06.14 val PER: 0.2192
2026-01-04 22:39:11,083: t15.2024.07.19 val PER: 0.3158
2026-01-04 22:39:11,083: t15.2024.07.21 val PER: 0.1572
2026-01-04 22:39:11,083: t15.2024.07.28 val PER: 0.2074
2026-01-04 22:39:11,083: t15.2025.01.10 val PER: 0.3623
2026-01-04 22:39:11,083: t15.2025.01.12 val PER: 0.2225
2026-01-04 22:39:11,083: t15.2025.03.14 val PER: 0.3772
2026-01-04 22:39:11,083: t15.2025.03.16 val PER: 0.2709
2026-01-04 22:39:11,083: t15.2025.03.30 val PER: 0.3448
2026-01-04 22:39:11,083: t15.2025.04.13 val PER: 0.2782
2026-01-04 22:39:11,084: New best val WER(1gram) 76.14% --> 73.10%
2026-01-04 22:39:11,084: Checkpointing model
2026-01-04 22:39:11,708: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:39:11,980: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_6500
2026-01-04 22:39:20,697: Train batch 6600: loss: 12.23 grad norm: 49.86 time: 0.045
2026-01-04 22:39:38,620: Train batch 6800: loss: 15.66 grad norm: 56.58 time: 0.048
2026-01-04 22:39:56,435: Train batch 7000: loss: 16.92 grad norm: 62.05 time: 0.060
2026-01-04 22:39:56,435: Running test after training batch: 7000
2026-01-04 22:39:56,591: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:40:01,426: WER debug example
  GT : you can see the code at this point as well
  PR : yu kenn e the cold this boye will
2026-01-04 22:40:01,455: WER debug example
  GT : how does it keep the cost down
  PR : how duh it the
2026-01-04 22:40:02,983: Val batch 7000: PER (avg): 0.1994 CTC Loss (avg): 19.3688 WER(1gram): 78.17% (n=64) time: 6.548
2026-01-04 22:40:02,983: WER lens: avg_true_words=6.16 avg_pred_words=4.39 max_pred_words=10
2026-01-04 22:40:02,983: t15.2023.08.13 val PER: 0.1570
2026-01-04 22:40:02,983: t15.2023.08.18 val PER: 0.1492
2026-01-04 22:40:02,984: t15.2023.08.20 val PER: 0.1525
2026-01-04 22:40:02,984: t15.2023.08.25 val PER: 0.1220
2026-01-04 22:40:02,984: t15.2023.08.27 val PER: 0.2331
2026-01-04 22:40:02,984: t15.2023.09.01 val PER: 0.1193
2026-01-04 22:40:02,984: t15.2023.09.03 val PER: 0.1995
2026-01-04 22:40:02,984: t15.2023.09.24 val PER: 0.1553
2026-01-04 22:40:02,984: t15.2023.09.29 val PER: 0.1646
2026-01-04 22:40:02,985: t15.2023.10.01 val PER: 0.2153
2026-01-04 22:40:02,985: t15.2023.10.06 val PER: 0.1216
2026-01-04 22:40:02,985: t15.2023.10.08 val PER: 0.2774
2026-01-04 22:40:02,985: t15.2023.10.13 val PER: 0.2514
2026-01-04 22:40:02,985: t15.2023.10.15 val PER: 0.1918
2026-01-04 22:40:02,985: t15.2023.10.20 val PER: 0.2148
2026-01-04 22:40:02,985: t15.2023.10.22 val PER: 0.1537
2026-01-04 22:40:02,985: t15.2023.11.03 val PER: 0.2157
2026-01-04 22:40:02,985: t15.2023.11.04 val PER: 0.0410
2026-01-04 22:40:02,985: t15.2023.11.17 val PER: 0.0684
2026-01-04 22:40:02,986: t15.2023.11.19 val PER: 0.0639
2026-01-04 22:40:02,986: t15.2023.11.26 val PER: 0.1993
2026-01-04 22:40:02,986: t15.2023.12.03 val PER: 0.1660
2026-01-04 22:40:02,986: t15.2023.12.08 val PER: 0.1658
2026-01-04 22:40:02,986: t15.2023.12.10 val PER: 0.1367
2026-01-04 22:40:02,986: t15.2023.12.17 val PER: 0.1892
2026-01-04 22:40:02,986: t15.2023.12.29 val PER: 0.1990
2026-01-04 22:40:02,986: t15.2024.02.25 val PER: 0.1671
2026-01-04 22:40:02,986: t15.2024.03.08 val PER: 0.2688
2026-01-04 22:40:02,986: t15.2024.03.15 val PER: 0.2495
2026-01-04 22:40:02,987: t15.2024.03.17 val PER: 0.2001
2026-01-04 22:40:02,987: t15.2024.05.10 val PER: 0.2140
2026-01-04 22:40:02,987: t15.2024.06.14 val PER: 0.2098
2026-01-04 22:40:02,987: t15.2024.07.19 val PER: 0.3131
2026-01-04 22:40:02,987: t15.2024.07.21 val PER: 0.1359
2026-01-04 22:40:02,987: t15.2024.07.28 val PER: 0.1934
2026-01-04 22:40:02,987: t15.2025.01.10 val PER: 0.3650
2026-01-04 22:40:02,987: t15.2025.01.12 val PER: 0.2179
2026-01-04 22:40:02,987: t15.2025.03.14 val PER: 0.3565
2026-01-04 22:40:02,988: t15.2025.03.16 val PER: 0.2474
2026-01-04 22:40:02,988: t15.2025.03.30 val PER: 0.3586
2026-01-04 22:40:02,988: t15.2025.04.13 val PER: 0.2639
2026-01-04 22:40:03,246: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_7000
2026-01-04 22:40:21,008: Train batch 7200: loss: 14.74 grad norm: 58.94 time: 0.078
2026-01-04 22:40:38,939: Train batch 7400: loss: 15.04 grad norm: 61.30 time: 0.075
2026-01-04 22:40:47,958: Running test after training batch: 7500
2026-01-04 22:40:48,172: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:40:53,080: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e the cold at this boye will
2026-01-04 22:40:53,109: WER debug example
  GT : how does it keep the cost down
  PR : how duh it keep the cussed
2026-01-04 22:40:54,679: Val batch 7500: PER (avg): 0.1935 CTC Loss (avg): 19.0542 WER(1gram): 71.83% (n=64) time: 6.721
2026-01-04 22:40:54,680: WER lens: avg_true_words=6.16 avg_pred_words=5.00 max_pred_words=10
2026-01-04 22:40:54,680: t15.2023.08.13 val PER: 0.1653
2026-01-04 22:40:54,680: t15.2023.08.18 val PER: 0.1475
2026-01-04 22:40:54,680: t15.2023.08.20 val PER: 0.1406
2026-01-04 22:40:54,680: t15.2023.08.25 val PER: 0.1220
2026-01-04 22:40:54,680: t15.2023.08.27 val PER: 0.2122
2026-01-04 22:40:54,680: t15.2023.09.01 val PER: 0.1185
2026-01-04 22:40:54,680: t15.2023.09.03 val PER: 0.1971
2026-01-04 22:40:54,680: t15.2023.09.24 val PER: 0.1553
2026-01-04 22:40:54,681: t15.2023.09.29 val PER: 0.1640
2026-01-04 22:40:54,681: t15.2023.10.01 val PER: 0.2001
2026-01-04 22:40:54,681: t15.2023.10.06 val PER: 0.1206
2026-01-04 22:40:54,681: t15.2023.10.08 val PER: 0.2760
2026-01-04 22:40:54,681: t15.2023.10.13 val PER: 0.2374
2026-01-04 22:40:54,681: t15.2023.10.15 val PER: 0.1905
2026-01-04 22:40:54,681: t15.2023.10.20 val PER: 0.2383
2026-01-04 22:40:54,681: t15.2023.10.22 val PER: 0.1492
2026-01-04 22:40:54,681: t15.2023.11.03 val PER: 0.2049
2026-01-04 22:40:54,681: t15.2023.11.04 val PER: 0.0478
2026-01-04 22:40:54,681: t15.2023.11.17 val PER: 0.0700
2026-01-04 22:40:54,682: t15.2023.11.19 val PER: 0.0739
2026-01-04 22:40:54,682: t15.2023.11.26 val PER: 0.1935
2026-01-04 22:40:54,682: t15.2023.12.03 val PER: 0.1628
2026-01-04 22:40:54,682: t15.2023.12.08 val PER: 0.1558
2026-01-04 22:40:54,682: t15.2023.12.10 val PER: 0.1551
2026-01-04 22:40:54,682: t15.2023.12.17 val PER: 0.1746
2026-01-04 22:40:54,682: t15.2023.12.29 val PER: 0.1894
2026-01-04 22:40:54,682: t15.2024.02.25 val PER: 0.1545
2026-01-04 22:40:54,682: t15.2024.03.08 val PER: 0.2632
2026-01-04 22:40:54,682: t15.2024.03.15 val PER: 0.2370
2026-01-04 22:40:54,682: t15.2024.03.17 val PER: 0.1876
2026-01-04 22:40:54,682: t15.2024.05.10 val PER: 0.2080
2026-01-04 22:40:54,682: t15.2024.06.14 val PER: 0.2066
2026-01-04 22:40:54,682: t15.2024.07.19 val PER: 0.2999
2026-01-04 22:40:54,682: t15.2024.07.21 val PER: 0.1476
2026-01-04 22:40:54,682: t15.2024.07.28 val PER: 0.1765
2026-01-04 22:40:54,683: t15.2025.01.10 val PER: 0.3512
2026-01-04 22:40:54,683: t15.2025.01.12 val PER: 0.2063
2026-01-04 22:40:54,683: t15.2025.03.14 val PER: 0.3624
2026-01-04 22:40:54,683: t15.2025.03.16 val PER: 0.2579
2026-01-04 22:40:54,683: t15.2025.03.30 val PER: 0.3287
2026-01-04 22:40:54,683: t15.2025.04.13 val PER: 0.2568
2026-01-04 22:40:54,684: New best val WER(1gram) 73.10% --> 71.83%
2026-01-04 22:40:54,685: Checkpointing model
2026-01-04 22:40:55,331: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:40:55,602: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_7500
2026-01-04 22:41:04,453: Train batch 7600: loss: 17.05 grad norm: 59.89 time: 0.068
2026-01-04 22:41:22,249: Train batch 7800: loss: 14.64 grad norm: 60.59 time: 0.055
2026-01-04 22:41:40,342: Train batch 8000: loss: 11.62 grad norm: 53.55 time: 0.076
2026-01-04 22:41:40,343: Running test after training batch: 8000
2026-01-04 22:41:40,440: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:41:45,262: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e the cold this point will
2026-01-04 22:41:45,292: WER debug example
  GT : how does it keep the cost down
  PR : how duh it keep the
2026-01-04 22:41:46,875: Val batch 8000: PER (avg): 0.1900 CTC Loss (avg): 18.2817 WER(1gram): 74.37% (n=64) time: 6.532
2026-01-04 22:41:46,875: WER lens: avg_true_words=6.16 avg_pred_words=4.78 max_pred_words=10
2026-01-04 22:41:46,876: t15.2023.08.13 val PER: 0.1601
2026-01-04 22:41:46,876: t15.2023.08.18 val PER: 0.1433
2026-01-04 22:41:46,876: t15.2023.08.20 val PER: 0.1461
2026-01-04 22:41:46,876: t15.2023.08.25 val PER: 0.1295
2026-01-04 22:41:46,876: t15.2023.08.27 val PER: 0.2347
2026-01-04 22:41:46,876: t15.2023.09.01 val PER: 0.1120
2026-01-04 22:41:46,876: t15.2023.09.03 val PER: 0.1924
2026-01-04 22:41:46,876: t15.2023.09.24 val PER: 0.1456
2026-01-04 22:41:46,876: t15.2023.09.29 val PER: 0.1615
2026-01-04 22:41:46,876: t15.2023.10.01 val PER: 0.1955
2026-01-04 22:41:46,876: t15.2023.10.06 val PER: 0.1098
2026-01-04 22:41:46,876: t15.2023.10.08 val PER: 0.2788
2026-01-04 22:41:46,876: t15.2023.10.13 val PER: 0.2475
2026-01-04 22:41:46,877: t15.2023.10.15 val PER: 0.1767
2026-01-04 22:41:46,877: t15.2023.10.20 val PER: 0.2047
2026-01-04 22:41:46,877: t15.2023.10.22 val PER: 0.1604
2026-01-04 22:41:46,877: t15.2023.11.03 val PER: 0.2110
2026-01-04 22:41:46,877: t15.2023.11.04 val PER: 0.0410
2026-01-04 22:41:46,877: t15.2023.11.17 val PER: 0.0622
2026-01-04 22:41:46,877: t15.2023.11.19 val PER: 0.0679
2026-01-04 22:41:46,877: t15.2023.11.26 val PER: 0.1862
2026-01-04 22:41:46,877: t15.2023.12.03 val PER: 0.1597
2026-01-04 22:41:46,877: t15.2023.12.08 val PER: 0.1498
2026-01-04 22:41:46,877: t15.2023.12.10 val PER: 0.1393
2026-01-04 22:41:46,877: t15.2023.12.17 val PER: 0.1580
2026-01-04 22:41:46,877: t15.2023.12.29 val PER: 0.1956
2026-01-04 22:41:46,877: t15.2024.02.25 val PER: 0.1306
2026-01-04 22:41:46,877: t15.2024.03.08 val PER: 0.2518
2026-01-04 22:41:46,878: t15.2024.03.15 val PER: 0.2439
2026-01-04 22:41:46,878: t15.2024.03.17 val PER: 0.1841
2026-01-04 22:41:46,878: t15.2024.05.10 val PER: 0.2051
2026-01-04 22:41:46,878: t15.2024.06.14 val PER: 0.2003
2026-01-04 22:41:46,878: t15.2024.07.19 val PER: 0.2914
2026-01-04 22:41:46,878: t15.2024.07.21 val PER: 0.1324
2026-01-04 22:41:46,878: t15.2024.07.28 val PER: 0.1809
2026-01-04 22:41:46,878: t15.2025.01.10 val PER: 0.3485
2026-01-04 22:41:46,878: t15.2025.01.12 val PER: 0.1948
2026-01-04 22:41:46,878: t15.2025.03.14 val PER: 0.3476
2026-01-04 22:41:46,878: t15.2025.03.16 val PER: 0.2487
2026-01-04 22:41:46,878: t15.2025.03.30 val PER: 0.3368
2026-01-04 22:41:46,878: t15.2025.04.13 val PER: 0.2653
2026-01-04 22:41:47,138: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_8000
2026-01-04 22:42:05,249: Train batch 8200: loss: 9.32 grad norm: 49.39 time: 0.054
2026-01-04 22:42:23,700: Train batch 8400: loss: 10.09 grad norm: 49.64 time: 0.064
2026-01-04 22:42:33,303: Running test after training batch: 8500
2026-01-04 22:42:33,416: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:42:39,023: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e the cold at this point will
2026-01-04 22:42:39,053: WER debug example
  GT : how does it keep the cost down
  PR : how it the quast
2026-01-04 22:42:40,621: Val batch 8500: PER (avg): 0.1855 CTC Loss (avg): 18.0391 WER(1gram): 69.29% (n=64) time: 7.317
2026-01-04 22:42:40,621: WER lens: avg_true_words=6.16 avg_pred_words=4.86 max_pred_words=9
2026-01-04 22:42:40,622: t15.2023.08.13 val PER: 0.1611
2026-01-04 22:42:40,622: t15.2023.08.18 val PER: 0.1475
2026-01-04 22:42:40,622: t15.2023.08.20 val PER: 0.1366
2026-01-04 22:42:40,622: t15.2023.08.25 val PER: 0.1431
2026-01-04 22:42:40,622: t15.2023.08.27 val PER: 0.2186
2026-01-04 22:42:40,622: t15.2023.09.01 val PER: 0.0982
2026-01-04 22:42:40,622: t15.2023.09.03 val PER: 0.1912
2026-01-04 22:42:40,622: t15.2023.09.24 val PER: 0.1444
2026-01-04 22:42:40,622: t15.2023.09.29 val PER: 0.1563
2026-01-04 22:42:40,623: t15.2023.10.01 val PER: 0.1935
2026-01-04 22:42:40,623: t15.2023.10.06 val PER: 0.1098
2026-01-04 22:42:40,623: t15.2023.10.08 val PER: 0.2788
2026-01-04 22:42:40,623: t15.2023.10.13 val PER: 0.2358
2026-01-04 22:42:40,623: t15.2023.10.15 val PER: 0.1819
2026-01-04 22:42:40,623: t15.2023.10.20 val PER: 0.2114
2026-01-04 22:42:40,623: t15.2023.10.22 val PER: 0.1481
2026-01-04 22:42:40,623: t15.2023.11.03 val PER: 0.2083
2026-01-04 22:42:40,623: t15.2023.11.04 val PER: 0.0478
2026-01-04 22:42:40,623: t15.2023.11.17 val PER: 0.0575
2026-01-04 22:42:40,624: t15.2023.11.19 val PER: 0.0659
2026-01-04 22:42:40,624: t15.2023.11.26 val PER: 0.1812
2026-01-04 22:42:40,624: t15.2023.12.03 val PER: 0.1544
2026-01-04 22:42:40,624: t15.2023.12.08 val PER: 0.1525
2026-01-04 22:42:40,624: t15.2023.12.10 val PER: 0.1353
2026-01-04 22:42:40,624: t15.2023.12.17 val PER: 0.1684
2026-01-04 22:42:40,624: t15.2023.12.29 val PER: 0.1839
2026-01-04 22:42:40,624: t15.2024.02.25 val PER: 0.1433
2026-01-04 22:42:40,624: t15.2024.03.08 val PER: 0.2560
2026-01-04 22:42:40,624: t15.2024.03.15 val PER: 0.2333
2026-01-04 22:42:40,624: t15.2024.03.17 val PER: 0.1702
2026-01-04 22:42:40,625: t15.2024.05.10 val PER: 0.1917
2026-01-04 22:42:40,625: t15.2024.06.14 val PER: 0.1940
2026-01-04 22:42:40,625: t15.2024.07.19 val PER: 0.2848
2026-01-04 22:42:40,625: t15.2024.07.21 val PER: 0.1303
2026-01-04 22:42:40,625: t15.2024.07.28 val PER: 0.1632
2026-01-04 22:42:40,625: t15.2025.01.10 val PER: 0.3457
2026-01-04 22:42:40,625: t15.2025.01.12 val PER: 0.1901
2026-01-04 22:42:40,625: t15.2025.03.14 val PER: 0.3669
2026-01-04 22:42:40,625: t15.2025.03.16 val PER: 0.2291
2026-01-04 22:42:40,625: t15.2025.03.30 val PER: 0.3264
2026-01-04 22:42:40,625: t15.2025.04.13 val PER: 0.2482
2026-01-04 22:42:40,626: New best val WER(1gram) 71.83% --> 69.29%
2026-01-04 22:42:40,626: Checkpointing model
2026-01-04 22:42:41,238: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:42:41,541: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_8500
2026-01-04 22:42:50,869: Train batch 8600: loss: 15.94 grad norm: 58.04 time: 0.054
2026-01-04 22:43:08,581: Train batch 8800: loss: 15.88 grad norm: 61.04 time: 0.060
2026-01-04 22:43:26,662: Train batch 9000: loss: 16.22 grad norm: 62.97 time: 0.072
2026-01-04 22:43:26,662: Running test after training batch: 9000
2026-01-04 22:43:26,781: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:43:31,497: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e the cold at this point will
2026-01-04 22:43:31,527: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the quast
2026-01-04 22:43:33,103: Val batch 9000: PER (avg): 0.1795 CTC Loss (avg): 17.5951 WER(1gram): 71.32% (n=64) time: 6.441
2026-01-04 22:43:33,103: WER lens: avg_true_words=6.16 avg_pred_words=5.17 max_pred_words=10
2026-01-04 22:43:33,104: t15.2023.08.13 val PER: 0.1518
2026-01-04 22:43:33,104: t15.2023.08.18 val PER: 0.1366
2026-01-04 22:43:33,104: t15.2023.08.20 val PER: 0.1319
2026-01-04 22:43:33,104: t15.2023.08.25 val PER: 0.1190
2026-01-04 22:43:33,104: t15.2023.08.27 val PER: 0.1994
2026-01-04 22:43:33,104: t15.2023.09.01 val PER: 0.0909
2026-01-04 22:43:33,104: t15.2023.09.03 val PER: 0.1900
2026-01-04 22:43:33,104: t15.2023.09.24 val PER: 0.1468
2026-01-04 22:43:33,104: t15.2023.09.29 val PER: 0.1481
2026-01-04 22:43:33,104: t15.2023.10.01 val PER: 0.1988
2026-01-04 22:43:33,104: t15.2023.10.06 val PER: 0.1055
2026-01-04 22:43:33,104: t15.2023.10.08 val PER: 0.2720
2026-01-04 22:43:33,104: t15.2023.10.13 val PER: 0.2227
2026-01-04 22:43:33,104: t15.2023.10.15 val PER: 0.1753
2026-01-04 22:43:33,105: t15.2023.10.20 val PER: 0.2148
2026-01-04 22:43:33,105: t15.2023.10.22 val PER: 0.1414
2026-01-04 22:43:33,105: t15.2023.11.03 val PER: 0.1995
2026-01-04 22:43:33,105: t15.2023.11.04 val PER: 0.0444
2026-01-04 22:43:33,105: t15.2023.11.17 val PER: 0.0607
2026-01-04 22:43:33,105: t15.2023.11.19 val PER: 0.0539
2026-01-04 22:43:33,105: t15.2023.11.26 val PER: 0.1688
2026-01-04 22:43:33,105: t15.2023.12.03 val PER: 0.1513
2026-01-04 22:43:33,105: t15.2023.12.08 val PER: 0.1338
2026-01-04 22:43:33,105: t15.2023.12.10 val PER: 0.1130
2026-01-04 22:43:33,105: t15.2023.12.17 val PER: 0.1642
2026-01-04 22:43:33,105: t15.2023.12.29 val PER: 0.1805
2026-01-04 22:43:33,105: t15.2024.02.25 val PER: 0.1447
2026-01-04 22:43:33,105: t15.2024.03.08 val PER: 0.2404
2026-01-04 22:43:33,105: t15.2024.03.15 val PER: 0.2402
2026-01-04 22:43:33,105: t15.2024.03.17 val PER: 0.1722
2026-01-04 22:43:33,106: t15.2024.05.10 val PER: 0.1813
2026-01-04 22:43:33,106: t15.2024.06.14 val PER: 0.1972
2026-01-04 22:43:33,106: t15.2024.07.19 val PER: 0.2802
2026-01-04 22:43:33,106: t15.2024.07.21 val PER: 0.1186
2026-01-04 22:43:33,106: t15.2024.07.28 val PER: 0.1640
2026-01-04 22:43:33,106: t15.2025.01.10 val PER: 0.3251
2026-01-04 22:43:33,106: t15.2025.01.12 val PER: 0.1824
2026-01-04 22:43:33,106: t15.2025.03.14 val PER: 0.3609
2026-01-04 22:43:33,107: t15.2025.03.16 val PER: 0.2291
2026-01-04 22:43:33,107: t15.2025.03.30 val PER: 0.3230
2026-01-04 22:43:33,107: t15.2025.04.13 val PER: 0.2568
2026-01-04 22:43:33,388: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_9000
2026-01-04 22:43:51,217: Train batch 9200: loss: 10.92 grad norm: 50.40 time: 0.055
2026-01-04 22:44:08,977: Train batch 9400: loss: 7.78 grad norm: 44.99 time: 0.067
2026-01-04 22:44:17,933: Running test after training batch: 9500
2026-01-04 22:44:18,031: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:44:22,740: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 22:44:22,772: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the cost
2026-01-04 22:44:24,395: Val batch 9500: PER (avg): 0.1754 CTC Loss (avg): 17.4421 WER(1gram): 60.91% (n=64) time: 6.461
2026-01-04 22:44:24,395: WER lens: avg_true_words=6.16 avg_pred_words=5.80 max_pred_words=10
2026-01-04 22:44:24,395: t15.2023.08.13 val PER: 0.1570
2026-01-04 22:44:24,395: t15.2023.08.18 val PER: 0.1291
2026-01-04 22:44:24,395: t15.2023.08.20 val PER: 0.1215
2026-01-04 22:44:24,395: t15.2023.08.25 val PER: 0.1130
2026-01-04 22:44:24,395: t15.2023.08.27 val PER: 0.2074
2026-01-04 22:44:24,396: t15.2023.09.01 val PER: 0.0966
2026-01-04 22:44:24,396: t15.2023.09.03 val PER: 0.1865
2026-01-04 22:44:24,396: t15.2023.09.24 val PER: 0.1371
2026-01-04 22:44:24,396: t15.2023.09.29 val PER: 0.1468
2026-01-04 22:44:24,396: t15.2023.10.01 val PER: 0.1882
2026-01-04 22:44:24,396: t15.2023.10.06 val PER: 0.1066
2026-01-04 22:44:24,396: t15.2023.10.08 val PER: 0.2571
2026-01-04 22:44:24,396: t15.2023.10.13 val PER: 0.2273
2026-01-04 22:44:24,396: t15.2023.10.15 val PER: 0.1727
2026-01-04 22:44:24,396: t15.2023.10.20 val PER: 0.2181
2026-01-04 22:44:24,396: t15.2023.10.22 val PER: 0.1281
2026-01-04 22:44:24,396: t15.2023.11.03 val PER: 0.2008
2026-01-04 22:44:24,396: t15.2023.11.04 val PER: 0.0375
2026-01-04 22:44:24,397: t15.2023.11.17 val PER: 0.0544
2026-01-04 22:44:24,397: t15.2023.11.19 val PER: 0.0579
2026-01-04 22:44:24,397: t15.2023.11.26 val PER: 0.1645
2026-01-04 22:44:24,397: t15.2023.12.03 val PER: 0.1387
2026-01-04 22:44:24,397: t15.2023.12.08 val PER: 0.1305
2026-01-04 22:44:24,397: t15.2023.12.10 val PER: 0.1196
2026-01-04 22:44:24,397: t15.2023.12.17 val PER: 0.1559
2026-01-04 22:44:24,397: t15.2023.12.29 val PER: 0.1743
2026-01-04 22:44:24,397: t15.2024.02.25 val PER: 0.1433
2026-01-04 22:44:24,397: t15.2024.03.08 val PER: 0.2461
2026-01-04 22:44:24,397: t15.2024.03.15 val PER: 0.2308
2026-01-04 22:44:24,398: t15.2024.03.17 val PER: 0.1625
2026-01-04 22:44:24,398: t15.2024.05.10 val PER: 0.1947
2026-01-04 22:44:24,398: t15.2024.06.14 val PER: 0.1814
2026-01-04 22:44:24,398: t15.2024.07.19 val PER: 0.2657
2026-01-04 22:44:24,398: t15.2024.07.21 val PER: 0.1200
2026-01-04 22:44:24,398: t15.2024.07.28 val PER: 0.1603
2026-01-04 22:44:24,398: t15.2025.01.10 val PER: 0.3264
2026-01-04 22:44:24,398: t15.2025.01.12 val PER: 0.1763
2026-01-04 22:44:24,398: t15.2025.03.14 val PER: 0.3595
2026-01-04 22:44:24,398: t15.2025.03.16 val PER: 0.2225
2026-01-04 22:44:24,398: t15.2025.03.30 val PER: 0.3287
2026-01-04 22:44:24,399: t15.2025.04.13 val PER: 0.2354
2026-01-04 22:44:24,399: New best val WER(1gram) 69.29% --> 60.91%
2026-01-04 22:44:24,399: Checkpointing model
2026-01-04 22:44:25,035: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:44:25,336: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_9500
2026-01-04 22:44:34,204: Train batch 9600: loss: 8.93 grad norm: 47.40 time: 0.073
2026-01-04 22:44:51,987: Train batch 9800: loss: 13.34 grad norm: 65.07 time: 0.063
2026-01-04 22:45:09,845: Train batch 10000: loss: 6.02 grad norm: 42.43 time: 0.061
2026-01-04 22:45:09,845: Running test after training batch: 10000
2026-01-04 22:45:09,942: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:45:14,600: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e the cold aunt this point will
2026-01-04 22:45:14,632: WER debug example
  GT : how does it keep the cost down
  PR : how duh it keep the karst
2026-01-04 22:45:16,287: Val batch 10000: PER (avg): 0.1697 CTC Loss (avg): 16.8508 WER(1gram): 66.75% (n=64) time: 6.442
2026-01-04 22:45:16,288: WER lens: avg_true_words=6.16 avg_pred_words=5.58 max_pred_words=10
2026-01-04 22:45:16,288: t15.2023.08.13 val PER: 0.1351
2026-01-04 22:45:16,288: t15.2023.08.18 val PER: 0.1241
2026-01-04 22:45:16,288: t15.2023.08.20 val PER: 0.1207
2026-01-04 22:45:16,288: t15.2023.08.25 val PER: 0.1114
2026-01-04 22:45:16,288: t15.2023.08.27 val PER: 0.2010
2026-01-04 22:45:16,288: t15.2023.09.01 val PER: 0.0828
2026-01-04 22:45:16,289: t15.2023.09.03 val PER: 0.1805
2026-01-04 22:45:16,289: t15.2023.09.24 val PER: 0.1481
2026-01-04 22:45:16,289: t15.2023.09.29 val PER: 0.1468
2026-01-04 22:45:16,289: t15.2023.10.01 val PER: 0.1797
2026-01-04 22:45:16,289: t15.2023.10.06 val PER: 0.0990
2026-01-04 22:45:16,289: t15.2023.10.08 val PER: 0.2625
2026-01-04 22:45:16,289: t15.2023.10.13 val PER: 0.2273
2026-01-04 22:45:16,289: t15.2023.10.15 val PER: 0.1707
2026-01-04 22:45:16,293: t15.2023.10.20 val PER: 0.2148
2026-01-04 22:45:16,293: t15.2023.10.22 val PER: 0.1347
2026-01-04 22:45:16,293: t15.2023.11.03 val PER: 0.1879
2026-01-04 22:45:16,293: t15.2023.11.04 val PER: 0.0478
2026-01-04 22:45:16,293: t15.2023.11.17 val PER: 0.0591
2026-01-04 22:45:16,293: t15.2023.11.19 val PER: 0.0639
2026-01-04 22:45:16,294: t15.2023.11.26 val PER: 0.1442
2026-01-04 22:45:16,294: t15.2023.12.03 val PER: 0.1376
2026-01-04 22:45:16,294: t15.2023.12.08 val PER: 0.1252
2026-01-04 22:45:16,294: t15.2023.12.10 val PER: 0.1235
2026-01-04 22:45:16,294: t15.2023.12.17 val PER: 0.1341
2026-01-04 22:45:16,294: t15.2023.12.29 val PER: 0.1633
2026-01-04 22:45:16,294: t15.2024.02.25 val PER: 0.1334
2026-01-04 22:45:16,294: t15.2024.03.08 val PER: 0.2333
2026-01-04 22:45:16,294: t15.2024.03.15 val PER: 0.2301
2026-01-04 22:45:16,294: t15.2024.03.17 val PER: 0.1597
2026-01-04 22:45:16,294: t15.2024.05.10 val PER: 0.1857
2026-01-04 22:45:16,294: t15.2024.06.14 val PER: 0.1782
2026-01-04 22:45:16,294: t15.2024.07.19 val PER: 0.2716
2026-01-04 22:45:16,294: t15.2024.07.21 val PER: 0.1159
2026-01-04 22:45:16,294: t15.2024.07.28 val PER: 0.1537
2026-01-04 22:45:16,294: t15.2025.01.10 val PER: 0.3237
2026-01-04 22:45:16,295: t15.2025.01.12 val PER: 0.1747
2026-01-04 22:45:16,295: t15.2025.03.14 val PER: 0.3388
2026-01-04 22:45:16,295: t15.2025.03.16 val PER: 0.2094
2026-01-04 22:45:16,295: t15.2025.03.30 val PER: 0.2977
2026-01-04 22:45:16,295: t15.2025.04.13 val PER: 0.2311
2026-01-04 22:45:16,582: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_10000
2026-01-04 22:45:34,864: Train batch 10200: loss: 6.60 grad norm: 44.43 time: 0.050
2026-01-04 22:45:53,526: Train batch 10400: loss: 9.41 grad norm: 52.67 time: 0.071
2026-01-04 22:46:02,863: Running test after training batch: 10500
2026-01-04 22:46:03,039: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:46:07,791: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e the cold at its point will
2026-01-04 22:46:07,822: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cussed
2026-01-04 22:46:09,465: Val batch 10500: PER (avg): 0.1695 CTC Loss (avg): 16.7341 WER(1gram): 68.53% (n=64) time: 6.602
2026-01-04 22:46:09,466: WER lens: avg_true_words=6.16 avg_pred_words=5.64 max_pred_words=10
2026-01-04 22:46:09,466: t15.2023.08.13 val PER: 0.1414
2026-01-04 22:46:09,466: t15.2023.08.18 val PER: 0.1215
2026-01-04 22:46:09,466: t15.2023.08.20 val PER: 0.1160
2026-01-04 22:46:09,466: t15.2023.08.25 val PER: 0.1130
2026-01-04 22:46:09,466: t15.2023.08.27 val PER: 0.1929
2026-01-04 22:46:09,466: t15.2023.09.01 val PER: 0.0950
2026-01-04 22:46:09,466: t15.2023.09.03 val PER: 0.1805
2026-01-04 22:46:09,466: t15.2023.09.24 val PER: 0.1311
2026-01-04 22:46:09,466: t15.2023.09.29 val PER: 0.1391
2026-01-04 22:46:09,467: t15.2023.10.01 val PER: 0.1882
2026-01-04 22:46:09,467: t15.2023.10.06 val PER: 0.0980
2026-01-04 22:46:09,467: t15.2023.10.08 val PER: 0.2625
2026-01-04 22:46:09,467: t15.2023.10.13 val PER: 0.2118
2026-01-04 22:46:09,467: t15.2023.10.15 val PER: 0.1668
2026-01-04 22:46:09,467: t15.2023.10.20 val PER: 0.1946
2026-01-04 22:46:09,467: t15.2023.10.22 val PER: 0.1336
2026-01-04 22:46:09,467: t15.2023.11.03 val PER: 0.1981
2026-01-04 22:46:09,467: t15.2023.11.04 val PER: 0.0444
2026-01-04 22:46:09,467: t15.2023.11.17 val PER: 0.0529
2026-01-04 22:46:09,467: t15.2023.11.19 val PER: 0.0499
2026-01-04 22:46:09,467: t15.2023.11.26 val PER: 0.1529
2026-01-04 22:46:09,467: t15.2023.12.03 val PER: 0.1261
2026-01-04 22:46:09,467: t15.2023.12.08 val PER: 0.1138
2026-01-04 22:46:09,467: t15.2023.12.10 val PER: 0.1196
2026-01-04 22:46:09,468: t15.2023.12.17 val PER: 0.1570
2026-01-04 22:46:09,468: t15.2023.12.29 val PER: 0.1702
2026-01-04 22:46:09,468: t15.2024.02.25 val PER: 0.1334
2026-01-04 22:46:09,468: t15.2024.03.08 val PER: 0.2390
2026-01-04 22:46:09,468: t15.2024.03.15 val PER: 0.2251
2026-01-04 22:46:09,468: t15.2024.03.17 val PER: 0.1625
2026-01-04 22:46:09,468: t15.2024.05.10 val PER: 0.1783
2026-01-04 22:46:09,468: t15.2024.06.14 val PER: 0.1751
2026-01-04 22:46:09,468: t15.2024.07.19 val PER: 0.2670
2026-01-04 22:46:09,468: t15.2024.07.21 val PER: 0.1124
2026-01-04 22:46:09,468: t15.2024.07.28 val PER: 0.1618
2026-01-04 22:46:09,468: t15.2025.01.10 val PER: 0.3182
2026-01-04 22:46:09,468: t15.2025.01.12 val PER: 0.1763
2026-01-04 22:46:09,468: t15.2025.03.14 val PER: 0.3595
2026-01-04 22:46:09,468: t15.2025.03.16 val PER: 0.2068
2026-01-04 22:46:09,468: t15.2025.03.30 val PER: 0.3023
2026-01-04 22:46:09,469: t15.2025.04.13 val PER: 0.2454
2026-01-04 22:46:09,752: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_10500
2026-01-04 22:46:19,095: Train batch 10600: loss: 9.05 grad norm: 55.47 time: 0.072
2026-01-04 22:46:36,894: Train batch 10800: loss: 15.14 grad norm: 65.77 time: 0.064
2026-01-04 22:46:54,665: Train batch 11000: loss: 14.33 grad norm: 57.42 time: 0.056
2026-01-04 22:46:54,665: Running test after training batch: 11000
2026-01-04 22:46:54,771: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:46:59,502: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e the cold at this point will
2026-01-04 22:46:59,534: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the cost
2026-01-04 22:47:01,249: Val batch 11000: PER (avg): 0.1682 CTC Loss (avg): 16.6047 WER(1gram): 61.68% (n=64) time: 6.584
2026-01-04 22:47:01,250: WER lens: avg_true_words=6.16 avg_pred_words=5.83 max_pred_words=10
2026-01-04 22:47:01,250: t15.2023.08.13 val PER: 0.1341
2026-01-04 22:47:01,250: t15.2023.08.18 val PER: 0.1190
2026-01-04 22:47:01,250: t15.2023.08.20 val PER: 0.1168
2026-01-04 22:47:01,250: t15.2023.08.25 val PER: 0.1114
2026-01-04 22:47:01,250: t15.2023.08.27 val PER: 0.2010
2026-01-04 22:47:01,250: t15.2023.09.01 val PER: 0.0917
2026-01-04 22:47:01,250: t15.2023.09.03 val PER: 0.1758
2026-01-04 22:47:01,250: t15.2023.09.24 val PER: 0.1420
2026-01-04 22:47:01,250: t15.2023.09.29 val PER: 0.1398
2026-01-04 22:47:01,251: t15.2023.10.01 val PER: 0.1836
2026-01-04 22:47:01,251: t15.2023.10.06 val PER: 0.1001
2026-01-04 22:47:01,251: t15.2023.10.08 val PER: 0.2544
2026-01-04 22:47:01,251: t15.2023.10.13 val PER: 0.2188
2026-01-04 22:47:01,251: t15.2023.10.15 val PER: 0.1648
2026-01-04 22:47:01,251: t15.2023.10.20 val PER: 0.2148
2026-01-04 22:47:01,251: t15.2023.10.22 val PER: 0.1269
2026-01-04 22:47:01,251: t15.2023.11.03 val PER: 0.1961
2026-01-04 22:47:01,251: t15.2023.11.04 val PER: 0.0444
2026-01-04 22:47:01,252: t15.2023.11.17 val PER: 0.0420
2026-01-04 22:47:01,252: t15.2023.11.19 val PER: 0.0499
2026-01-04 22:47:01,252: t15.2023.11.26 val PER: 0.1507
2026-01-04 22:47:01,252: t15.2023.12.03 val PER: 0.1303
2026-01-04 22:47:01,252: t15.2023.12.08 val PER: 0.1152
2026-01-04 22:47:01,252: t15.2023.12.10 val PER: 0.1130
2026-01-04 22:47:01,252: t15.2023.12.17 val PER: 0.1622
2026-01-04 22:47:01,252: t15.2023.12.29 val PER: 0.1544
2026-01-04 22:47:01,252: t15.2024.02.25 val PER: 0.1320
2026-01-04 22:47:01,252: t15.2024.03.08 val PER: 0.2418
2026-01-04 22:47:01,252: t15.2024.03.15 val PER: 0.2308
2026-01-04 22:47:01,252: t15.2024.03.17 val PER: 0.1604
2026-01-04 22:47:01,252: t15.2024.05.10 val PER: 0.1887
2026-01-04 22:47:01,252: t15.2024.06.14 val PER: 0.1656
2026-01-04 22:47:01,252: t15.2024.07.19 val PER: 0.2630
2026-01-04 22:47:01,252: t15.2024.07.21 val PER: 0.1062
2026-01-04 22:47:01,253: t15.2024.07.28 val PER: 0.1559
2026-01-04 22:47:01,253: t15.2025.01.10 val PER: 0.3154
2026-01-04 22:47:01,253: t15.2025.01.12 val PER: 0.1647
2026-01-04 22:47:01,253: t15.2025.03.14 val PER: 0.3506
2026-01-04 22:47:01,253: t15.2025.03.16 val PER: 0.2238
2026-01-04 22:47:01,253: t15.2025.03.30 val PER: 0.3172
2026-01-04 22:47:01,253: t15.2025.04.13 val PER: 0.2397
2026-01-04 22:47:01,540: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_11000
2026-01-04 22:47:20,019: Train batch 11200: loss: 11.90 grad norm: 59.59 time: 0.072
2026-01-04 22:47:38,350: Train batch 11400: loss: 10.32 grad norm: 54.81 time: 0.057
2026-01-04 22:47:47,646: Running test after training batch: 11500
2026-01-04 22:47:47,749: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:47:52,466: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 22:47:52,497: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the karst
2026-01-04 22:47:54,191: Val batch 11500: PER (avg): 0.1627 CTC Loss (avg): 16.3430 WER(1gram): 63.71% (n=64) time: 6.544
2026-01-04 22:47:54,191: WER lens: avg_true_words=6.16 avg_pred_words=5.95 max_pred_words=10
2026-01-04 22:47:54,191: t15.2023.08.13 val PER: 0.1279
2026-01-04 22:47:54,191: t15.2023.08.18 val PER: 0.1257
2026-01-04 22:47:54,192: t15.2023.08.20 val PER: 0.1160
2026-01-04 22:47:54,192: t15.2023.08.25 val PER: 0.1099
2026-01-04 22:47:54,192: t15.2023.08.27 val PER: 0.1945
2026-01-04 22:47:54,192: t15.2023.09.01 val PER: 0.0869
2026-01-04 22:47:54,192: t15.2023.09.03 val PER: 0.1698
2026-01-04 22:47:54,192: t15.2023.09.24 val PER: 0.1274
2026-01-04 22:47:54,192: t15.2023.09.29 val PER: 0.1334
2026-01-04 22:47:54,192: t15.2023.10.01 val PER: 0.1744
2026-01-04 22:47:54,192: t15.2023.10.06 val PER: 0.0872
2026-01-04 22:47:54,192: t15.2023.10.08 val PER: 0.2503
2026-01-04 22:47:54,192: t15.2023.10.13 val PER: 0.2079
2026-01-04 22:47:54,192: t15.2023.10.15 val PER: 0.1602
2026-01-04 22:47:54,192: t15.2023.10.20 val PER: 0.1879
2026-01-04 22:47:54,192: t15.2023.10.22 val PER: 0.1370
2026-01-04 22:47:54,193: t15.2023.11.03 val PER: 0.1845
2026-01-04 22:47:54,193: t15.2023.11.04 val PER: 0.0375
2026-01-04 22:47:54,193: t15.2023.11.17 val PER: 0.0420
2026-01-04 22:47:54,193: t15.2023.11.19 val PER: 0.0339
2026-01-04 22:47:54,193: t15.2023.11.26 val PER: 0.1362
2026-01-04 22:47:54,193: t15.2023.12.03 val PER: 0.1218
2026-01-04 22:47:54,193: t15.2023.12.08 val PER: 0.1132
2026-01-04 22:47:54,193: t15.2023.12.10 val PER: 0.1143
2026-01-04 22:47:54,193: t15.2023.12.17 val PER: 0.1549
2026-01-04 22:47:54,193: t15.2023.12.29 val PER: 0.1551
2026-01-04 22:47:54,193: t15.2024.02.25 val PER: 0.1362
2026-01-04 22:47:54,193: t15.2024.03.08 val PER: 0.2262
2026-01-04 22:47:54,193: t15.2024.03.15 val PER: 0.2201
2026-01-04 22:47:54,193: t15.2024.03.17 val PER: 0.1485
2026-01-04 22:47:54,193: t15.2024.05.10 val PER: 0.1842
2026-01-04 22:47:54,193: t15.2024.06.14 val PER: 0.1688
2026-01-04 22:47:54,193: t15.2024.07.19 val PER: 0.2584
2026-01-04 22:47:54,193: t15.2024.07.21 val PER: 0.1069
2026-01-04 22:47:54,194: t15.2024.07.28 val PER: 0.1581
2026-01-04 22:47:54,194: t15.2025.01.10 val PER: 0.3154
2026-01-04 22:47:54,194: t15.2025.01.12 val PER: 0.1732
2026-01-04 22:47:54,194: t15.2025.03.14 val PER: 0.3269
2026-01-04 22:47:54,194: t15.2025.03.16 val PER: 0.2199
2026-01-04 22:47:54,194: t15.2025.03.30 val PER: 0.2977
2026-01-04 22:47:54,194: t15.2025.04.13 val PER: 0.2282
2026-01-04 22:47:54,479: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_11500
2026-01-04 22:48:03,464: Train batch 11600: loss: 11.51 grad norm: 51.26 time: 0.061
2026-01-04 22:48:21,501: Train batch 11800: loss: 6.84 grad norm: 42.19 time: 0.044
2026-01-04 22:48:39,366: Train batch 12000: loss: 13.96 grad norm: 59.18 time: 0.071
2026-01-04 22:48:39,366: Running test after training batch: 12000
2026-01-04 22:48:39,461: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:48:44,205: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 22:48:44,237: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the karst
2026-01-04 22:48:45,987: Val batch 12000: PER (avg): 0.1618 CTC Loss (avg): 16.2553 WER(1gram): 64.97% (n=64) time: 6.621
2026-01-04 22:48:45,988: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=10
2026-01-04 22:48:45,988: t15.2023.08.13 val PER: 0.1320
2026-01-04 22:48:45,988: t15.2023.08.18 val PER: 0.1123
2026-01-04 22:48:45,988: t15.2023.08.20 val PER: 0.1144
2026-01-04 22:48:45,988: t15.2023.08.25 val PER: 0.1054
2026-01-04 22:48:45,988: t15.2023.08.27 val PER: 0.1817
2026-01-04 22:48:45,988: t15.2023.09.01 val PER: 0.0925
2026-01-04 22:48:45,988: t15.2023.09.03 val PER: 0.1651
2026-01-04 22:48:45,988: t15.2023.09.24 val PER: 0.1359
2026-01-04 22:48:45,989: t15.2023.09.29 val PER: 0.1283
2026-01-04 22:48:45,989: t15.2023.10.01 val PER: 0.1770
2026-01-04 22:48:45,989: t15.2023.10.06 val PER: 0.0980
2026-01-04 22:48:45,989: t15.2023.10.08 val PER: 0.2449
2026-01-04 22:48:45,989: t15.2023.10.13 val PER: 0.2025
2026-01-04 22:48:45,989: t15.2023.10.15 val PER: 0.1635
2026-01-04 22:48:45,989: t15.2023.10.20 val PER: 0.2047
2026-01-04 22:48:45,989: t15.2023.10.22 val PER: 0.1247
2026-01-04 22:48:45,989: t15.2023.11.03 val PER: 0.1852
2026-01-04 22:48:45,989: t15.2023.11.04 val PER: 0.0375
2026-01-04 22:48:45,989: t15.2023.11.17 val PER: 0.0373
2026-01-04 22:48:45,989: t15.2023.11.19 val PER: 0.0459
2026-01-04 22:48:45,989: t15.2023.11.26 val PER: 0.1377
2026-01-04 22:48:45,989: t15.2023.12.03 val PER: 0.1166
2026-01-04 22:48:45,989: t15.2023.12.08 val PER: 0.1099
2026-01-04 22:48:45,990: t15.2023.12.10 val PER: 0.1064
2026-01-04 22:48:45,990: t15.2023.12.17 val PER: 0.1362
2026-01-04 22:48:45,990: t15.2023.12.29 val PER: 0.1510
2026-01-04 22:48:45,990: t15.2024.02.25 val PER: 0.1250
2026-01-04 22:48:45,990: t15.2024.03.08 val PER: 0.2319
2026-01-04 22:48:45,990: t15.2024.03.15 val PER: 0.2176
2026-01-04 22:48:45,990: t15.2024.03.17 val PER: 0.1534
2026-01-04 22:48:45,990: t15.2024.05.10 val PER: 0.1872
2026-01-04 22:48:45,990: t15.2024.06.14 val PER: 0.1814
2026-01-04 22:48:45,990: t15.2024.07.19 val PER: 0.2657
2026-01-04 22:48:45,991: t15.2024.07.21 val PER: 0.1103
2026-01-04 22:48:45,991: t15.2024.07.28 val PER: 0.1596
2026-01-04 22:48:45,991: t15.2025.01.10 val PER: 0.2989
2026-01-04 22:48:45,991: t15.2025.01.12 val PER: 0.1632
2026-01-04 22:48:45,991: t15.2025.03.14 val PER: 0.3476
2026-01-04 22:48:45,991: t15.2025.03.16 val PER: 0.2055
2026-01-04 22:48:45,991: t15.2025.03.30 val PER: 0.3195
2026-01-04 22:48:45,991: t15.2025.04.13 val PER: 0.2168
2026-01-04 22:48:46,273: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_12000
2026-01-04 22:49:04,032: Train batch 12200: loss: 5.93 grad norm: 39.09 time: 0.066
2026-01-04 22:49:21,696: Train batch 12400: loss: 4.90 grad norm: 38.03 time: 0.042
2026-01-04 22:49:30,959: Running test after training batch: 12500
2026-01-04 22:49:31,120: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:49:35,984: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e the cold aunt this point will
2026-01-04 22:49:36,017: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the cost
2026-01-04 22:49:37,757: Val batch 12500: PER (avg): 0.1588 CTC Loss (avg): 15.9067 WER(1gram): 64.47% (n=64) time: 6.798
2026-01-04 22:49:37,757: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=10
2026-01-04 22:49:37,758: t15.2023.08.13 val PER: 0.1247
2026-01-04 22:49:37,758: t15.2023.08.18 val PER: 0.1174
2026-01-04 22:49:37,758: t15.2023.08.20 val PER: 0.1096
2026-01-04 22:49:37,758: t15.2023.08.25 val PER: 0.1009
2026-01-04 22:49:37,758: t15.2023.08.27 val PER: 0.1929
2026-01-04 22:49:37,758: t15.2023.09.01 val PER: 0.0852
2026-01-04 22:49:37,758: t15.2023.09.03 val PER: 0.1651
2026-01-04 22:49:37,758: t15.2023.09.24 val PER: 0.1335
2026-01-04 22:49:37,758: t15.2023.09.29 val PER: 0.1276
2026-01-04 22:49:37,758: t15.2023.10.01 val PER: 0.1737
2026-01-04 22:49:37,758: t15.2023.10.06 val PER: 0.0936
2026-01-04 22:49:37,758: t15.2023.10.08 val PER: 0.2652
2026-01-04 22:49:37,759: t15.2023.10.13 val PER: 0.2126
2026-01-04 22:49:37,759: t15.2023.10.15 val PER: 0.1549
2026-01-04 22:49:37,759: t15.2023.10.20 val PER: 0.1779
2026-01-04 22:49:37,759: t15.2023.10.22 val PER: 0.1147
2026-01-04 22:49:37,759: t15.2023.11.03 val PER: 0.1811
2026-01-04 22:49:37,759: t15.2023.11.04 val PER: 0.0307
2026-01-04 22:49:37,759: t15.2023.11.17 val PER: 0.0482
2026-01-04 22:49:37,759: t15.2023.11.19 val PER: 0.0339
2026-01-04 22:49:37,759: t15.2023.11.26 val PER: 0.1413
2026-01-04 22:49:37,759: t15.2023.12.03 val PER: 0.1218
2026-01-04 22:49:37,759: t15.2023.12.08 val PER: 0.1132
2026-01-04 22:49:37,759: t15.2023.12.10 val PER: 0.1117
2026-01-04 22:49:37,759: t15.2023.12.17 val PER: 0.1403
2026-01-04 22:49:37,759: t15.2023.12.29 val PER: 0.1393
2026-01-04 22:49:37,760: t15.2024.02.25 val PER: 0.1320
2026-01-04 22:49:37,760: t15.2024.03.08 val PER: 0.2333
2026-01-04 22:49:37,760: t15.2024.03.15 val PER: 0.2139
2026-01-04 22:49:37,760: t15.2024.03.17 val PER: 0.1464
2026-01-04 22:49:37,760: t15.2024.05.10 val PER: 0.1842
2026-01-04 22:49:37,760: t15.2024.06.14 val PER: 0.1672
2026-01-04 22:49:37,760: t15.2024.07.19 val PER: 0.2492
2026-01-04 22:49:37,760: t15.2024.07.21 val PER: 0.0986
2026-01-04 22:49:37,760: t15.2024.07.28 val PER: 0.1449
2026-01-04 22:49:37,760: t15.2025.01.10 val PER: 0.3099
2026-01-04 22:49:37,760: t15.2025.01.12 val PER: 0.1594
2026-01-04 22:49:37,761: t15.2025.03.14 val PER: 0.3328
2026-01-04 22:49:37,761: t15.2025.03.16 val PER: 0.2068
2026-01-04 22:49:37,761: t15.2025.03.30 val PER: 0.3080
2026-01-04 22:49:37,761: t15.2025.04.13 val PER: 0.2211
2026-01-04 22:49:38,041: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_12500
2026-01-04 22:49:47,219: Train batch 12600: loss: 7.48 grad norm: 40.93 time: 0.057
2026-01-04 22:50:05,612: Train batch 12800: loss: 6.27 grad norm: 46.33 time: 0.051
2026-01-04 22:50:24,106: Train batch 13000: loss: 6.61 grad norm: 42.22 time: 0.065
2026-01-04 22:50:24,106: Running test after training batch: 13000
2026-01-04 22:50:24,242: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:50:28,919: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 22:50:28,952: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the cost
2026-01-04 22:50:30,690: Val batch 13000: PER (avg): 0.1575 CTC Loss (avg): 15.6940 WER(1gram): 63.71% (n=64) time: 6.584
2026-01-04 22:50:30,690: WER lens: avg_true_words=6.16 avg_pred_words=5.84 max_pred_words=10
2026-01-04 22:50:30,691: t15.2023.08.13 val PER: 0.1279
2026-01-04 22:50:30,691: t15.2023.08.18 val PER: 0.1148
2026-01-04 22:50:30,691: t15.2023.08.20 val PER: 0.1096
2026-01-04 22:50:30,691: t15.2023.08.25 val PER: 0.1069
2026-01-04 22:50:30,691: t15.2023.08.27 val PER: 0.1881
2026-01-04 22:50:30,691: t15.2023.09.01 val PER: 0.0828
2026-01-04 22:50:30,691: t15.2023.09.03 val PER: 0.1746
2026-01-04 22:50:30,691: t15.2023.09.24 val PER: 0.1189
2026-01-04 22:50:30,691: t15.2023.09.29 val PER: 0.1340
2026-01-04 22:50:30,691: t15.2023.10.01 val PER: 0.1678
2026-01-04 22:50:30,692: t15.2023.10.06 val PER: 0.0893
2026-01-04 22:50:30,692: t15.2023.10.08 val PER: 0.2503
2026-01-04 22:50:30,692: t15.2023.10.13 val PER: 0.2009
2026-01-04 22:50:30,692: t15.2023.10.15 val PER: 0.1628
2026-01-04 22:50:30,692: t15.2023.10.20 val PER: 0.1846
2026-01-04 22:50:30,692: t15.2023.10.22 val PER: 0.1225
2026-01-04 22:50:30,692: t15.2023.11.03 val PER: 0.1791
2026-01-04 22:50:30,692: t15.2023.11.04 val PER: 0.0410
2026-01-04 22:50:30,692: t15.2023.11.17 val PER: 0.0358
2026-01-04 22:50:30,692: t15.2023.11.19 val PER: 0.0479
2026-01-04 22:50:30,692: t15.2023.11.26 val PER: 0.1377
2026-01-04 22:50:30,692: t15.2023.12.03 val PER: 0.1218
2026-01-04 22:50:30,692: t15.2023.12.08 val PER: 0.1105
2026-01-04 22:50:30,692: t15.2023.12.10 val PER: 0.1064
2026-01-04 22:50:30,693: t15.2023.12.17 val PER: 0.1424
2026-01-04 22:50:30,693: t15.2023.12.29 val PER: 0.1462
2026-01-04 22:50:30,693: t15.2024.02.25 val PER: 0.1208
2026-01-04 22:50:30,693: t15.2024.03.08 val PER: 0.2276
2026-01-04 22:50:30,693: t15.2024.03.15 val PER: 0.2108
2026-01-04 22:50:30,693: t15.2024.03.17 val PER: 0.1332
2026-01-04 22:50:30,693: t15.2024.05.10 val PER: 0.1724
2026-01-04 22:50:30,693: t15.2024.06.14 val PER: 0.1546
2026-01-04 22:50:30,693: t15.2024.07.19 val PER: 0.2518
2026-01-04 22:50:30,693: t15.2024.07.21 val PER: 0.0979
2026-01-04 22:50:30,693: t15.2024.07.28 val PER: 0.1493
2026-01-04 22:50:30,693: t15.2025.01.10 val PER: 0.2906
2026-01-04 22:50:30,693: t15.2025.01.12 val PER: 0.1640
2026-01-04 22:50:30,694: t15.2025.03.14 val PER: 0.3432
2026-01-04 22:50:30,694: t15.2025.03.16 val PER: 0.2016
2026-01-04 22:50:30,694: t15.2025.03.30 val PER: 0.3057
2026-01-04 22:50:30,694: t15.2025.04.13 val PER: 0.2382
2026-01-04 22:50:30,982: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_13000
2026-01-04 22:50:49,236: Train batch 13200: loss: 13.12 grad norm: 62.85 time: 0.054
2026-01-04 22:51:07,394: Train batch 13400: loss: 9.54 grad norm: 55.83 time: 0.062
2026-01-04 22:51:16,493: Running test after training batch: 13500
2026-01-04 22:51:16,624: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:51:21,443: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 22:51:21,476: WER debug example
  GT : how does it keep the cost down
  PR : how sta it keep the cost
2026-01-04 22:51:23,237: Val batch 13500: PER (avg): 0.1542 CTC Loss (avg): 15.5719 WER(1gram): 62.94% (n=64) time: 6.743
2026-01-04 22:51:23,237: WER lens: avg_true_words=6.16 avg_pred_words=5.91 max_pred_words=10
2026-01-04 22:51:23,237: t15.2023.08.13 val PER: 0.1279
2026-01-04 22:51:23,237: t15.2023.08.18 val PER: 0.1048
2026-01-04 22:51:23,237: t15.2023.08.20 val PER: 0.1112
2026-01-04 22:51:23,237: t15.2023.08.25 val PER: 0.1024
2026-01-04 22:51:23,237: t15.2023.08.27 val PER: 0.1768
2026-01-04 22:51:23,238: t15.2023.09.01 val PER: 0.0820
2026-01-04 22:51:23,238: t15.2023.09.03 val PER: 0.1698
2026-01-04 22:51:23,238: t15.2023.09.24 val PER: 0.1274
2026-01-04 22:51:23,238: t15.2023.09.29 val PER: 0.1295
2026-01-04 22:51:23,238: t15.2023.10.01 val PER: 0.1691
2026-01-04 22:51:23,238: t15.2023.10.06 val PER: 0.0893
2026-01-04 22:51:23,238: t15.2023.10.08 val PER: 0.2463
2026-01-04 22:51:23,238: t15.2023.10.13 val PER: 0.1994
2026-01-04 22:51:23,238: t15.2023.10.15 val PER: 0.1529
2026-01-04 22:51:23,238: t15.2023.10.20 val PER: 0.1779
2026-01-04 22:51:23,239: t15.2023.10.22 val PER: 0.1169
2026-01-04 22:51:23,239: t15.2023.11.03 val PER: 0.1852
2026-01-04 22:51:23,239: t15.2023.11.04 val PER: 0.0341
2026-01-04 22:51:23,239: t15.2023.11.17 val PER: 0.0358
2026-01-04 22:51:23,239: t15.2023.11.19 val PER: 0.0319
2026-01-04 22:51:23,239: t15.2023.11.26 val PER: 0.1268
2026-01-04 22:51:23,239: t15.2023.12.03 val PER: 0.1134
2026-01-04 22:51:23,239: t15.2023.12.08 val PER: 0.1085
2026-01-04 22:51:23,239: t15.2023.12.10 val PER: 0.0933
2026-01-04 22:51:23,239: t15.2023.12.17 val PER: 0.1331
2026-01-04 22:51:23,239: t15.2023.12.29 val PER: 0.1366
2026-01-04 22:51:23,239: t15.2024.02.25 val PER: 0.1250
2026-01-04 22:51:23,239: t15.2024.03.08 val PER: 0.2248
2026-01-04 22:51:23,239: t15.2024.03.15 val PER: 0.2126
2026-01-04 22:51:23,239: t15.2024.03.17 val PER: 0.1395
2026-01-04 22:51:23,239: t15.2024.05.10 val PER: 0.1768
2026-01-04 22:51:23,239: t15.2024.06.14 val PER: 0.1562
2026-01-04 22:51:23,239: t15.2024.07.19 val PER: 0.2373
2026-01-04 22:51:23,240: t15.2024.07.21 val PER: 0.1014
2026-01-04 22:51:23,240: t15.2024.07.28 val PER: 0.1449
2026-01-04 22:51:23,240: t15.2025.01.10 val PER: 0.3030
2026-01-04 22:51:23,240: t15.2025.01.12 val PER: 0.1594
2026-01-04 22:51:23,240: t15.2025.03.14 val PER: 0.3417
2026-01-04 22:51:23,240: t15.2025.03.16 val PER: 0.1976
2026-01-04 22:51:23,240: t15.2025.03.30 val PER: 0.2954
2026-01-04 22:51:23,240: t15.2025.04.13 val PER: 0.2197
2026-01-04 22:51:23,530: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_13500
2026-01-04 22:51:32,791: Train batch 13600: loss: 13.14 grad norm: 68.01 time: 0.062
2026-01-04 22:51:51,129: Train batch 13800: loss: 9.14 grad norm: 53.93 time: 0.056
2026-01-04 22:52:09,565: Train batch 14000: loss: 12.03 grad norm: 68.57 time: 0.050
2026-01-04 22:52:09,565: Running test after training batch: 14000
2026-01-04 22:52:09,671: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:52:14,406: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 22:52:14,440: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the karst
2026-01-04 22:52:16,232: Val batch 14000: PER (avg): 0.1529 CTC Loss (avg): 15.4432 WER(1gram): 62.94% (n=64) time: 6.667
2026-01-04 22:52:16,232: WER lens: avg_true_words=6.16 avg_pred_words=5.84 max_pred_words=10
2026-01-04 22:52:16,233: t15.2023.08.13 val PER: 0.1206
2026-01-04 22:52:16,233: t15.2023.08.18 val PER: 0.1065
2026-01-04 22:52:16,233: t15.2023.08.20 val PER: 0.1128
2026-01-04 22:52:16,233: t15.2023.08.25 val PER: 0.0994
2026-01-04 22:52:16,233: t15.2023.08.27 val PER: 0.1768
2026-01-04 22:52:16,233: t15.2023.09.01 val PER: 0.0755
2026-01-04 22:52:16,233: t15.2023.09.03 val PER: 0.1722
2026-01-04 22:52:16,233: t15.2023.09.24 val PER: 0.1311
2026-01-04 22:52:16,234: t15.2023.09.29 val PER: 0.1264
2026-01-04 22:52:16,234: t15.2023.10.01 val PER: 0.1684
2026-01-04 22:52:16,234: t15.2023.10.06 val PER: 0.0926
2026-01-04 22:52:16,234: t15.2023.10.08 val PER: 0.2463
2026-01-04 22:52:16,234: t15.2023.10.13 val PER: 0.2002
2026-01-04 22:52:16,234: t15.2023.10.15 val PER: 0.1595
2026-01-04 22:52:16,234: t15.2023.10.20 val PER: 0.1913
2026-01-04 22:52:16,234: t15.2023.10.22 val PER: 0.1225
2026-01-04 22:52:16,234: t15.2023.11.03 val PER: 0.1750
2026-01-04 22:52:16,234: t15.2023.11.04 val PER: 0.0273
2026-01-04 22:52:16,234: t15.2023.11.17 val PER: 0.0311
2026-01-04 22:52:16,234: t15.2023.11.19 val PER: 0.0339
2026-01-04 22:52:16,235: t15.2023.11.26 val PER: 0.1225
2026-01-04 22:52:16,235: t15.2023.12.03 val PER: 0.1155
2026-01-04 22:52:16,235: t15.2023.12.08 val PER: 0.1145
2026-01-04 22:52:16,235: t15.2023.12.10 val PER: 0.1091
2026-01-04 22:52:16,235: t15.2023.12.17 val PER: 0.1195
2026-01-04 22:52:16,235: t15.2023.12.29 val PER: 0.1325
2026-01-04 22:52:16,235: t15.2024.02.25 val PER: 0.1208
2026-01-04 22:52:16,235: t15.2024.03.08 val PER: 0.2176
2026-01-04 22:52:16,235: t15.2024.03.15 val PER: 0.2045
2026-01-04 22:52:16,235: t15.2024.03.17 val PER: 0.1311
2026-01-04 22:52:16,235: t15.2024.05.10 val PER: 0.1709
2026-01-04 22:52:16,235: t15.2024.06.14 val PER: 0.1640
2026-01-04 22:52:16,235: t15.2024.07.19 val PER: 0.2498
2026-01-04 22:52:16,235: t15.2024.07.21 val PER: 0.0966
2026-01-04 22:52:16,235: t15.2024.07.28 val PER: 0.1390
2026-01-04 22:52:16,236: t15.2025.01.10 val PER: 0.2810
2026-01-04 22:52:16,236: t15.2025.01.12 val PER: 0.1617
2026-01-04 22:52:16,236: t15.2025.03.14 val PER: 0.3343
2026-01-04 22:52:16,236: t15.2025.03.16 val PER: 0.1924
2026-01-04 22:52:16,236: t15.2025.03.30 val PER: 0.3092
2026-01-04 22:52:16,236: t15.2025.04.13 val PER: 0.2140
2026-01-04 22:52:16,530: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_14000
2026-01-04 22:52:34,665: Train batch 14200: loss: 8.27 grad norm: 54.21 time: 0.055
2026-01-04 22:52:53,143: Train batch 14400: loss: 6.24 grad norm: 42.19 time: 0.065
2026-01-04 22:53:02,501: Running test after training batch: 14500
2026-01-04 22:53:02,596: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:53:07,485: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 22:53:07,519: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the cost
2026-01-04 22:53:09,337: Val batch 14500: PER (avg): 0.1542 CTC Loss (avg): 15.5394 WER(1gram): 60.41% (n=64) time: 6.836
2026-01-04 22:53:09,337: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=10
2026-01-04 22:53:09,338: t15.2023.08.13 val PER: 0.1279
2026-01-04 22:53:09,338: t15.2023.08.18 val PER: 0.1174
2026-01-04 22:53:09,338: t15.2023.08.20 val PER: 0.1072
2026-01-04 22:53:09,338: t15.2023.08.25 val PER: 0.1009
2026-01-04 22:53:09,338: t15.2023.08.27 val PER: 0.1736
2026-01-04 22:53:09,338: t15.2023.09.01 val PER: 0.0714
2026-01-04 22:53:09,338: t15.2023.09.03 val PER: 0.1722
2026-01-04 22:53:09,339: t15.2023.09.24 val PER: 0.1323
2026-01-04 22:53:09,339: t15.2023.09.29 val PER: 0.1264
2026-01-04 22:53:09,339: t15.2023.10.01 val PER: 0.1684
2026-01-04 22:53:09,339: t15.2023.10.06 val PER: 0.0980
2026-01-04 22:53:09,339: t15.2023.10.08 val PER: 0.2368
2026-01-04 22:53:09,339: t15.2023.10.13 val PER: 0.2048
2026-01-04 22:53:09,339: t15.2023.10.15 val PER: 0.1694
2026-01-04 22:53:09,339: t15.2023.10.20 val PER: 0.1745
2026-01-04 22:53:09,339: t15.2023.10.22 val PER: 0.1247
2026-01-04 22:53:09,339: t15.2023.11.03 val PER: 0.1852
2026-01-04 22:53:09,339: t15.2023.11.04 val PER: 0.0307
2026-01-04 22:53:09,339: t15.2023.11.17 val PER: 0.0420
2026-01-04 22:53:09,339: t15.2023.11.19 val PER: 0.0339
2026-01-04 22:53:09,339: t15.2023.11.26 val PER: 0.1188
2026-01-04 22:53:09,340: t15.2023.12.03 val PER: 0.1019
2026-01-04 22:53:09,340: t15.2023.12.08 val PER: 0.1012
2026-01-04 22:53:09,340: t15.2023.12.10 val PER: 0.1038
2026-01-04 22:53:09,340: t15.2023.12.17 val PER: 0.1268
2026-01-04 22:53:09,340: t15.2023.12.29 val PER: 0.1338
2026-01-04 22:53:09,340: t15.2024.02.25 val PER: 0.1166
2026-01-04 22:53:09,340: t15.2024.03.08 val PER: 0.2290
2026-01-04 22:53:09,340: t15.2024.03.15 val PER: 0.2108
2026-01-04 22:53:09,340: t15.2024.03.17 val PER: 0.1346
2026-01-04 22:53:09,341: t15.2024.05.10 val PER: 0.1738
2026-01-04 22:53:09,341: t15.2024.06.14 val PER: 0.1640
2026-01-04 22:53:09,341: t15.2024.07.19 val PER: 0.2472
2026-01-04 22:53:09,341: t15.2024.07.21 val PER: 0.1028
2026-01-04 22:53:09,341: t15.2024.07.28 val PER: 0.1471
2026-01-04 22:53:09,341: t15.2025.01.10 val PER: 0.2906
2026-01-04 22:53:09,341: t15.2025.01.12 val PER: 0.1640
2026-01-04 22:53:09,341: t15.2025.03.14 val PER: 0.3269
2026-01-04 22:53:09,341: t15.2025.03.16 val PER: 0.1990
2026-01-04 22:53:09,341: t15.2025.03.30 val PER: 0.3011
2026-01-04 22:53:09,341: t15.2025.04.13 val PER: 0.2126
2026-01-04 22:53:09,342: New best val WER(1gram) 60.91% --> 60.41%
2026-01-04 22:53:09,342: Checkpointing model
2026-01-04 22:53:09,969: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:53:10,269: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_14500
2026-01-04 22:53:19,180: Train batch 14600: loss: 12.70 grad norm: 63.15 time: 0.058
2026-01-04 22:53:37,059: Train batch 14800: loss: 5.60 grad norm: 40.20 time: 0.050
2026-01-04 22:53:54,903: Train batch 15000: loss: 8.88 grad norm: 48.56 time: 0.051
2026-01-04 22:53:54,903: Running test after training batch: 15000
2026-01-04 22:53:55,002: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:53:59,712: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 22:53:59,746: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the karst
2026-01-04 22:54:01,563: Val batch 15000: PER (avg): 0.1512 CTC Loss (avg): 15.3315 WER(1gram): 61.93% (n=64) time: 6.660
2026-01-04 22:54:01,564: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=10
2026-01-04 22:54:01,564: t15.2023.08.13 val PER: 0.1247
2026-01-04 22:54:01,564: t15.2023.08.18 val PER: 0.1048
2026-01-04 22:54:01,564: t15.2023.08.20 val PER: 0.1104
2026-01-04 22:54:01,564: t15.2023.08.25 val PER: 0.1009
2026-01-04 22:54:01,564: t15.2023.08.27 val PER: 0.1704
2026-01-04 22:54:01,564: t15.2023.09.01 val PER: 0.0739
2026-01-04 22:54:01,564: t15.2023.09.03 val PER: 0.1603
2026-01-04 22:54:01,564: t15.2023.09.24 val PER: 0.1214
2026-01-04 22:54:01,565: t15.2023.09.29 val PER: 0.1295
2026-01-04 22:54:01,565: t15.2023.10.01 val PER: 0.1645
2026-01-04 22:54:01,565: t15.2023.10.06 val PER: 0.0926
2026-01-04 22:54:01,565: t15.2023.10.08 val PER: 0.2368
2026-01-04 22:54:01,565: t15.2023.10.13 val PER: 0.1978
2026-01-04 22:54:01,565: t15.2023.10.15 val PER: 0.1556
2026-01-04 22:54:01,565: t15.2023.10.20 val PER: 0.1846
2026-01-04 22:54:01,565: t15.2023.10.22 val PER: 0.1136
2026-01-04 22:54:01,565: t15.2023.11.03 val PER: 0.1805
2026-01-04 22:54:01,565: t15.2023.11.04 val PER: 0.0341
2026-01-04 22:54:01,565: t15.2023.11.17 val PER: 0.0389
2026-01-04 22:54:01,565: t15.2023.11.19 val PER: 0.0319
2026-01-04 22:54:01,565: t15.2023.11.26 val PER: 0.1152
2026-01-04 22:54:01,565: t15.2023.12.03 val PER: 0.1019
2026-01-04 22:54:01,566: t15.2023.12.08 val PER: 0.0972
2026-01-04 22:54:01,566: t15.2023.12.10 val PER: 0.0972
2026-01-04 22:54:01,566: t15.2023.12.17 val PER: 0.1310
2026-01-04 22:54:01,566: t15.2023.12.29 val PER: 0.1373
2026-01-04 22:54:01,566: t15.2024.02.25 val PER: 0.1166
2026-01-04 22:54:01,566: t15.2024.03.08 val PER: 0.2134
2026-01-04 22:54:01,566: t15.2024.03.15 val PER: 0.2114
2026-01-04 22:54:01,566: t15.2024.03.17 val PER: 0.1360
2026-01-04 22:54:01,566: t15.2024.05.10 val PER: 0.1753
2026-01-04 22:54:01,566: t15.2024.06.14 val PER: 0.1530
2026-01-04 22:54:01,567: t15.2024.07.19 val PER: 0.2393
2026-01-04 22:54:01,567: t15.2024.07.21 val PER: 0.0952
2026-01-04 22:54:01,567: t15.2024.07.28 val PER: 0.1404
2026-01-04 22:54:01,567: t15.2025.01.10 val PER: 0.2906
2026-01-04 22:54:01,567: t15.2025.01.12 val PER: 0.1609
2026-01-04 22:54:01,567: t15.2025.03.14 val PER: 0.3491
2026-01-04 22:54:01,567: t15.2025.03.16 val PER: 0.2068
2026-01-04 22:54:01,567: t15.2025.03.30 val PER: 0.3023
2026-01-04 22:54:01,567: t15.2025.04.13 val PER: 0.2068
2026-01-04 22:54:01,853: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_15000
2026-01-04 22:54:20,519: Train batch 15200: loss: 4.75 grad norm: 37.35 time: 0.056
2026-01-04 22:54:38,525: Train batch 15400: loss: 11.36 grad norm: 57.00 time: 0.048
2026-01-04 22:54:47,758: Running test after training batch: 15500
2026-01-04 22:54:47,876: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:54:52,901: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 22:54:52,936: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the karst
2026-01-04 22:54:54,737: Val batch 15500: PER (avg): 0.1492 CTC Loss (avg): 15.2349 WER(1gram): 64.97% (n=64) time: 6.979
2026-01-04 22:54:54,738: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=10
2026-01-04 22:54:54,738: t15.2023.08.13 val PER: 0.1258
2026-01-04 22:54:54,738: t15.2023.08.18 val PER: 0.1073
2026-01-04 22:54:54,738: t15.2023.08.20 val PER: 0.1033
2026-01-04 22:54:54,738: t15.2023.08.25 val PER: 0.1024
2026-01-04 22:54:54,738: t15.2023.08.27 val PER: 0.1752
2026-01-04 22:54:54,739: t15.2023.09.01 val PER: 0.0755
2026-01-04 22:54:54,739: t15.2023.09.03 val PER: 0.1615
2026-01-04 22:54:54,739: t15.2023.09.24 val PER: 0.1238
2026-01-04 22:54:54,739: t15.2023.09.29 val PER: 0.1225
2026-01-04 22:54:54,739: t15.2023.10.01 val PER: 0.1671
2026-01-04 22:54:54,739: t15.2023.10.06 val PER: 0.0872
2026-01-04 22:54:54,739: t15.2023.10.08 val PER: 0.2314
2026-01-04 22:54:54,739: t15.2023.10.13 val PER: 0.1978
2026-01-04 22:54:54,739: t15.2023.10.15 val PER: 0.1569
2026-01-04 22:54:54,739: t15.2023.10.20 val PER: 0.1913
2026-01-04 22:54:54,740: t15.2023.10.22 val PER: 0.1125
2026-01-04 22:54:54,740: t15.2023.11.03 val PER: 0.1757
2026-01-04 22:54:54,740: t15.2023.11.04 val PER: 0.0341
2026-01-04 22:54:54,740: t15.2023.11.17 val PER: 0.0342
2026-01-04 22:54:54,740: t15.2023.11.19 val PER: 0.0299
2026-01-04 22:54:54,740: t15.2023.11.26 val PER: 0.1196
2026-01-04 22:54:54,740: t15.2023.12.03 val PER: 0.1029
2026-01-04 22:54:54,740: t15.2023.12.08 val PER: 0.0939
2026-01-04 22:54:54,740: t15.2023.12.10 val PER: 0.1064
2026-01-04 22:54:54,740: t15.2023.12.17 val PER: 0.1258
2026-01-04 22:54:54,741: t15.2023.12.29 val PER: 0.1332
2026-01-04 22:54:54,741: t15.2024.02.25 val PER: 0.1138
2026-01-04 22:54:54,741: t15.2024.03.08 val PER: 0.2134
2026-01-04 22:54:54,741: t15.2024.03.15 val PER: 0.2001
2026-01-04 22:54:54,741: t15.2024.03.17 val PER: 0.1367
2026-01-04 22:54:54,741: t15.2024.05.10 val PER: 0.1709
2026-01-04 22:54:54,741: t15.2024.06.14 val PER: 0.1514
2026-01-04 22:54:54,741: t15.2024.07.19 val PER: 0.2347
2026-01-04 22:54:54,741: t15.2024.07.21 val PER: 0.0959
2026-01-04 22:54:54,741: t15.2024.07.28 val PER: 0.1397
2026-01-04 22:54:54,741: t15.2025.01.10 val PER: 0.2865
2026-01-04 22:54:54,742: t15.2025.01.12 val PER: 0.1532
2026-01-04 22:54:54,742: t15.2025.03.14 val PER: 0.3210
2026-01-04 22:54:54,742: t15.2025.03.16 val PER: 0.2081
2026-01-04 22:54:54,742: t15.2025.03.30 val PER: 0.2966
2026-01-04 22:54:54,742: t15.2025.04.13 val PER: 0.2140
2026-01-04 22:54:55,027: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_15500
2026-01-04 22:55:03,918: Train batch 15600: loss: 11.23 grad norm: 61.51 time: 0.062
2026-01-04 22:55:21,911: Train batch 15800: loss: 14.21 grad norm: 61.93 time: 0.067
2026-01-04 22:55:40,553: Train batch 16000: loss: 7.15 grad norm: 46.10 time: 0.055
2026-01-04 22:55:40,554: Running test after training batch: 16000
2026-01-04 22:55:40,698: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:55:45,384: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 22:55:45,420: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the karst
2026-01-04 22:55:47,314: Val batch 16000: PER (avg): 0.1494 CTC Loss (avg): 15.2401 WER(1gram): 62.94% (n=64) time: 6.761
2026-01-04 22:55:47,315: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-04 22:55:47,315: t15.2023.08.13 val PER: 0.1206
2026-01-04 22:55:47,315: t15.2023.08.18 val PER: 0.1056
2026-01-04 22:55:47,315: t15.2023.08.20 val PER: 0.0945
2026-01-04 22:55:47,315: t15.2023.08.25 val PER: 0.0979
2026-01-04 22:55:47,315: t15.2023.08.27 val PER: 0.1720
2026-01-04 22:55:47,315: t15.2023.09.01 val PER: 0.0706
2026-01-04 22:55:47,316: t15.2023.09.03 val PER: 0.1651
2026-01-04 22:55:47,316: t15.2023.09.24 val PER: 0.1262
2026-01-04 22:55:47,316: t15.2023.09.29 val PER: 0.1238
2026-01-04 22:55:47,316: t15.2023.10.01 val PER: 0.1678
2026-01-04 22:55:47,316: t15.2023.10.06 val PER: 0.0936
2026-01-04 22:55:47,316: t15.2023.10.08 val PER: 0.2409
2026-01-04 22:55:47,316: t15.2023.10.13 val PER: 0.1893
2026-01-04 22:55:47,316: t15.2023.10.15 val PER: 0.1608
2026-01-04 22:55:47,316: t15.2023.10.20 val PER: 0.1846
2026-01-04 22:55:47,316: t15.2023.10.22 val PER: 0.1102
2026-01-04 22:55:47,316: t15.2023.11.03 val PER: 0.1730
2026-01-04 22:55:47,316: t15.2023.11.04 val PER: 0.0273
2026-01-04 22:55:47,316: t15.2023.11.17 val PER: 0.0342
2026-01-04 22:55:47,316: t15.2023.11.19 val PER: 0.0359
2026-01-04 22:55:47,316: t15.2023.11.26 val PER: 0.1188
2026-01-04 22:55:47,316: t15.2023.12.03 val PER: 0.1082
2026-01-04 22:55:47,317: t15.2023.12.08 val PER: 0.0959
2026-01-04 22:55:47,317: t15.2023.12.10 val PER: 0.0986
2026-01-04 22:55:47,317: t15.2023.12.17 val PER: 0.1227
2026-01-04 22:55:47,317: t15.2023.12.29 val PER: 0.1338
2026-01-04 22:55:47,317: t15.2024.02.25 val PER: 0.1152
2026-01-04 22:55:47,317: t15.2024.03.08 val PER: 0.2191
2026-01-04 22:55:47,317: t15.2024.03.15 val PER: 0.2070
2026-01-04 22:55:47,317: t15.2024.03.17 val PER: 0.1360
2026-01-04 22:55:47,317: t15.2024.05.10 val PER: 0.1753
2026-01-04 22:55:47,317: t15.2024.06.14 val PER: 0.1609
2026-01-04 22:55:47,317: t15.2024.07.19 val PER: 0.2334
2026-01-04 22:55:47,317: t15.2024.07.21 val PER: 0.0910
2026-01-04 22:55:47,317: t15.2024.07.28 val PER: 0.1382
2026-01-04 22:55:47,317: t15.2025.01.10 val PER: 0.2906
2026-01-04 22:55:47,317: t15.2025.01.12 val PER: 0.1609
2026-01-04 22:55:47,317: t15.2025.03.14 val PER: 0.3314
2026-01-04 22:55:47,317: t15.2025.03.16 val PER: 0.1950
2026-01-04 22:55:47,318: t15.2025.03.30 val PER: 0.3000
2026-01-04 22:55:47,318: t15.2025.04.13 val PER: 0.2197
2026-01-04 22:55:47,607: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_16000
2026-01-04 22:56:05,991: Train batch 16200: loss: 6.60 grad norm: 47.79 time: 0.055
2026-01-04 22:56:24,116: Train batch 16400: loss: 10.47 grad norm: 59.76 time: 0.057
2026-01-04 22:56:33,198: Running test after training batch: 16500
2026-01-04 22:56:33,288: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:56:38,124: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 22:56:38,159: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the karst
2026-01-04 22:56:40,050: Val batch 16500: PER (avg): 0.1479 CTC Loss (avg): 15.1708 WER(1gram): 61.68% (n=64) time: 6.852
2026-01-04 22:56:40,051: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-04 22:56:40,051: t15.2023.08.13 val PER: 0.1154
2026-01-04 22:56:40,051: t15.2023.08.18 val PER: 0.1039
2026-01-04 22:56:40,051: t15.2023.08.20 val PER: 0.1009
2026-01-04 22:56:40,051: t15.2023.08.25 val PER: 0.0979
2026-01-04 22:56:40,051: t15.2023.08.27 val PER: 0.1752
2026-01-04 22:56:40,051: t15.2023.09.01 val PER: 0.0666
2026-01-04 22:56:40,051: t15.2023.09.03 val PER: 0.1686
2026-01-04 22:56:40,051: t15.2023.09.24 val PER: 0.1201
2026-01-04 22:56:40,052: t15.2023.09.29 val PER: 0.1219
2026-01-04 22:56:40,052: t15.2023.10.01 val PER: 0.1684
2026-01-04 22:56:40,052: t15.2023.10.06 val PER: 0.0872
2026-01-04 22:56:40,052: t15.2023.10.08 val PER: 0.2273
2026-01-04 22:56:40,052: t15.2023.10.13 val PER: 0.1893
2026-01-04 22:56:40,052: t15.2023.10.15 val PER: 0.1575
2026-01-04 22:56:40,052: t15.2023.10.20 val PER: 0.1779
2026-01-04 22:56:40,052: t15.2023.10.22 val PER: 0.1169
2026-01-04 22:56:40,052: t15.2023.11.03 val PER: 0.1737
2026-01-04 22:56:40,052: t15.2023.11.04 val PER: 0.0307
2026-01-04 22:56:40,053: t15.2023.11.17 val PER: 0.0358
2026-01-04 22:56:40,053: t15.2023.11.19 val PER: 0.0379
2026-01-04 22:56:40,053: t15.2023.11.26 val PER: 0.1138
2026-01-04 22:56:40,053: t15.2023.12.03 val PER: 0.1134
2026-01-04 22:56:40,053: t15.2023.12.08 val PER: 0.0952
2026-01-04 22:56:40,053: t15.2023.12.10 val PER: 0.0946
2026-01-04 22:56:40,053: t15.2023.12.17 val PER: 0.1227
2026-01-04 22:56:40,053: t15.2023.12.29 val PER: 0.1345
2026-01-04 22:56:40,054: t15.2024.02.25 val PER: 0.1081
2026-01-04 22:56:40,054: t15.2024.03.08 val PER: 0.2119
2026-01-04 22:56:40,054: t15.2024.03.15 val PER: 0.2083
2026-01-04 22:56:40,054: t15.2024.03.17 val PER: 0.1374
2026-01-04 22:56:40,054: t15.2024.05.10 val PER: 0.1709
2026-01-04 22:56:40,054: t15.2024.06.14 val PER: 0.1562
2026-01-04 22:56:40,054: t15.2024.07.19 val PER: 0.2386
2026-01-04 22:56:40,054: t15.2024.07.21 val PER: 0.0917
2026-01-04 22:56:40,054: t15.2024.07.28 val PER: 0.1368
2026-01-04 22:56:40,055: t15.2025.01.10 val PER: 0.2741
2026-01-04 22:56:40,055: t15.2025.01.12 val PER: 0.1501
2026-01-04 22:56:40,055: t15.2025.03.14 val PER: 0.3388
2026-01-04 22:56:40,055: t15.2025.03.16 val PER: 0.1898
2026-01-04 22:56:40,055: t15.2025.03.30 val PER: 0.2897
2026-01-04 22:56:40,055: t15.2025.04.13 val PER: 0.2183
2026-01-04 22:56:40,343: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_16500
2026-01-04 22:56:49,255: Train batch 16600: loss: 8.29 grad norm: 52.65 time: 0.052
2026-01-04 22:57:07,418: Train batch 16800: loss: 17.15 grad norm: 71.39 time: 0.061
2026-01-04 22:57:25,697: Train batch 17000: loss: 8.51 grad norm: 48.70 time: 0.081
2026-01-04 22:57:25,697: Running test after training batch: 17000
2026-01-04 22:57:25,789: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:57:30,506: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 22:57:30,541: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the cost
2026-01-04 22:57:32,466: Val batch 17000: PER (avg): 0.1478 CTC Loss (avg): 15.0707 WER(1gram): 59.39% (n=64) time: 6.769
2026-01-04 22:57:32,466: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-04 22:57:32,466: t15.2023.08.13 val PER: 0.1154
2026-01-04 22:57:32,466: t15.2023.08.18 val PER: 0.1048
2026-01-04 22:57:32,466: t15.2023.08.20 val PER: 0.1064
2026-01-04 22:57:32,467: t15.2023.08.25 val PER: 0.0979
2026-01-04 22:57:32,467: t15.2023.08.27 val PER: 0.1704
2026-01-04 22:57:32,467: t15.2023.09.01 val PER: 0.0666
2026-01-04 22:57:32,467: t15.2023.09.03 val PER: 0.1698
2026-01-04 22:57:32,467: t15.2023.09.24 val PER: 0.1238
2026-01-04 22:57:32,467: t15.2023.09.29 val PER: 0.1251
2026-01-04 22:57:32,467: t15.2023.10.01 val PER: 0.1671
2026-01-04 22:57:32,467: t15.2023.10.06 val PER: 0.0861
2026-01-04 22:57:32,467: t15.2023.10.08 val PER: 0.2300
2026-01-04 22:57:32,467: t15.2023.10.13 val PER: 0.1932
2026-01-04 22:57:32,467: t15.2023.10.15 val PER: 0.1529
2026-01-04 22:57:32,467: t15.2023.10.20 val PER: 0.1678
2026-01-04 22:57:32,468: t15.2023.10.22 val PER: 0.1203
2026-01-04 22:57:32,468: t15.2023.11.03 val PER: 0.1703
2026-01-04 22:57:32,468: t15.2023.11.04 val PER: 0.0307
2026-01-04 22:57:32,468: t15.2023.11.17 val PER: 0.0358
2026-01-04 22:57:32,468: t15.2023.11.19 val PER: 0.0339
2026-01-04 22:57:32,468: t15.2023.11.26 val PER: 0.1094
2026-01-04 22:57:32,468: t15.2023.12.03 val PER: 0.0998
2026-01-04 22:57:32,468: t15.2023.12.08 val PER: 0.0945
2026-01-04 22:57:32,468: t15.2023.12.10 val PER: 0.0959
2026-01-04 22:57:32,468: t15.2023.12.17 val PER: 0.1227
2026-01-04 22:57:32,468: t15.2023.12.29 val PER: 0.1297
2026-01-04 22:57:32,468: t15.2024.02.25 val PER: 0.1124
2026-01-04 22:57:32,468: t15.2024.03.08 val PER: 0.2105
2026-01-04 22:57:32,468: t15.2024.03.15 val PER: 0.2008
2026-01-04 22:57:32,468: t15.2024.03.17 val PER: 0.1360
2026-01-04 22:57:32,468: t15.2024.05.10 val PER: 0.1694
2026-01-04 22:57:32,468: t15.2024.06.14 val PER: 0.1625
2026-01-04 22:57:32,469: t15.2024.07.19 val PER: 0.2439
2026-01-04 22:57:32,469: t15.2024.07.21 val PER: 0.0897
2026-01-04 22:57:32,469: t15.2024.07.28 val PER: 0.1390
2026-01-04 22:57:32,469: t15.2025.01.10 val PER: 0.2810
2026-01-04 22:57:32,469: t15.2025.01.12 val PER: 0.1517
2026-01-04 22:57:32,469: t15.2025.03.14 val PER: 0.3417
2026-01-04 22:57:32,469: t15.2025.03.16 val PER: 0.1885
2026-01-04 22:57:32,469: t15.2025.03.30 val PER: 0.2989
2026-01-04 22:57:32,469: t15.2025.04.13 val PER: 0.2225
2026-01-04 22:57:32,470: New best val WER(1gram) 60.41% --> 59.39%
2026-01-04 22:57:32,470: Checkpointing model
2026-01-04 22:57:33,082: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-04 22:57:33,377: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_17000
2026-01-04 22:57:51,680: Train batch 17200: loss: 9.60 grad norm: 55.94 time: 0.083
2026-01-04 22:58:10,501: Train batch 17400: loss: 11.94 grad norm: 57.94 time: 0.071
2026-01-04 22:58:19,723: Running test after training batch: 17500
2026-01-04 22:58:19,904: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:58:24,860: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 22:58:24,896: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the karst
2026-01-04 22:58:26,822: Val batch 17500: PER (avg): 0.1476 CTC Loss (avg): 15.1093 WER(1gram): 59.90% (n=64) time: 7.099
2026-01-04 22:58:26,823: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 22:58:26,823: t15.2023.08.13 val PER: 0.1154
2026-01-04 22:58:26,823: t15.2023.08.18 val PER: 0.1006
2026-01-04 22:58:26,823: t15.2023.08.20 val PER: 0.1033
2026-01-04 22:58:26,823: t15.2023.08.25 val PER: 0.0949
2026-01-04 22:58:26,823: t15.2023.08.27 val PER: 0.1704
2026-01-04 22:58:26,823: t15.2023.09.01 val PER: 0.0657
2026-01-04 22:58:26,823: t15.2023.09.03 val PER: 0.1686
2026-01-04 22:58:26,823: t15.2023.09.24 val PER: 0.1250
2026-01-04 22:58:26,824: t15.2023.09.29 val PER: 0.1238
2026-01-04 22:58:26,824: t15.2023.10.01 val PER: 0.1645
2026-01-04 22:58:26,824: t15.2023.10.06 val PER: 0.0872
2026-01-04 22:58:26,824: t15.2023.10.08 val PER: 0.2409
2026-01-04 22:58:26,824: t15.2023.10.13 val PER: 0.1877
2026-01-04 22:58:26,824: t15.2023.10.15 val PER: 0.1549
2026-01-04 22:58:26,824: t15.2023.10.20 val PER: 0.1879
2026-01-04 22:58:26,824: t15.2023.10.22 val PER: 0.1136
2026-01-04 22:58:26,824: t15.2023.11.03 val PER: 0.1764
2026-01-04 22:58:26,824: t15.2023.11.04 val PER: 0.0307
2026-01-04 22:58:26,824: t15.2023.11.17 val PER: 0.0373
2026-01-04 22:58:26,824: t15.2023.11.19 val PER: 0.0299
2026-01-04 22:58:26,824: t15.2023.11.26 val PER: 0.1167
2026-01-04 22:58:26,825: t15.2023.12.03 val PER: 0.1040
2026-01-04 22:58:26,825: t15.2023.12.08 val PER: 0.0992
2026-01-04 22:58:26,825: t15.2023.12.10 val PER: 0.0959
2026-01-04 22:58:26,825: t15.2023.12.17 val PER: 0.1268
2026-01-04 22:58:26,825: t15.2023.12.29 val PER: 0.1352
2026-01-04 22:58:26,825: t15.2024.02.25 val PER: 0.1096
2026-01-04 22:58:26,825: t15.2024.03.08 val PER: 0.2134
2026-01-04 22:58:26,825: t15.2024.03.15 val PER: 0.2020
2026-01-04 22:58:26,825: t15.2024.03.17 val PER: 0.1311
2026-01-04 22:58:26,825: t15.2024.05.10 val PER: 0.1738
2026-01-04 22:58:26,825: t15.2024.06.14 val PER: 0.1483
2026-01-04 22:58:26,825: t15.2024.07.19 val PER: 0.2406
2026-01-04 22:58:26,825: t15.2024.07.21 val PER: 0.0903
2026-01-04 22:58:26,825: t15.2024.07.28 val PER: 0.1390
2026-01-04 22:58:26,825: t15.2025.01.10 val PER: 0.2782
2026-01-04 22:58:26,825: t15.2025.01.12 val PER: 0.1478
2026-01-04 22:58:26,826: t15.2025.03.14 val PER: 0.3284
2026-01-04 22:58:26,826: t15.2025.03.16 val PER: 0.1937
2026-01-04 22:58:26,826: t15.2025.03.30 val PER: 0.2897
2026-01-04 22:58:26,826: t15.2025.04.13 val PER: 0.2211
2026-01-04 22:58:27,115: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_17500
2026-01-04 22:58:36,165: Train batch 17600: loss: 9.82 grad norm: 58.20 time: 0.050
2026-01-04 22:58:54,703: Train batch 17800: loss: 6.50 grad norm: 45.49 time: 0.040
2026-01-04 22:59:13,070: Train batch 18000: loss: 10.84 grad norm: 63.74 time: 0.060
2026-01-04 22:59:13,070: Running test after training batch: 18000
2026-01-04 22:59:13,169: WER debug GT example: You can see the code at this point as well.
2026-01-04 22:59:18,033: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 22:59:18,068: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the cost
2026-01-04 22:59:19,998: Val batch 18000: PER (avg): 0.1470 CTC Loss (avg): 15.0251 WER(1gram): 60.66% (n=64) time: 6.928
2026-01-04 22:59:19,999: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 22:59:19,999: t15.2023.08.13 val PER: 0.1237
2026-01-04 22:59:19,999: t15.2023.08.18 val PER: 0.1081
2026-01-04 22:59:19,999: t15.2023.08.20 val PER: 0.1025
2026-01-04 22:59:19,999: t15.2023.08.25 val PER: 0.0979
2026-01-04 22:59:20,000: t15.2023.08.27 val PER: 0.1833
2026-01-04 22:59:20,000: t15.2023.09.01 val PER: 0.0674
2026-01-04 22:59:20,000: t15.2023.09.03 val PER: 0.1651
2026-01-04 22:59:20,000: t15.2023.09.24 val PER: 0.1214
2026-01-04 22:59:20,000: t15.2023.09.29 val PER: 0.1238
2026-01-04 22:59:20,000: t15.2023.10.01 val PER: 0.1651
2026-01-04 22:59:20,000: t15.2023.10.06 val PER: 0.0850
2026-01-04 22:59:20,000: t15.2023.10.08 val PER: 0.2300
2026-01-04 22:59:20,000: t15.2023.10.13 val PER: 0.1916
2026-01-04 22:59:20,000: t15.2023.10.15 val PER: 0.1575
2026-01-04 22:59:20,000: t15.2023.10.20 val PER: 0.1611
2026-01-04 22:59:20,000: t15.2023.10.22 val PER: 0.1147
2026-01-04 22:59:20,000: t15.2023.11.03 val PER: 0.1744
2026-01-04 22:59:20,000: t15.2023.11.04 val PER: 0.0307
2026-01-04 22:59:20,001: t15.2023.11.17 val PER: 0.0389
2026-01-04 22:59:20,001: t15.2023.11.19 val PER: 0.0259
2026-01-04 22:59:20,001: t15.2023.11.26 val PER: 0.1152
2026-01-04 22:59:20,001: t15.2023.12.03 val PER: 0.1019
2026-01-04 22:59:20,001: t15.2023.12.08 val PER: 0.0979
2026-01-04 22:59:20,001: t15.2023.12.10 val PER: 0.0933
2026-01-04 22:59:20,001: t15.2023.12.17 val PER: 0.1247
2026-01-04 22:59:20,001: t15.2023.12.29 val PER: 0.1311
2026-01-04 22:59:20,001: t15.2024.02.25 val PER: 0.1039
2026-01-04 22:59:20,001: t15.2024.03.08 val PER: 0.2091
2026-01-04 22:59:20,002: t15.2024.03.15 val PER: 0.2051
2026-01-04 22:59:20,002: t15.2024.03.17 val PER: 0.1346
2026-01-04 22:59:20,002: t15.2024.05.10 val PER: 0.1560
2026-01-04 22:59:20,002: t15.2024.06.14 val PER: 0.1498
2026-01-04 22:59:20,002: t15.2024.07.19 val PER: 0.2406
2026-01-04 22:59:20,002: t15.2024.07.21 val PER: 0.0910
2026-01-04 22:59:20,002: t15.2024.07.28 val PER: 0.1331
2026-01-04 22:59:20,002: t15.2025.01.10 val PER: 0.2810
2026-01-04 22:59:20,002: t15.2025.01.12 val PER: 0.1470
2026-01-04 22:59:20,002: t15.2025.03.14 val PER: 0.3314
2026-01-04 22:59:20,002: t15.2025.03.16 val PER: 0.1898
2026-01-04 22:59:20,003: t15.2025.03.30 val PER: 0.2897
2026-01-04 22:59:20,003: t15.2025.04.13 val PER: 0.2154
2026-01-04 22:59:20,298: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_18000
2026-01-04 22:59:38,929: Train batch 18200: loss: 8.21 grad norm: 51.89 time: 0.074
2026-01-04 22:59:57,237: Train batch 18400: loss: 4.91 grad norm: 39.21 time: 0.058
2026-01-04 23:00:06,365: Running test after training batch: 18500
2026-01-04 23:00:06,519: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:00:11,212: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 23:00:11,247: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the cost
2026-01-04 23:00:13,155: Val batch 18500: PER (avg): 0.1467 CTC Loss (avg): 14.9995 WER(1gram): 59.64% (n=64) time: 6.789
2026-01-04 23:00:13,155: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 23:00:13,155: t15.2023.08.13 val PER: 0.1206
2026-01-04 23:00:13,155: t15.2023.08.18 val PER: 0.1031
2026-01-04 23:00:13,155: t15.2023.08.20 val PER: 0.1009
2026-01-04 23:00:13,155: t15.2023.08.25 val PER: 0.0994
2026-01-04 23:00:13,156: t15.2023.08.27 val PER: 0.1768
2026-01-04 23:00:13,156: t15.2023.09.01 val PER: 0.0682
2026-01-04 23:00:13,156: t15.2023.09.03 val PER: 0.1639
2026-01-04 23:00:13,156: t15.2023.09.24 val PER: 0.1189
2026-01-04 23:00:13,156: t15.2023.09.29 val PER: 0.1219
2026-01-04 23:00:13,156: t15.2023.10.01 val PER: 0.1678
2026-01-04 23:00:13,156: t15.2023.10.06 val PER: 0.0861
2026-01-04 23:00:13,156: t15.2023.10.08 val PER: 0.2314
2026-01-04 23:00:13,156: t15.2023.10.13 val PER: 0.1939
2026-01-04 23:00:13,156: t15.2023.10.15 val PER: 0.1602
2026-01-04 23:00:13,156: t15.2023.10.20 val PER: 0.1711
2026-01-04 23:00:13,156: t15.2023.10.22 val PER: 0.1125
2026-01-04 23:00:13,156: t15.2023.11.03 val PER: 0.1750
2026-01-04 23:00:13,156: t15.2023.11.04 val PER: 0.0375
2026-01-04 23:00:13,156: t15.2023.11.17 val PER: 0.0389
2026-01-04 23:00:13,157: t15.2023.11.19 val PER: 0.0319
2026-01-04 23:00:13,157: t15.2023.11.26 val PER: 0.1094
2026-01-04 23:00:13,157: t15.2023.12.03 val PER: 0.0987
2026-01-04 23:00:13,157: t15.2023.12.08 val PER: 0.0972
2026-01-04 23:00:13,157: t15.2023.12.10 val PER: 0.0907
2026-01-04 23:00:13,157: t15.2023.12.17 val PER: 0.1227
2026-01-04 23:00:13,157: t15.2023.12.29 val PER: 0.1332
2026-01-04 23:00:13,157: t15.2024.02.25 val PER: 0.1025
2026-01-04 23:00:13,157: t15.2024.03.08 val PER: 0.2091
2026-01-04 23:00:13,157: t15.2024.03.15 val PER: 0.2039
2026-01-04 23:00:13,157: t15.2024.03.17 val PER: 0.1353
2026-01-04 23:00:13,157: t15.2024.05.10 val PER: 0.1724
2026-01-04 23:00:13,157: t15.2024.06.14 val PER: 0.1467
2026-01-04 23:00:13,157: t15.2024.07.19 val PER: 0.2419
2026-01-04 23:00:13,157: t15.2024.07.21 val PER: 0.0862
2026-01-04 23:00:13,157: t15.2024.07.28 val PER: 0.1360
2026-01-04 23:00:13,158: t15.2025.01.10 val PER: 0.2769
2026-01-04 23:00:13,158: t15.2025.01.12 val PER: 0.1501
2026-01-04 23:00:13,158: t15.2025.03.14 val PER: 0.3254
2026-01-04 23:00:13,158: t15.2025.03.16 val PER: 0.1911
2026-01-04 23:00:13,158: t15.2025.03.30 val PER: 0.2908
2026-01-04 23:00:13,158: t15.2025.04.13 val PER: 0.2097
2026-01-04 23:00:13,443: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_18500
2026-01-04 23:00:22,862: Train batch 18600: loss: 12.93 grad norm: 63.49 time: 0.067
2026-01-04 23:00:41,537: Train batch 18800: loss: 8.84 grad norm: 52.69 time: 0.064
2026-01-04 23:01:00,542: Train batch 19000: loss: 8.24 grad norm: 52.53 time: 0.064
2026-01-04 23:01:00,542: Running test after training batch: 19000
2026-01-04 23:01:00,688: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:01:05,390: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 23:01:05,426: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the cost
2026-01-04 23:01:07,392: Val batch 19000: PER (avg): 0.1464 CTC Loss (avg): 15.0057 WER(1gram): 60.41% (n=64) time: 6.850
2026-01-04 23:01:07,393: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 23:01:07,393: t15.2023.08.13 val PER: 0.1185
2026-01-04 23:01:07,393: t15.2023.08.18 val PER: 0.1023
2026-01-04 23:01:07,393: t15.2023.08.20 val PER: 0.1001
2026-01-04 23:01:07,393: t15.2023.08.25 val PER: 0.1054
2026-01-04 23:01:07,393: t15.2023.08.27 val PER: 0.1768
2026-01-04 23:01:07,393: t15.2023.09.01 val PER: 0.0690
2026-01-04 23:01:07,393: t15.2023.09.03 val PER: 0.1675
2026-01-04 23:01:07,393: t15.2023.09.24 val PER: 0.1117
2026-01-04 23:01:07,394: t15.2023.09.29 val PER: 0.1251
2026-01-04 23:01:07,394: t15.2023.10.01 val PER: 0.1645
2026-01-04 23:01:07,394: t15.2023.10.06 val PER: 0.0893
2026-01-04 23:01:07,394: t15.2023.10.08 val PER: 0.2314
2026-01-04 23:01:07,394: t15.2023.10.13 val PER: 0.1924
2026-01-04 23:01:07,394: t15.2023.10.15 val PER: 0.1529
2026-01-04 23:01:07,394: t15.2023.10.20 val PER: 0.1678
2026-01-04 23:01:07,394: t15.2023.10.22 val PER: 0.1080
2026-01-04 23:01:07,394: t15.2023.11.03 val PER: 0.1750
2026-01-04 23:01:07,394: t15.2023.11.04 val PER: 0.0375
2026-01-04 23:01:07,395: t15.2023.11.17 val PER: 0.0327
2026-01-04 23:01:07,395: t15.2023.11.19 val PER: 0.0279
2026-01-04 23:01:07,395: t15.2023.11.26 val PER: 0.1109
2026-01-04 23:01:07,395: t15.2023.12.03 val PER: 0.1019
2026-01-04 23:01:07,395: t15.2023.12.08 val PER: 0.0939
2026-01-04 23:01:07,395: t15.2023.12.10 val PER: 0.0972
2026-01-04 23:01:07,395: t15.2023.12.17 val PER: 0.1258
2026-01-04 23:01:07,395: t15.2023.12.29 val PER: 0.1325
2026-01-04 23:01:07,395: t15.2024.02.25 val PER: 0.1011
2026-01-04 23:01:07,395: t15.2024.03.08 val PER: 0.2233
2026-01-04 23:01:07,395: t15.2024.03.15 val PER: 0.2039
2026-01-04 23:01:07,395: t15.2024.03.17 val PER: 0.1290
2026-01-04 23:01:07,396: t15.2024.05.10 val PER: 0.1664
2026-01-04 23:01:07,396: t15.2024.06.14 val PER: 0.1625
2026-01-04 23:01:07,396: t15.2024.07.19 val PER: 0.2393
2026-01-04 23:01:07,396: t15.2024.07.21 val PER: 0.0890
2026-01-04 23:01:07,396: t15.2024.07.28 val PER: 0.1360
2026-01-04 23:01:07,396: t15.2025.01.10 val PER: 0.2727
2026-01-04 23:01:07,396: t15.2025.01.12 val PER: 0.1493
2026-01-04 23:01:07,396: t15.2025.03.14 val PER: 0.3254
2026-01-04 23:01:07,396: t15.2025.03.16 val PER: 0.1846
2026-01-04 23:01:07,396: t15.2025.03.30 val PER: 0.2966
2026-01-04 23:01:07,396: t15.2025.04.13 val PER: 0.2126
2026-01-04 23:01:07,687: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_19000
2026-01-04 23:01:26,452: Train batch 19200: loss: 6.46 grad norm: 53.48 time: 0.063
2026-01-04 23:01:45,356: Train batch 19400: loss: 5.19 grad norm: 40.50 time: 0.053
2026-01-04 23:01:54,779: Running test after training batch: 19500
2026-01-04 23:01:54,931: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:01:59,637: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 23:01:59,673: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the karst
2026-01-04 23:02:01,598: Val batch 19500: PER (avg): 0.1457 CTC Loss (avg): 14.9649 WER(1gram): 61.93% (n=64) time: 6.818
2026-01-04 23:02:01,598: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 23:02:01,598: t15.2023.08.13 val PER: 0.1164
2026-01-04 23:02:01,598: t15.2023.08.18 val PER: 0.1006
2026-01-04 23:02:01,598: t15.2023.08.20 val PER: 0.0993
2026-01-04 23:02:01,599: t15.2023.08.25 val PER: 0.0994
2026-01-04 23:02:01,599: t15.2023.08.27 val PER: 0.1672
2026-01-04 23:02:01,599: t15.2023.09.01 val PER: 0.0666
2026-01-04 23:02:01,599: t15.2023.09.03 val PER: 0.1698
2026-01-04 23:02:01,599: t15.2023.09.24 val PER: 0.1129
2026-01-04 23:02:01,599: t15.2023.09.29 val PER: 0.1238
2026-01-04 23:02:01,599: t15.2023.10.01 val PER: 0.1638
2026-01-04 23:02:01,599: t15.2023.10.06 val PER: 0.0797
2026-01-04 23:02:01,599: t15.2023.10.08 val PER: 0.2395
2026-01-04 23:02:01,599: t15.2023.10.13 val PER: 0.1908
2026-01-04 23:02:01,599: t15.2023.10.15 val PER: 0.1575
2026-01-04 23:02:01,599: t15.2023.10.20 val PER: 0.1779
2026-01-04 23:02:01,600: t15.2023.10.22 val PER: 0.1091
2026-01-04 23:02:01,600: t15.2023.11.03 val PER: 0.1784
2026-01-04 23:02:01,600: t15.2023.11.04 val PER: 0.0307
2026-01-04 23:02:01,600: t15.2023.11.17 val PER: 0.0358
2026-01-04 23:02:01,600: t15.2023.11.19 val PER: 0.0359
2026-01-04 23:02:01,600: t15.2023.11.26 val PER: 0.1116
2026-01-04 23:02:01,600: t15.2023.12.03 val PER: 0.1008
2026-01-04 23:02:01,600: t15.2023.12.08 val PER: 0.0919
2026-01-04 23:02:01,600: t15.2023.12.10 val PER: 0.0959
2026-01-04 23:02:01,600: t15.2023.12.17 val PER: 0.1289
2026-01-04 23:02:01,600: t15.2023.12.29 val PER: 0.1325
2026-01-04 23:02:01,600: t15.2024.02.25 val PER: 0.1011
2026-01-04 23:02:01,600: t15.2024.03.08 val PER: 0.2162
2026-01-04 23:02:01,601: t15.2024.03.15 val PER: 0.2001
2026-01-04 23:02:01,601: t15.2024.03.17 val PER: 0.1304
2026-01-04 23:02:01,601: t15.2024.05.10 val PER: 0.1634
2026-01-04 23:02:01,601: t15.2024.06.14 val PER: 0.1498
2026-01-04 23:02:01,601: t15.2024.07.19 val PER: 0.2367
2026-01-04 23:02:01,601: t15.2024.07.21 val PER: 0.0862
2026-01-04 23:02:01,601: t15.2024.07.28 val PER: 0.1338
2026-01-04 23:02:01,601: t15.2025.01.10 val PER: 0.2686
2026-01-04 23:02:01,601: t15.2025.01.12 val PER: 0.1493
2026-01-04 23:02:01,601: t15.2025.03.14 val PER: 0.3343
2026-01-04 23:02:01,601: t15.2025.03.16 val PER: 0.1924
2026-01-04 23:02:01,601: t15.2025.03.30 val PER: 0.2920
2026-01-04 23:02:01,601: t15.2025.04.13 val PER: 0.2111
2026-01-04 23:02:01,897: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_19500
2026-01-04 23:02:11,156: Train batch 19600: loss: 7.90 grad norm: 50.02 time: 0.057
2026-01-04 23:02:29,453: Train batch 19800: loss: 7.52 grad norm: 51.11 time: 0.055
2026-01-04 23:02:48,258: Running test after training batch: 19999
2026-01-04 23:02:48,347: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:02:52,951: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the cold at this point will
2026-01-04 23:02:52,987: WER debug example
  GT : how does it keep the cost down
  PR : how dunst it keep the cost
2026-01-04 23:02:54,922: Val batch 19999: PER (avg): 0.1451 CTC Loss (avg): 14.9523 WER(1gram): 59.64% (n=64) time: 6.664
2026-01-04 23:02:54,923: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-04 23:02:54,923: t15.2023.08.13 val PER: 0.1112
2026-01-04 23:02:54,923: t15.2023.08.18 val PER: 0.0997
2026-01-04 23:02:54,923: t15.2023.08.20 val PER: 0.0993
2026-01-04 23:02:54,923: t15.2023.08.25 val PER: 0.0979
2026-01-04 23:02:54,923: t15.2023.08.27 val PER: 0.1736
2026-01-04 23:02:54,923: t15.2023.09.01 val PER: 0.0657
2026-01-04 23:02:54,923: t15.2023.09.03 val PER: 0.1663
2026-01-04 23:02:54,924: t15.2023.09.24 val PER: 0.1201
2026-01-04 23:02:54,924: t15.2023.09.29 val PER: 0.1251
2026-01-04 23:02:54,924: t15.2023.10.01 val PER: 0.1658
2026-01-04 23:02:54,924: t15.2023.10.06 val PER: 0.0861
2026-01-04 23:02:54,924: t15.2023.10.08 val PER: 0.2327
2026-01-04 23:02:54,924: t15.2023.10.13 val PER: 0.1885
2026-01-04 23:02:54,924: t15.2023.10.15 val PER: 0.1516
2026-01-04 23:02:54,924: t15.2023.10.20 val PER: 0.1644
2026-01-04 23:02:54,924: t15.2023.10.22 val PER: 0.1091
2026-01-04 23:02:54,924: t15.2023.11.03 val PER: 0.1737
2026-01-04 23:02:54,924: t15.2023.11.04 val PER: 0.0307
2026-01-04 23:02:54,924: t15.2023.11.17 val PER: 0.0342
2026-01-04 23:02:54,924: t15.2023.11.19 val PER: 0.0339
2026-01-04 23:02:54,924: t15.2023.11.26 val PER: 0.1072
2026-01-04 23:02:54,924: t15.2023.12.03 val PER: 0.1008
2026-01-04 23:02:54,925: t15.2023.12.08 val PER: 0.0912
2026-01-04 23:02:54,925: t15.2023.12.10 val PER: 0.0959
2026-01-04 23:02:54,925: t15.2023.12.17 val PER: 0.1237
2026-01-04 23:02:54,925: t15.2023.12.29 val PER: 0.1290
2026-01-04 23:02:54,925: t15.2024.02.25 val PER: 0.1067
2026-01-04 23:02:54,925: t15.2024.03.08 val PER: 0.2205
2026-01-04 23:02:54,925: t15.2024.03.15 val PER: 0.2014
2026-01-04 23:02:54,925: t15.2024.03.17 val PER: 0.1346
2026-01-04 23:02:54,925: t15.2024.05.10 val PER: 0.1679
2026-01-04 23:02:54,925: t15.2024.06.14 val PER: 0.1577
2026-01-04 23:02:54,925: t15.2024.07.19 val PER: 0.2380
2026-01-04 23:02:54,925: t15.2024.07.21 val PER: 0.0883
2026-01-04 23:02:54,925: t15.2024.07.28 val PER: 0.1338
2026-01-04 23:02:54,925: t15.2025.01.10 val PER: 0.2700
2026-01-04 23:02:54,925: t15.2025.01.12 val PER: 0.1463
2026-01-04 23:02:54,925: t15.2025.03.14 val PER: 0.3254
2026-01-04 23:02:54,926: t15.2025.03.16 val PER: 0.1793
2026-01-04 23:02:54,926: t15.2025.03.30 val PER: 0.2920
2026-01-04 23:02:54,926: t15.2025.04.13 val PER: 0.2140
2026-01-04 23:02:55,208: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id10_wd1e-5/checkpoint/checkpoint_batch_19999
2026-01-04 23:02:55,239: Best avg val PER achieved: 0.14776
2026-01-04 23:02:55,239: Total training time: 35.73 minutes

=== RUN id15_wd1e-5.yaml ===
2026-01-04 23:03:01,084: Using device: cuda:0
2026-01-04 23:03:02,890: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-04 23:03:02,917: Using 45 sessions after filtering (from 45).
2026-01-04 23:03:03,329: Using torch.compile (if available)
2026-01-04 23:03:03,330: torch.compile not available (torch<2.0). Skipping.
2026-01-04 23:03:03,330: Initialized RNN decoding model
2026-01-04 23:03:03,330: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.15, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-04 23:03:03,331: Model has 44,907,305 parameters
2026-01-04 23:03:03,331: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-04 23:03:04,604: Successfully initialized datasets
2026-01-04 23:03:04,605: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-04 23:03:06,679: Train batch 0: loss: 581.43 grad norm: 1399.87 time: 0.184
2026-01-04 23:03:06,680: Running test after training batch: 0
2026-01-04 23:03:06,792: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:03:11,998: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-04 23:03:12,721: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-04 23:03:46,737: Val batch 0: PER (avg): 1.4307 CTC Loss (avg): 633.1903 WER(1gram): 100.00% (n=64) time: 40.057
2026-01-04 23:03:46,739: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-04 23:03:46,739: t15.2023.08.13 val PER: 1.3077
2026-01-04 23:03:46,739: t15.2023.08.18 val PER: 1.4233
2026-01-04 23:03:46,739: t15.2023.08.20 val PER: 1.3018
2026-01-04 23:03:46,739: t15.2023.08.25 val PER: 1.3343
2026-01-04 23:03:46,739: t15.2023.08.27 val PER: 1.2492
2026-01-04 23:03:46,739: t15.2023.09.01 val PER: 1.4529
2026-01-04 23:03:46,739: t15.2023.09.03 val PER: 1.3159
2026-01-04 23:03:46,739: t15.2023.09.24 val PER: 1.5413
2026-01-04 23:03:46,739: t15.2023.09.29 val PER: 1.4729
2026-01-04 23:03:46,739: t15.2023.10.01 val PER: 1.2160
2026-01-04 23:03:46,739: t15.2023.10.06 val PER: 1.4952
2026-01-04 23:03:46,740: t15.2023.10.08 val PER: 1.1840
2026-01-04 23:03:46,740: t15.2023.10.13 val PER: 1.3964
2026-01-04 23:03:46,740: t15.2023.10.15 val PER: 1.3863
2026-01-04 23:03:46,740: t15.2023.10.20 val PER: 1.5034
2026-01-04 23:03:46,740: t15.2023.10.22 val PER: 1.3920
2026-01-04 23:03:46,740: t15.2023.11.03 val PER: 1.5902
2026-01-04 23:03:46,740: t15.2023.11.04 val PER: 2.0273
2026-01-04 23:03:46,740: t15.2023.11.17 val PER: 1.9673
2026-01-04 23:03:46,740: t15.2023.11.19 val PER: 1.6806
2026-01-04 23:03:46,740: t15.2023.11.26 val PER: 1.5341
2026-01-04 23:03:46,740: t15.2023.12.03 val PER: 1.4286
2026-01-04 23:03:46,740: t15.2023.12.08 val PER: 1.4494
2026-01-04 23:03:46,740: t15.2023.12.10 val PER: 1.6991
2026-01-04 23:03:46,740: t15.2023.12.17 val PER: 1.3087
2026-01-04 23:03:46,740: t15.2023.12.29 val PER: 1.4125
2026-01-04 23:03:46,740: t15.2024.02.25 val PER: 1.4284
2026-01-04 23:03:46,741: t15.2024.03.08 val PER: 1.3229
2026-01-04 23:03:46,741: t15.2024.03.15 val PER: 1.3202
2026-01-04 23:03:46,741: t15.2024.03.17 val PER: 1.4003
2026-01-04 23:03:46,741: t15.2024.05.10 val PER: 1.3210
2026-01-04 23:03:46,741: t15.2024.06.14 val PER: 1.5315
2026-01-04 23:03:46,741: t15.2024.07.19 val PER: 1.0824
2026-01-04 23:03:46,741: t15.2024.07.21 val PER: 1.6352
2026-01-04 23:03:46,741: t15.2024.07.28 val PER: 1.6588
2026-01-04 23:03:46,741: t15.2025.01.10 val PER: 1.0923
2026-01-04 23:03:46,741: t15.2025.01.12 val PER: 1.7706
2026-01-04 23:03:46,741: t15.2025.03.14 val PER: 1.0399
2026-01-04 23:03:46,741: t15.2025.03.16 val PER: 1.6296
2026-01-04 23:03:46,741: t15.2025.03.30 val PER: 1.2885
2026-01-04 23:03:46,742: t15.2025.04.13 val PER: 1.5863
2026-01-04 23:03:46,743: New best val WER(1gram) inf% --> 100.00%
2026-01-04 23:03:46,743: Checkpointing model
2026-01-04 23:03:47,004: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:03:47,274: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_0
2026-01-04 23:04:05,370: Train batch 200: loss: 77.56 grad norm: 96.83 time: 0.054
2026-01-04 23:04:23,099: Train batch 400: loss: 53.47 grad norm: 87.81 time: 0.063
2026-01-04 23:04:31,987: Running test after training batch: 500
2026-01-04 23:04:32,080: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:04:37,121: WER debug example
  GT : you can see the code at this point as well
  PR : yule and ease thus uhde at this ide is aisle
2026-01-04 23:04:37,154: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus ass adz
2026-01-04 23:04:39,446: Val batch 500: PER (avg): 0.5167 CTC Loss (avg): 55.5281 WER(1gram): 88.58% (n=64) time: 7.459
2026-01-04 23:04:39,446: WER lens: avg_true_words=6.16 avg_pred_words=5.58 max_pred_words=11
2026-01-04 23:04:39,447: t15.2023.08.13 val PER: 0.4688
2026-01-04 23:04:39,447: t15.2023.08.18 val PER: 0.4518
2026-01-04 23:04:39,447: t15.2023.08.20 val PER: 0.4424
2026-01-04 23:04:39,447: t15.2023.08.25 val PER: 0.4247
2026-01-04 23:04:39,447: t15.2023.08.27 val PER: 0.5177
2026-01-04 23:04:39,447: t15.2023.09.01 val PER: 0.4148
2026-01-04 23:04:39,447: t15.2023.09.03 val PER: 0.4834
2026-01-04 23:04:39,447: t15.2023.09.24 val PER: 0.4357
2026-01-04 23:04:39,447: t15.2023.09.29 val PER: 0.4633
2026-01-04 23:04:39,447: t15.2023.10.01 val PER: 0.5132
2026-01-04 23:04:39,447: t15.2023.10.06 val PER: 0.4252
2026-01-04 23:04:39,447: t15.2023.10.08 val PER: 0.5345
2026-01-04 23:04:39,448: t15.2023.10.13 val PER: 0.5702
2026-01-04 23:04:39,448: t15.2023.10.15 val PER: 0.4891
2026-01-04 23:04:39,448: t15.2023.10.20 val PER: 0.4564
2026-01-04 23:04:39,448: t15.2023.10.22 val PER: 0.4432
2026-01-04 23:04:39,448: t15.2023.11.03 val PER: 0.4986
2026-01-04 23:04:39,448: t15.2023.11.04 val PER: 0.2526
2026-01-04 23:04:39,448: t15.2023.11.17 val PER: 0.3639
2026-01-04 23:04:39,448: t15.2023.11.19 val PER: 0.3253
2026-01-04 23:04:39,448: t15.2023.11.26 val PER: 0.5522
2026-01-04 23:04:39,448: t15.2023.12.03 val PER: 0.4958
2026-01-04 23:04:39,448: t15.2023.12.08 val PER: 0.5140
2026-01-04 23:04:39,448: t15.2023.12.10 val PER: 0.4586
2026-01-04 23:04:39,448: t15.2023.12.17 val PER: 0.5686
2026-01-04 23:04:39,449: t15.2023.12.29 val PER: 0.5525
2026-01-04 23:04:39,449: t15.2024.02.25 val PER: 0.4733
2026-01-04 23:04:39,449: t15.2024.03.08 val PER: 0.6316
2026-01-04 23:04:39,449: t15.2024.03.15 val PER: 0.5453
2026-01-04 23:04:39,449: t15.2024.03.17 val PER: 0.5007
2026-01-04 23:04:39,449: t15.2024.05.10 val PER: 0.5468
2026-01-04 23:04:39,449: t15.2024.06.14 val PER: 0.5174
2026-01-04 23:04:39,449: t15.2024.07.19 val PER: 0.6737
2026-01-04 23:04:39,449: t15.2024.07.21 val PER: 0.4745
2026-01-04 23:04:39,449: t15.2024.07.28 val PER: 0.5059
2026-01-04 23:04:39,449: t15.2025.01.10 val PER: 0.7548
2026-01-04 23:04:39,449: t15.2025.01.12 val PER: 0.5666
2026-01-04 23:04:39,449: t15.2025.03.14 val PER: 0.7396
2026-01-04 23:04:39,449: t15.2025.03.16 val PER: 0.5877
2026-01-04 23:04:39,449: t15.2025.03.30 val PER: 0.7368
2026-01-04 23:04:39,449: t15.2025.04.13 val PER: 0.5763
2026-01-04 23:04:39,451: New best val WER(1gram) 100.00% --> 88.58%
2026-01-04 23:04:39,451: Checkpointing model
2026-01-04 23:04:40,073: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:04:40,345: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_500
2026-01-04 23:04:49,720: Train batch 600: loss: 49.43 grad norm: 75.28 time: 0.078
2026-01-04 23:05:08,097: Train batch 800: loss: 40.95 grad norm: 84.09 time: 0.057
2026-01-04 23:05:26,646: Train batch 1000: loss: 42.94 grad norm: 81.02 time: 0.066
2026-01-04 23:05:26,647: Running test after training batch: 1000
2026-01-04 23:05:26,840: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:05:31,601: WER debug example
  GT : you can see the code at this point as well
  PR : used id ease thus code it this boyde is while
2026-01-04 23:05:31,633: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus us it
2026-01-04 23:05:33,425: Val batch 1000: PER (avg): 0.4091 CTC Loss (avg): 42.3709 WER(1gram): 81.22% (n=64) time: 6.778
2026-01-04 23:05:33,426: WER lens: avg_true_words=6.16 avg_pred_words=5.48 max_pred_words=12
2026-01-04 23:05:33,426: t15.2023.08.13 val PER: 0.3753
2026-01-04 23:05:33,426: t15.2023.08.18 val PER: 0.3428
2026-01-04 23:05:33,426: t15.2023.08.20 val PER: 0.3463
2026-01-04 23:05:33,426: t15.2023.08.25 val PER: 0.3027
2026-01-04 23:05:33,426: t15.2023.08.27 val PER: 0.4293
2026-01-04 23:05:33,426: t15.2023.09.01 val PER: 0.3052
2026-01-04 23:05:33,426: t15.2023.09.03 val PER: 0.3979
2026-01-04 23:05:33,426: t15.2023.09.24 val PER: 0.3204
2026-01-04 23:05:33,426: t15.2023.09.29 val PER: 0.3740
2026-01-04 23:05:33,427: t15.2023.10.01 val PER: 0.4022
2026-01-04 23:05:33,427: t15.2023.10.06 val PER: 0.3100
2026-01-04 23:05:33,427: t15.2023.10.08 val PER: 0.4655
2026-01-04 23:05:33,427: t15.2023.10.13 val PER: 0.4670
2026-01-04 23:05:33,427: t15.2023.10.15 val PER: 0.3837
2026-01-04 23:05:33,427: t15.2023.10.20 val PER: 0.3523
2026-01-04 23:05:33,427: t15.2023.10.22 val PER: 0.3486
2026-01-04 23:05:33,427: t15.2023.11.03 val PER: 0.3894
2026-01-04 23:05:33,427: t15.2023.11.04 val PER: 0.1570
2026-01-04 23:05:33,427: t15.2023.11.17 val PER: 0.2566
2026-01-04 23:05:33,427: t15.2023.11.19 val PER: 0.2156
2026-01-04 23:05:33,427: t15.2023.11.26 val PER: 0.4500
2026-01-04 23:05:33,428: t15.2023.12.03 val PER: 0.3971
2026-01-04 23:05:33,428: t15.2023.12.08 val PER: 0.4061
2026-01-04 23:05:33,428: t15.2023.12.10 val PER: 0.3509
2026-01-04 23:05:33,428: t15.2023.12.17 val PER: 0.4085
2026-01-04 23:05:33,428: t15.2023.12.29 val PER: 0.4043
2026-01-04 23:05:33,428: t15.2024.02.25 val PER: 0.3610
2026-01-04 23:05:33,428: t15.2024.03.08 val PER: 0.5007
2026-01-04 23:05:33,428: t15.2024.03.15 val PER: 0.4478
2026-01-04 23:05:33,428: t15.2024.03.17 val PER: 0.4079
2026-01-04 23:05:33,428: t15.2024.05.10 val PER: 0.4235
2026-01-04 23:05:33,428: t15.2024.06.14 val PER: 0.3896
2026-01-04 23:05:33,428: t15.2024.07.19 val PER: 0.5320
2026-01-04 23:05:33,428: t15.2024.07.21 val PER: 0.3786
2026-01-04 23:05:33,428: t15.2024.07.28 val PER: 0.4147
2026-01-04 23:05:33,429: t15.2025.01.10 val PER: 0.6157
2026-01-04 23:05:33,429: t15.2025.01.12 val PER: 0.4527
2026-01-04 23:05:33,429: t15.2025.03.14 val PER: 0.6405
2026-01-04 23:05:33,429: t15.2025.03.16 val PER: 0.4791
2026-01-04 23:05:33,429: t15.2025.03.30 val PER: 0.6414
2026-01-04 23:05:33,429: t15.2025.04.13 val PER: 0.4993
2026-01-04 23:05:33,430: New best val WER(1gram) 88.58% --> 81.22%
2026-01-04 23:05:33,430: Checkpointing model
2026-01-04 23:05:34,034: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:05:34,306: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_1000
2026-01-04 23:05:52,605: Train batch 1200: loss: 32.91 grad norm: 75.54 time: 0.067
2026-01-04 23:06:11,201: Train batch 1400: loss: 35.94 grad norm: 78.46 time: 0.060
2026-01-04 23:06:20,571: Running test after training batch: 1500
2026-01-04 23:06:20,691: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:06:25,578: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt e the owed it this boyde is wheel
2026-01-04 23:06:25,608: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that os
2026-01-04 23:06:27,170: Val batch 1500: PER (avg): 0.3815 CTC Loss (avg): 37.1720 WER(1gram): 77.16% (n=64) time: 6.598
2026-01-04 23:06:27,170: WER lens: avg_true_words=6.16 avg_pred_words=4.97 max_pred_words=11
2026-01-04 23:06:27,171: t15.2023.08.13 val PER: 0.3524
2026-01-04 23:06:27,171: t15.2023.08.18 val PER: 0.3160
2026-01-04 23:06:27,171: t15.2023.08.20 val PER: 0.3106
2026-01-04 23:06:27,172: t15.2023.08.25 val PER: 0.2726
2026-01-04 23:06:27,172: t15.2023.08.27 val PER: 0.4003
2026-01-04 23:06:27,172: t15.2023.09.01 val PER: 0.2687
2026-01-04 23:06:27,172: t15.2023.09.03 val PER: 0.3777
2026-01-04 23:06:27,172: t15.2023.09.24 val PER: 0.3131
2026-01-04 23:06:27,172: t15.2023.09.29 val PER: 0.3344
2026-01-04 23:06:27,172: t15.2023.10.01 val PER: 0.3923
2026-01-04 23:06:27,172: t15.2023.10.06 val PER: 0.2928
2026-01-04 23:06:27,173: t15.2023.10.08 val PER: 0.4411
2026-01-04 23:06:27,173: t15.2023.10.13 val PER: 0.4399
2026-01-04 23:06:27,173: t15.2023.10.15 val PER: 0.3626
2026-01-04 23:06:27,173: t15.2023.10.20 val PER: 0.3289
2026-01-04 23:06:27,173: t15.2023.10.22 val PER: 0.3085
2026-01-04 23:06:27,173: t15.2023.11.03 val PER: 0.3643
2026-01-04 23:06:27,173: t15.2023.11.04 val PER: 0.1229
2026-01-04 23:06:27,173: t15.2023.11.17 val PER: 0.2068
2026-01-04 23:06:27,173: t15.2023.11.19 val PER: 0.1776
2026-01-04 23:06:27,173: t15.2023.11.26 val PER: 0.4254
2026-01-04 23:06:27,174: t15.2023.12.03 val PER: 0.3697
2026-01-04 23:06:27,174: t15.2023.12.08 val PER: 0.3575
2026-01-04 23:06:27,174: t15.2023.12.10 val PER: 0.2996
2026-01-04 23:06:27,174: t15.2023.12.17 val PER: 0.3721
2026-01-04 23:06:27,174: t15.2023.12.29 val PER: 0.3789
2026-01-04 23:06:27,174: t15.2024.02.25 val PER: 0.3202
2026-01-04 23:06:27,174: t15.2024.03.08 val PER: 0.4580
2026-01-04 23:06:27,174: t15.2024.03.15 val PER: 0.4184
2026-01-04 23:06:27,174: t15.2024.03.17 val PER: 0.3773
2026-01-04 23:06:27,174: t15.2024.05.10 val PER: 0.3938
2026-01-04 23:06:27,174: t15.2024.06.14 val PER: 0.4054
2026-01-04 23:06:27,174: t15.2024.07.19 val PER: 0.5241
2026-01-04 23:06:27,174: t15.2024.07.21 val PER: 0.3510
2026-01-04 23:06:27,174: t15.2024.07.28 val PER: 0.3735
2026-01-04 23:06:27,175: t15.2025.01.10 val PER: 0.6129
2026-01-04 23:06:27,175: t15.2025.01.12 val PER: 0.4388
2026-01-04 23:06:27,175: t15.2025.03.14 val PER: 0.5902
2026-01-04 23:06:27,175: t15.2025.03.16 val PER: 0.4555
2026-01-04 23:06:27,175: t15.2025.03.30 val PER: 0.6172
2026-01-04 23:06:27,175: t15.2025.04.13 val PER: 0.4822
2026-01-04 23:06:27,175: New best val WER(1gram) 81.22% --> 77.16%
2026-01-04 23:06:27,176: Checkpointing model
2026-01-04 23:06:27,801: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:06:28,076: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_1500
2026-01-04 23:06:37,278: Train batch 1600: loss: 35.90 grad norm: 79.48 time: 0.063
2026-01-04 23:06:55,779: Train batch 1800: loss: 34.72 grad norm: 67.71 time: 0.087
2026-01-04 23:07:14,264: Train batch 2000: loss: 33.07 grad norm: 70.78 time: 0.065
2026-01-04 23:07:14,265: Running test after training batch: 2000
2026-01-04 23:07:14,416: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:07:19,161: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this boyte is will
2026-01-04 23:07:19,189: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke the kos it
2026-01-04 23:07:20,680: Val batch 2000: PER (avg): 0.3286 CTC Loss (avg): 32.6666 WER(1gram): 70.05% (n=64) time: 6.415
2026-01-04 23:07:20,681: WER lens: avg_true_words=6.16 avg_pred_words=5.28 max_pred_words=11
2026-01-04 23:07:20,681: t15.2023.08.13 val PER: 0.2994
2026-01-04 23:07:20,681: t15.2023.08.18 val PER: 0.2540
2026-01-04 23:07:20,681: t15.2023.08.20 val PER: 0.2542
2026-01-04 23:07:20,681: t15.2023.08.25 val PER: 0.2349
2026-01-04 23:07:20,681: t15.2023.08.27 val PER: 0.3312
2026-01-04 23:07:20,681: t15.2023.09.01 val PER: 0.2281
2026-01-04 23:07:20,681: t15.2023.09.03 val PER: 0.3266
2026-01-04 23:07:20,682: t15.2023.09.24 val PER: 0.2694
2026-01-04 23:07:20,682: t15.2023.09.29 val PER: 0.2757
2026-01-04 23:07:20,682: t15.2023.10.01 val PER: 0.3329
2026-01-04 23:07:20,682: t15.2023.10.06 val PER: 0.2357
2026-01-04 23:07:20,682: t15.2023.10.08 val PER: 0.3951
2026-01-04 23:07:20,682: t15.2023.10.13 val PER: 0.3809
2026-01-04 23:07:20,682: t15.2023.10.15 val PER: 0.3072
2026-01-04 23:07:20,682: t15.2023.10.20 val PER: 0.3054
2026-01-04 23:07:20,682: t15.2023.10.22 val PER: 0.2717
2026-01-04 23:07:20,682: t15.2023.11.03 val PER: 0.3209
2026-01-04 23:07:20,682: t15.2023.11.04 val PER: 0.0819
2026-01-04 23:07:20,682: t15.2023.11.17 val PER: 0.1711
2026-01-04 23:07:20,682: t15.2023.11.19 val PER: 0.1297
2026-01-04 23:07:20,683: t15.2023.11.26 val PER: 0.3645
2026-01-04 23:07:20,683: t15.2023.12.03 val PER: 0.3036
2026-01-04 23:07:20,683: t15.2023.12.08 val PER: 0.3096
2026-01-04 23:07:20,683: t15.2023.12.10 val PER: 0.2576
2026-01-04 23:07:20,683: t15.2023.12.17 val PER: 0.3170
2026-01-04 23:07:20,683: t15.2023.12.29 val PER: 0.3205
2026-01-04 23:07:20,683: t15.2024.02.25 val PER: 0.2851
2026-01-04 23:07:20,683: t15.2024.03.08 val PER: 0.3926
2026-01-04 23:07:20,683: t15.2024.03.15 val PER: 0.3615
2026-01-04 23:07:20,683: t15.2024.03.17 val PER: 0.3473
2026-01-04 23:07:20,683: t15.2024.05.10 val PER: 0.3447
2026-01-04 23:07:20,683: t15.2024.06.14 val PER: 0.3391
2026-01-04 23:07:20,683: t15.2024.07.19 val PER: 0.4707
2026-01-04 23:07:20,684: t15.2024.07.21 val PER: 0.2903
2026-01-04 23:07:20,684: t15.2024.07.28 val PER: 0.3213
2026-01-04 23:07:20,684: t15.2025.01.10 val PER: 0.5482
2026-01-04 23:07:20,684: t15.2025.01.12 val PER: 0.3903
2026-01-04 23:07:20,684: t15.2025.03.14 val PER: 0.5251
2026-01-04 23:07:20,684: t15.2025.03.16 val PER: 0.3979
2026-01-04 23:07:20,684: t15.2025.03.30 val PER: 0.5540
2026-01-04 23:07:20,684: t15.2025.04.13 val PER: 0.4194
2026-01-04 23:07:20,686: New best val WER(1gram) 77.16% --> 70.05%
2026-01-04 23:07:20,686: Checkpointing model
2026-01-04 23:07:21,308: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:07:21,589: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_2000
2026-01-04 23:07:39,797: Train batch 2200: loss: 28.72 grad norm: 73.76 time: 0.059
2026-01-04 23:07:58,011: Train batch 2400: loss: 28.43 grad norm: 62.29 time: 0.051
2026-01-04 23:08:07,204: Running test after training batch: 2500
2026-01-04 23:08:07,411: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:08:12,166: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the good at this point is will
2026-01-04 23:08:12,196: WER debug example
  GT : how does it keep the cost down
  PR : houde des it yip the cost nit
2026-01-04 23:08:13,815: Val batch 2500: PER (avg): 0.3015 CTC Loss (avg): 30.0091 WER(1gram): 67.77% (n=64) time: 6.610
2026-01-04 23:08:13,815: WER lens: avg_true_words=6.16 avg_pred_words=5.47 max_pred_words=11
2026-01-04 23:08:13,815: t15.2023.08.13 val PER: 0.2817
2026-01-04 23:08:13,815: t15.2023.08.18 val PER: 0.2330
2026-01-04 23:08:13,815: t15.2023.08.20 val PER: 0.2415
2026-01-04 23:08:13,815: t15.2023.08.25 val PER: 0.1958
2026-01-04 23:08:13,815: t15.2023.08.27 val PER: 0.3183
2026-01-04 23:08:13,816: t15.2023.09.01 val PER: 0.1997
2026-01-04 23:08:13,816: t15.2023.09.03 val PER: 0.3017
2026-01-04 23:08:13,816: t15.2023.09.24 val PER: 0.2330
2026-01-04 23:08:13,816: t15.2023.09.29 val PER: 0.2565
2026-01-04 23:08:13,816: t15.2023.10.01 val PER: 0.3098
2026-01-04 23:08:13,816: t15.2023.10.06 val PER: 0.2142
2026-01-04 23:08:13,816: t15.2023.10.08 val PER: 0.3762
2026-01-04 23:08:13,816: t15.2023.10.13 val PER: 0.3623
2026-01-04 23:08:13,816: t15.2023.10.15 val PER: 0.2887
2026-01-04 23:08:13,816: t15.2023.10.20 val PER: 0.2919
2026-01-04 23:08:13,816: t15.2023.10.22 val PER: 0.2249
2026-01-04 23:08:13,816: t15.2023.11.03 val PER: 0.3026
2026-01-04 23:08:13,817: t15.2023.11.04 val PER: 0.0819
2026-01-04 23:08:13,817: t15.2023.11.17 val PER: 0.1446
2026-01-04 23:08:13,817: t15.2023.11.19 val PER: 0.1277
2026-01-04 23:08:13,817: t15.2023.11.26 val PER: 0.3420
2026-01-04 23:08:13,817: t15.2023.12.03 val PER: 0.2805
2026-01-04 23:08:13,817: t15.2023.12.08 val PER: 0.2750
2026-01-04 23:08:13,817: t15.2023.12.10 val PER: 0.2352
2026-01-04 23:08:13,817: t15.2023.12.17 val PER: 0.2900
2026-01-04 23:08:13,817: t15.2023.12.29 val PER: 0.3006
2026-01-04 23:08:13,817: t15.2024.02.25 val PER: 0.2444
2026-01-04 23:08:13,817: t15.2024.03.08 val PER: 0.3627
2026-01-04 23:08:13,817: t15.2024.03.15 val PER: 0.3440
2026-01-04 23:08:13,817: t15.2024.03.17 val PER: 0.3082
2026-01-04 23:08:13,817: t15.2024.05.10 val PER: 0.3105
2026-01-04 23:08:13,817: t15.2024.06.14 val PER: 0.3028
2026-01-04 23:08:13,817: t15.2024.07.19 val PER: 0.4344
2026-01-04 23:08:13,817: t15.2024.07.21 val PER: 0.2566
2026-01-04 23:08:13,818: t15.2024.07.28 val PER: 0.2963
2026-01-04 23:08:13,818: t15.2025.01.10 val PER: 0.4959
2026-01-04 23:08:13,818: t15.2025.01.12 val PER: 0.3549
2026-01-04 23:08:13,818: t15.2025.03.14 val PER: 0.4970
2026-01-04 23:08:13,818: t15.2025.03.16 val PER: 0.3560
2026-01-04 23:08:13,818: t15.2025.03.30 val PER: 0.5034
2026-01-04 23:08:13,818: t15.2025.04.13 val PER: 0.3809
2026-01-04 23:08:13,819: New best val WER(1gram) 70.05% --> 67.77%
2026-01-04 23:08:13,819: Checkpointing model
2026-01-04 23:08:14,449: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:08:14,724: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_2500
2026-01-04 23:08:23,636: Train batch 2600: loss: 34.85 grad norm: 82.29 time: 0.054
2026-01-04 23:08:41,820: Train batch 2800: loss: 25.58 grad norm: 69.41 time: 0.080
2026-01-04 23:09:00,070: Train batch 3000: loss: 31.24 grad norm: 76.59 time: 0.082
2026-01-04 23:09:00,070: Running test after training batch: 3000
2026-01-04 23:09:00,214: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:09:04,943: WER debug example
  GT : you can see the code at this point as well
  PR : yule end sze the could at this point is will
2026-01-04 23:09:04,972: WER debug example
  GT : how does it keep the cost down
  PR : houde des it keep the rost et
2026-01-04 23:09:06,630: Val batch 3000: PER (avg): 0.2776 CTC Loss (avg): 27.6745 WER(1gram): 64.97% (n=64) time: 6.560
2026-01-04 23:09:06,631: WER lens: avg_true_words=6.16 avg_pred_words=5.84 max_pred_words=10
2026-01-04 23:09:06,631: t15.2023.08.13 val PER: 0.2620
2026-01-04 23:09:06,631: t15.2023.08.18 val PER: 0.2154
2026-01-04 23:09:06,631: t15.2023.08.20 val PER: 0.2184
2026-01-04 23:09:06,631: t15.2023.08.25 val PER: 0.1852
2026-01-04 23:09:06,631: t15.2023.08.27 val PER: 0.2862
2026-01-04 23:09:06,631: t15.2023.09.01 val PER: 0.1810
2026-01-04 23:09:06,631: t15.2023.09.03 val PER: 0.2838
2026-01-04 23:09:06,632: t15.2023.09.24 val PER: 0.2087
2026-01-04 23:09:06,632: t15.2023.09.29 val PER: 0.2272
2026-01-04 23:09:06,632: t15.2023.10.01 val PER: 0.2728
2026-01-04 23:09:06,632: t15.2023.10.06 val PER: 0.1905
2026-01-04 23:09:06,632: t15.2023.10.08 val PER: 0.3424
2026-01-04 23:09:06,632: t15.2023.10.13 val PER: 0.3351
2026-01-04 23:09:06,632: t15.2023.10.15 val PER: 0.2630
2026-01-04 23:09:06,632: t15.2023.10.20 val PER: 0.2550
2026-01-04 23:09:06,632: t15.2023.10.22 val PER: 0.1982
2026-01-04 23:09:06,632: t15.2023.11.03 val PER: 0.2700
2026-01-04 23:09:06,633: t15.2023.11.04 val PER: 0.0887
2026-01-04 23:09:06,633: t15.2023.11.17 val PER: 0.1291
2026-01-04 23:09:06,633: t15.2023.11.19 val PER: 0.1118
2026-01-04 23:09:06,633: t15.2023.11.26 val PER: 0.3051
2026-01-04 23:09:06,633: t15.2023.12.03 val PER: 0.2679
2026-01-04 23:09:06,633: t15.2023.12.08 val PER: 0.2490
2026-01-04 23:09:06,633: t15.2023.12.10 val PER: 0.2089
2026-01-04 23:09:06,633: t15.2023.12.17 val PER: 0.2838
2026-01-04 23:09:06,633: t15.2023.12.29 val PER: 0.2773
2026-01-04 23:09:06,633: t15.2024.02.25 val PER: 0.2430
2026-01-04 23:09:06,633: t15.2024.03.08 val PER: 0.3656
2026-01-04 23:09:06,633: t15.2024.03.15 val PER: 0.3358
2026-01-04 23:09:06,634: t15.2024.03.17 val PER: 0.2741
2026-01-04 23:09:06,634: t15.2024.05.10 val PER: 0.3061
2026-01-04 23:09:06,634: t15.2024.06.14 val PER: 0.3044
2026-01-04 23:09:06,634: t15.2024.07.19 val PER: 0.3896
2026-01-04 23:09:06,634: t15.2024.07.21 val PER: 0.2297
2026-01-04 23:09:06,634: t15.2024.07.28 val PER: 0.2654
2026-01-04 23:09:06,634: t15.2025.01.10 val PER: 0.4904
2026-01-04 23:09:06,634: t15.2025.01.12 val PER: 0.3287
2026-01-04 23:09:06,634: t15.2025.03.14 val PER: 0.4349
2026-01-04 23:09:06,634: t15.2025.03.16 val PER: 0.3325
2026-01-04 23:09:06,635: t15.2025.03.30 val PER: 0.4747
2026-01-04 23:09:06,635: t15.2025.04.13 val PER: 0.3566
2026-01-04 23:09:06,635: New best val WER(1gram) 67.77% --> 64.97%
2026-01-04 23:09:06,635: Checkpointing model
2026-01-04 23:09:07,235: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:09:07,504: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_3000
2026-01-04 23:09:26,619: Train batch 3200: loss: 26.45 grad norm: 70.17 time: 0.075
2026-01-04 23:09:44,263: Train batch 3400: loss: 17.99 grad norm: 55.48 time: 0.048
2026-01-04 23:09:53,275: Running test after training batch: 3500
2026-01-04 23:09:53,415: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:09:58,136: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-04 23:09:58,165: WER debug example
  GT : how does it keep the cost down
  PR : aue des it heap thus wass get
2026-01-04 23:09:59,782: Val batch 3500: PER (avg): 0.2655 CTC Loss (avg): 26.5054 WER(1gram): 64.97% (n=64) time: 6.506
2026-01-04 23:09:59,782: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=11
2026-01-04 23:09:59,782: t15.2023.08.13 val PER: 0.2432
2026-01-04 23:09:59,782: t15.2023.08.18 val PER: 0.2037
2026-01-04 23:09:59,782: t15.2023.08.20 val PER: 0.2153
2026-01-04 23:09:59,783: t15.2023.08.25 val PER: 0.1822
2026-01-04 23:09:59,783: t15.2023.08.27 val PER: 0.2717
2026-01-04 23:09:59,783: t15.2023.09.01 val PER: 0.1769
2026-01-04 23:09:59,783: t15.2023.09.03 val PER: 0.2720
2026-01-04 23:09:59,783: t15.2023.09.24 val PER: 0.2112
2026-01-04 23:09:59,783: t15.2023.09.29 val PER: 0.2240
2026-01-04 23:09:59,783: t15.2023.10.01 val PER: 0.2741
2026-01-04 23:09:59,783: t15.2023.10.06 val PER: 0.1927
2026-01-04 23:09:59,783: t15.2023.10.08 val PER: 0.3329
2026-01-04 23:09:59,783: t15.2023.10.13 val PER: 0.3057
2026-01-04 23:09:59,783: t15.2023.10.15 val PER: 0.2446
2026-01-04 23:09:59,783: t15.2023.10.20 val PER: 0.2483
2026-01-04 23:09:59,783: t15.2023.10.22 val PER: 0.1927
2026-01-04 23:09:59,784: t15.2023.11.03 val PER: 0.2612
2026-01-04 23:09:59,784: t15.2023.11.04 val PER: 0.0751
2026-01-04 23:09:59,784: t15.2023.11.17 val PER: 0.1182
2026-01-04 23:09:59,784: t15.2023.11.19 val PER: 0.1078
2026-01-04 23:09:59,784: t15.2023.11.26 val PER: 0.2870
2026-01-04 23:09:59,784: t15.2023.12.03 val PER: 0.2374
2026-01-04 23:09:59,784: t15.2023.12.08 val PER: 0.2503
2026-01-04 23:09:59,784: t15.2023.12.10 val PER: 0.1984
2026-01-04 23:09:59,784: t15.2023.12.17 val PER: 0.2599
2026-01-04 23:09:59,784: t15.2023.12.29 val PER: 0.2629
2026-01-04 23:09:59,785: t15.2024.02.25 val PER: 0.2149
2026-01-04 23:09:59,785: t15.2024.03.08 val PER: 0.3329
2026-01-04 23:09:59,785: t15.2024.03.15 val PER: 0.3071
2026-01-04 23:09:59,785: t15.2024.03.17 val PER: 0.2768
2026-01-04 23:09:59,785: t15.2024.05.10 val PER: 0.2660
2026-01-04 23:09:59,785: t15.2024.06.14 val PER: 0.2886
2026-01-04 23:09:59,785: t15.2024.07.19 val PER: 0.3863
2026-01-04 23:09:59,785: t15.2024.07.21 val PER: 0.2221
2026-01-04 23:09:59,785: t15.2024.07.28 val PER: 0.2684
2026-01-04 23:09:59,786: t15.2025.01.10 val PER: 0.4559
2026-01-04 23:09:59,786: t15.2025.01.12 val PER: 0.2918
2026-01-04 23:09:59,786: t15.2025.03.14 val PER: 0.4571
2026-01-04 23:09:59,786: t15.2025.03.16 val PER: 0.3154
2026-01-04 23:09:59,786: t15.2025.03.30 val PER: 0.4517
2026-01-04 23:09:59,786: t15.2025.04.13 val PER: 0.3509
2026-01-04 23:10:00,047: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_3500
2026-01-04 23:10:09,093: Train batch 3600: loss: 22.60 grad norm: 62.99 time: 0.067
2026-01-04 23:10:27,412: Train batch 3800: loss: 25.42 grad norm: 69.60 time: 0.066
2026-01-04 23:10:46,297: Train batch 4000: loss: 19.06 grad norm: 54.32 time: 0.056
2026-01-04 23:10:46,298: Running test after training batch: 4000
2026-01-04 23:10:46,461: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:10:51,302: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this boyte is will
2026-01-04 23:10:51,330: WER debug example
  GT : how does it keep the cost down
  PR : aue des it keep the cost nett
2026-01-04 23:10:52,930: Val batch 4000: PER (avg): 0.2463 CTC Loss (avg): 24.2426 WER(1gram): 63.96% (n=64) time: 6.632
2026-01-04 23:10:52,930: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=11
2026-01-04 23:10:52,930: t15.2023.08.13 val PER: 0.2287
2026-01-04 23:10:52,930: t15.2023.08.18 val PER: 0.2037
2026-01-04 23:10:52,930: t15.2023.08.20 val PER: 0.2041
2026-01-04 23:10:52,931: t15.2023.08.25 val PER: 0.1476
2026-01-04 23:10:52,931: t15.2023.08.27 val PER: 0.2781
2026-01-04 23:10:52,931: t15.2023.09.01 val PER: 0.1575
2026-01-04 23:10:52,931: t15.2023.09.03 val PER: 0.2399
2026-01-04 23:10:52,931: t15.2023.09.24 val PER: 0.2002
2026-01-04 23:10:52,931: t15.2023.09.29 val PER: 0.2061
2026-01-04 23:10:52,931: t15.2023.10.01 val PER: 0.2583
2026-01-04 23:10:52,931: t15.2023.10.06 val PER: 0.1593
2026-01-04 23:10:52,931: t15.2023.10.08 val PER: 0.3194
2026-01-04 23:10:52,931: t15.2023.10.13 val PER: 0.3010
2026-01-04 23:10:52,931: t15.2023.10.15 val PER: 0.2399
2026-01-04 23:10:52,931: t15.2023.10.20 val PER: 0.2349
2026-01-04 23:10:52,931: t15.2023.10.22 val PER: 0.1815
2026-01-04 23:10:52,931: t15.2023.11.03 val PER: 0.2422
2026-01-04 23:10:52,931: t15.2023.11.04 val PER: 0.0717
2026-01-04 23:10:52,932: t15.2023.11.17 val PER: 0.0964
2026-01-04 23:10:52,932: t15.2023.11.19 val PER: 0.1018
2026-01-04 23:10:52,932: t15.2023.11.26 val PER: 0.2630
2026-01-04 23:10:52,932: t15.2023.12.03 val PER: 0.2174
2026-01-04 23:10:52,932: t15.2023.12.08 val PER: 0.2170
2026-01-04 23:10:52,932: t15.2023.12.10 val PER: 0.1748
2026-01-04 23:10:52,932: t15.2023.12.17 val PER: 0.2277
2026-01-04 23:10:52,932: t15.2023.12.29 val PER: 0.2464
2026-01-04 23:10:52,932: t15.2024.02.25 val PER: 0.2107
2026-01-04 23:10:52,932: t15.2024.03.08 val PER: 0.3286
2026-01-04 23:10:52,932: t15.2024.03.15 val PER: 0.2871
2026-01-04 23:10:52,933: t15.2024.03.17 val PER: 0.2615
2026-01-04 23:10:52,933: t15.2024.05.10 val PER: 0.2764
2026-01-04 23:10:52,933: t15.2024.06.14 val PER: 0.2760
2026-01-04 23:10:52,933: t15.2024.07.19 val PER: 0.3579
2026-01-04 23:10:52,933: t15.2024.07.21 val PER: 0.1903
2026-01-04 23:10:52,933: t15.2024.07.28 val PER: 0.2375
2026-01-04 23:10:52,933: t15.2025.01.10 val PER: 0.4242
2026-01-04 23:10:52,933: t15.2025.01.12 val PER: 0.2725
2026-01-04 23:10:52,933: t15.2025.03.14 val PER: 0.4157
2026-01-04 23:10:52,933: t15.2025.03.16 val PER: 0.3010
2026-01-04 23:10:52,933: t15.2025.03.30 val PER: 0.4000
2026-01-04 23:10:52,933: t15.2025.04.13 val PER: 0.3195
2026-01-04 23:10:52,934: New best val WER(1gram) 64.97% --> 63.96%
2026-01-04 23:10:52,934: Checkpointing model
2026-01-04 23:10:53,555: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:10:53,825: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_4000
2026-01-04 23:11:12,124: Train batch 4200: loss: 22.91 grad norm: 65.20 time: 0.079
2026-01-04 23:11:30,511: Train batch 4400: loss: 17.15 grad norm: 57.71 time: 0.066
2026-01-04 23:11:39,748: Running test after training batch: 4500
2026-01-04 23:11:39,844: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:11:44,618: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 23:11:44,648: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it yip thus cost get
2026-01-04 23:11:46,227: Val batch 4500: PER (avg): 0.2376 CTC Loss (avg): 23.2750 WER(1gram): 59.64% (n=64) time: 6.479
2026-01-04 23:11:46,227: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-04 23:11:46,227: t15.2023.08.13 val PER: 0.1985
2026-01-04 23:11:46,228: t15.2023.08.18 val PER: 0.1777
2026-01-04 23:11:46,228: t15.2023.08.20 val PER: 0.1938
2026-01-04 23:11:46,228: t15.2023.08.25 val PER: 0.1310
2026-01-04 23:11:46,228: t15.2023.08.27 val PER: 0.2556
2026-01-04 23:11:46,228: t15.2023.09.01 val PER: 0.1461
2026-01-04 23:11:46,228: t15.2023.09.03 val PER: 0.2411
2026-01-04 23:11:46,228: t15.2023.09.24 val PER: 0.1711
2026-01-04 23:11:46,228: t15.2023.09.29 val PER: 0.1959
2026-01-04 23:11:46,228: t15.2023.10.01 val PER: 0.2589
2026-01-04 23:11:46,228: t15.2023.10.06 val PER: 0.1561
2026-01-04 23:11:46,229: t15.2023.10.08 val PER: 0.3342
2026-01-04 23:11:46,229: t15.2023.10.13 val PER: 0.2971
2026-01-04 23:11:46,229: t15.2023.10.15 val PER: 0.2353
2026-01-04 23:11:46,229: t15.2023.10.20 val PER: 0.2248
2026-01-04 23:11:46,229: t15.2023.10.22 val PER: 0.1882
2026-01-04 23:11:46,229: t15.2023.11.03 val PER: 0.2381
2026-01-04 23:11:46,229: t15.2023.11.04 val PER: 0.0546
2026-01-04 23:11:46,229: t15.2023.11.17 val PER: 0.0933
2026-01-04 23:11:46,229: t15.2023.11.19 val PER: 0.0898
2026-01-04 23:11:46,229: t15.2023.11.26 val PER: 0.2710
2026-01-04 23:11:46,230: t15.2023.12.03 val PER: 0.2153
2026-01-04 23:11:46,230: t15.2023.12.08 val PER: 0.2144
2026-01-04 23:11:46,230: t15.2023.12.10 val PER: 0.1761
2026-01-04 23:11:46,230: t15.2023.12.17 val PER: 0.2308
2026-01-04 23:11:46,230: t15.2023.12.29 val PER: 0.2478
2026-01-04 23:11:46,230: t15.2024.02.25 val PER: 0.1924
2026-01-04 23:11:46,230: t15.2024.03.08 val PER: 0.3300
2026-01-04 23:11:46,230: t15.2024.03.15 val PER: 0.2921
2026-01-04 23:11:46,230: t15.2024.03.17 val PER: 0.2336
2026-01-04 23:11:46,231: t15.2024.05.10 val PER: 0.2541
2026-01-04 23:11:46,231: t15.2024.06.14 val PER: 0.2555
2026-01-04 23:11:46,231: t15.2024.07.19 val PER: 0.3368
2026-01-04 23:11:46,231: t15.2024.07.21 val PER: 0.1683
2026-01-04 23:11:46,231: t15.2024.07.28 val PER: 0.2235
2026-01-04 23:11:46,231: t15.2025.01.10 val PER: 0.4132
2026-01-04 23:11:46,231: t15.2025.01.12 val PER: 0.2694
2026-01-04 23:11:46,231: t15.2025.03.14 val PER: 0.3876
2026-01-04 23:11:46,231: t15.2025.03.16 val PER: 0.2866
2026-01-04 23:11:46,231: t15.2025.03.30 val PER: 0.4218
2026-01-04 23:11:46,231: t15.2025.04.13 val PER: 0.2867
2026-01-04 23:11:46,232: New best val WER(1gram) 63.96% --> 59.64%
2026-01-04 23:11:46,232: Checkpointing model
2026-01-04 23:11:46,877: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:11:47,150: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_4500
2026-01-04 23:11:56,320: Train batch 4600: loss: 20.24 grad norm: 61.56 time: 0.062
2026-01-04 23:12:15,007: Train batch 4800: loss: 13.85 grad norm: 56.05 time: 0.063
2026-01-04 23:12:33,136: Train batch 5000: loss: 31.31 grad norm: 87.07 time: 0.064
2026-01-04 23:12:33,136: Running test after training batch: 5000
2026-01-04 23:12:33,277: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:12:38,109: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-04 23:12:38,136: WER debug example
  GT : how does it keep the cost down
  PR : houde des it keep the cost et
2026-01-04 23:12:39,741: Val batch 5000: PER (avg): 0.2253 CTC Loss (avg): 22.0064 WER(1gram): 61.68% (n=64) time: 6.605
2026-01-04 23:12:39,742: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 23:12:39,742: t15.2023.08.13 val PER: 0.2037
2026-01-04 23:12:39,742: t15.2023.08.18 val PER: 0.1702
2026-01-04 23:12:39,742: t15.2023.08.20 val PER: 0.1732
2026-01-04 23:12:39,742: t15.2023.08.25 val PER: 0.1340
2026-01-04 23:12:39,742: t15.2023.08.27 val PER: 0.2444
2026-01-04 23:12:39,742: t15.2023.09.01 val PER: 0.1388
2026-01-04 23:12:39,742: t15.2023.09.03 val PER: 0.2114
2026-01-04 23:12:39,742: t15.2023.09.24 val PER: 0.1917
2026-01-04 23:12:39,742: t15.2023.09.29 val PER: 0.1825
2026-01-04 23:12:39,742: t15.2023.10.01 val PER: 0.2312
2026-01-04 23:12:39,742: t15.2023.10.06 val PER: 0.1399
2026-01-04 23:12:39,743: t15.2023.10.08 val PER: 0.3045
2026-01-04 23:12:39,743: t15.2023.10.13 val PER: 0.2777
2026-01-04 23:12:39,743: t15.2023.10.15 val PER: 0.2281
2026-01-04 23:12:39,743: t15.2023.10.20 val PER: 0.2315
2026-01-04 23:12:39,743: t15.2023.10.22 val PER: 0.1693
2026-01-04 23:12:39,743: t15.2023.11.03 val PER: 0.2266
2026-01-04 23:12:39,743: t15.2023.11.04 val PER: 0.0478
2026-01-04 23:12:39,743: t15.2023.11.17 val PER: 0.0809
2026-01-04 23:12:39,743: t15.2023.11.19 val PER: 0.0679
2026-01-04 23:12:39,743: t15.2023.11.26 val PER: 0.2355
2026-01-04 23:12:39,743: t15.2023.12.03 val PER: 0.1996
2026-01-04 23:12:39,743: t15.2023.12.08 val PER: 0.2064
2026-01-04 23:12:39,743: t15.2023.12.10 val PER: 0.1774
2026-01-04 23:12:39,743: t15.2023.12.17 val PER: 0.2235
2026-01-04 23:12:39,743: t15.2023.12.29 val PER: 0.2203
2026-01-04 23:12:39,744: t15.2024.02.25 val PER: 0.1980
2026-01-04 23:12:39,744: t15.2024.03.08 val PER: 0.2987
2026-01-04 23:12:39,744: t15.2024.03.15 val PER: 0.2833
2026-01-04 23:12:39,744: t15.2024.03.17 val PER: 0.2434
2026-01-04 23:12:39,744: t15.2024.05.10 val PER: 0.2422
2026-01-04 23:12:39,744: t15.2024.06.14 val PER: 0.2461
2026-01-04 23:12:39,744: t15.2024.07.19 val PER: 0.3263
2026-01-04 23:12:39,744: t15.2024.07.21 val PER: 0.1717
2026-01-04 23:12:39,744: t15.2024.07.28 val PER: 0.2074
2026-01-04 23:12:39,744: t15.2025.01.10 val PER: 0.3815
2026-01-04 23:12:39,744: t15.2025.01.12 val PER: 0.2564
2026-01-04 23:12:39,744: t15.2025.03.14 val PER: 0.3861
2026-01-04 23:12:39,744: t15.2025.03.16 val PER: 0.2670
2026-01-04 23:12:39,744: t15.2025.03.30 val PER: 0.3839
2026-01-04 23:12:39,744: t15.2025.04.13 val PER: 0.2981
2026-01-04 23:12:40,007: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_5000
2026-01-04 23:12:58,581: Train batch 5200: loss: 16.45 grad norm: 57.63 time: 0.051
2026-01-04 23:13:16,773: Train batch 5400: loss: 17.12 grad norm: 57.35 time: 0.068
2026-01-04 23:13:25,807: Running test after training batch: 5500
2026-01-04 23:13:25,961: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:13:30,690: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point will
2026-01-04 23:13:30,718: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost tet
2026-01-04 23:13:32,314: Val batch 5500: PER (avg): 0.2154 CTC Loss (avg): 21.0207 WER(1gram): 54.57% (n=64) time: 6.506
2026-01-04 23:13:32,314: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=11
2026-01-04 23:13:32,314: t15.2023.08.13 val PER: 0.1788
2026-01-04 23:13:32,314: t15.2023.08.18 val PER: 0.1668
2026-01-04 23:13:32,314: t15.2023.08.20 val PER: 0.1708
2026-01-04 23:13:32,314: t15.2023.08.25 val PER: 0.1205
2026-01-04 23:13:32,314: t15.2023.08.27 val PER: 0.2299
2026-01-04 23:13:32,315: t15.2023.09.01 val PER: 0.1218
2026-01-04 23:13:32,315: t15.2023.09.03 val PER: 0.2257
2026-01-04 23:13:32,315: t15.2023.09.24 val PER: 0.1699
2026-01-04 23:13:32,315: t15.2023.09.29 val PER: 0.1672
2026-01-04 23:13:32,315: t15.2023.10.01 val PER: 0.2371
2026-01-04 23:13:32,315: t15.2023.10.06 val PER: 0.1335
2026-01-04 23:13:32,315: t15.2023.10.08 val PER: 0.3045
2026-01-04 23:13:32,315: t15.2023.10.13 val PER: 0.2731
2026-01-04 23:13:32,315: t15.2023.10.15 val PER: 0.2030
2026-01-04 23:13:32,315: t15.2023.10.20 val PER: 0.2315
2026-01-04 23:13:32,315: t15.2023.10.22 val PER: 0.1537
2026-01-04 23:13:32,315: t15.2023.11.03 val PER: 0.2212
2026-01-04 23:13:32,315: t15.2023.11.04 val PER: 0.0614
2026-01-04 23:13:32,315: t15.2023.11.17 val PER: 0.0793
2026-01-04 23:13:32,316: t15.2023.11.19 val PER: 0.0599
2026-01-04 23:13:32,316: t15.2023.11.26 val PER: 0.2239
2026-01-04 23:13:32,316: t15.2023.12.03 val PER: 0.1891
2026-01-04 23:13:32,316: t15.2023.12.08 val PER: 0.1924
2026-01-04 23:13:32,316: t15.2023.12.10 val PER: 0.1551
2026-01-04 23:13:32,316: t15.2023.12.17 val PER: 0.2266
2026-01-04 23:13:32,316: t15.2023.12.29 val PER: 0.2100
2026-01-04 23:13:32,316: t15.2024.02.25 val PER: 0.1770
2026-01-04 23:13:32,316: t15.2024.03.08 val PER: 0.2916
2026-01-04 23:13:32,316: t15.2024.03.15 val PER: 0.2552
2026-01-04 23:13:32,316: t15.2024.03.17 val PER: 0.2127
2026-01-04 23:13:32,316: t15.2024.05.10 val PER: 0.2467
2026-01-04 23:13:32,316: t15.2024.06.14 val PER: 0.2461
2026-01-04 23:13:32,316: t15.2024.07.19 val PER: 0.3125
2026-01-04 23:13:32,317: t15.2024.07.21 val PER: 0.1731
2026-01-04 23:13:32,317: t15.2024.07.28 val PER: 0.2110
2026-01-04 23:13:32,317: t15.2025.01.10 val PER: 0.3981
2026-01-04 23:13:32,317: t15.2025.01.12 val PER: 0.2379
2026-01-04 23:13:32,317: t15.2025.03.14 val PER: 0.3669
2026-01-04 23:13:32,317: t15.2025.03.16 val PER: 0.2670
2026-01-04 23:13:32,317: t15.2025.03.30 val PER: 0.3644
2026-01-04 23:13:32,317: t15.2025.04.13 val PER: 0.2867
2026-01-04 23:13:32,318: New best val WER(1gram) 59.64% --> 54.57%
2026-01-04 23:13:32,318: Checkpointing model
2026-01-04 23:13:32,927: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:13:33,198: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_5500
2026-01-04 23:13:42,164: Train batch 5600: loss: 19.55 grad norm: 78.70 time: 0.062
2026-01-04 23:14:00,230: Train batch 5800: loss: 13.19 grad norm: 56.40 time: 0.082
2026-01-04 23:14:18,302: Train batch 6000: loss: 13.97 grad norm: 56.86 time: 0.049
2026-01-04 23:14:18,302: Running test after training batch: 6000
2026-01-04 23:14:18,420: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:14:23,210: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the good at this point is will
2026-01-04 23:14:23,240: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 23:14:24,845: Val batch 6000: PER (avg): 0.2100 CTC Loss (avg): 20.7515 WER(1gram): 58.88% (n=64) time: 6.543
2026-01-04 23:14:24,846: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-04 23:14:24,846: t15.2023.08.13 val PER: 0.1757
2026-01-04 23:14:24,846: t15.2023.08.18 val PER: 0.1618
2026-01-04 23:14:24,846: t15.2023.08.20 val PER: 0.1660
2026-01-04 23:14:24,846: t15.2023.08.25 val PER: 0.1220
2026-01-04 23:14:24,846: t15.2023.08.27 val PER: 0.2508
2026-01-04 23:14:24,846: t15.2023.09.01 val PER: 0.1282
2026-01-04 23:14:24,846: t15.2023.09.03 val PER: 0.2162
2026-01-04 23:14:24,846: t15.2023.09.24 val PER: 0.1663
2026-01-04 23:14:24,846: t15.2023.09.29 val PER: 0.1678
2026-01-04 23:14:24,847: t15.2023.10.01 val PER: 0.2147
2026-01-04 23:14:24,847: t15.2023.10.06 val PER: 0.1302
2026-01-04 23:14:24,847: t15.2023.10.08 val PER: 0.2936
2026-01-04 23:14:24,847: t15.2023.10.13 val PER: 0.2645
2026-01-04 23:14:24,847: t15.2023.10.15 val PER: 0.2123
2026-01-04 23:14:24,847: t15.2023.10.20 val PER: 0.2349
2026-01-04 23:14:24,847: t15.2023.10.22 val PER: 0.1748
2026-01-04 23:14:24,847: t15.2023.11.03 val PER: 0.2246
2026-01-04 23:14:24,847: t15.2023.11.04 val PER: 0.0580
2026-01-04 23:14:24,847: t15.2023.11.17 val PER: 0.0793
2026-01-04 23:14:24,847: t15.2023.11.19 val PER: 0.0818
2026-01-04 23:14:24,848: t15.2023.11.26 val PER: 0.2261
2026-01-04 23:14:24,848: t15.2023.12.03 val PER: 0.1670
2026-01-04 23:14:24,848: t15.2023.12.08 val PER: 0.1824
2026-01-04 23:14:24,848: t15.2023.12.10 val PER: 0.1459
2026-01-04 23:14:24,848: t15.2023.12.17 val PER: 0.2027
2026-01-04 23:14:24,848: t15.2023.12.29 val PER: 0.2114
2026-01-04 23:14:24,848: t15.2024.02.25 val PER: 0.1559
2026-01-04 23:14:24,848: t15.2024.03.08 val PER: 0.2802
2026-01-04 23:14:24,848: t15.2024.03.15 val PER: 0.2614
2026-01-04 23:14:24,848: t15.2024.03.17 val PER: 0.2085
2026-01-04 23:14:24,848: t15.2024.05.10 val PER: 0.2318
2026-01-04 23:14:24,848: t15.2024.06.14 val PER: 0.2082
2026-01-04 23:14:24,848: t15.2024.07.19 val PER: 0.3039
2026-01-04 23:14:24,849: t15.2024.07.21 val PER: 0.1641
2026-01-04 23:14:24,849: t15.2024.07.28 val PER: 0.1949
2026-01-04 23:14:24,849: t15.2025.01.10 val PER: 0.3719
2026-01-04 23:14:24,849: t15.2025.01.12 val PER: 0.2156
2026-01-04 23:14:24,849: t15.2025.03.14 val PER: 0.3757
2026-01-04 23:14:24,849: t15.2025.03.16 val PER: 0.2696
2026-01-04 23:14:24,849: t15.2025.03.30 val PER: 0.3713
2026-01-04 23:14:24,849: t15.2025.04.13 val PER: 0.2611
2026-01-04 23:14:25,117: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_6000
2026-01-04 23:14:43,225: Train batch 6200: loss: 15.74 grad norm: 57.76 time: 0.070
2026-01-04 23:15:01,594: Train batch 6400: loss: 18.67 grad norm: 65.66 time: 0.061
2026-01-04 23:15:10,556: Running test after training batch: 6500
2026-01-04 23:15:10,707: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:15:15,452: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 23:15:15,481: WER debug example
  GT : how does it keep the cost down
  PR : how des it keep the cost get
2026-01-04 23:15:17,050: Val batch 6500: PER (avg): 0.2028 CTC Loss (avg): 19.9258 WER(1gram): 54.57% (n=64) time: 6.494
2026-01-04 23:15:17,051: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 23:15:17,051: t15.2023.08.13 val PER: 0.1715
2026-01-04 23:15:17,051: t15.2023.08.18 val PER: 0.1500
2026-01-04 23:15:17,051: t15.2023.08.20 val PER: 0.1581
2026-01-04 23:15:17,051: t15.2023.08.25 val PER: 0.1099
2026-01-04 23:15:17,052: t15.2023.08.27 val PER: 0.2412
2026-01-04 23:15:17,052: t15.2023.09.01 val PER: 0.1161
2026-01-04 23:15:17,052: t15.2023.09.03 val PER: 0.1971
2026-01-04 23:15:17,052: t15.2023.09.24 val PER: 0.1663
2026-01-04 23:15:17,052: t15.2023.09.29 val PER: 0.1653
2026-01-04 23:15:17,052: t15.2023.10.01 val PER: 0.2186
2026-01-04 23:15:17,052: t15.2023.10.06 val PER: 0.1195
2026-01-04 23:15:17,052: t15.2023.10.08 val PER: 0.2923
2026-01-04 23:15:17,052: t15.2023.10.13 val PER: 0.2715
2026-01-04 23:15:17,052: t15.2023.10.15 val PER: 0.2050
2026-01-04 23:15:17,052: t15.2023.10.20 val PER: 0.1946
2026-01-04 23:15:17,053: t15.2023.10.22 val PER: 0.1548
2026-01-04 23:15:17,053: t15.2023.11.03 val PER: 0.2137
2026-01-04 23:15:17,053: t15.2023.11.04 val PER: 0.0478
2026-01-04 23:15:17,053: t15.2023.11.17 val PER: 0.0684
2026-01-04 23:15:17,053: t15.2023.11.19 val PER: 0.0719
2026-01-04 23:15:17,053: t15.2023.11.26 val PER: 0.2101
2026-01-04 23:15:17,053: t15.2023.12.03 val PER: 0.1702
2026-01-04 23:15:17,053: t15.2023.12.08 val PER: 0.1684
2026-01-04 23:15:17,053: t15.2023.12.10 val PER: 0.1445
2026-01-04 23:15:17,053: t15.2023.12.17 val PER: 0.1861
2026-01-04 23:15:17,053: t15.2023.12.29 val PER: 0.1977
2026-01-04 23:15:17,054: t15.2024.02.25 val PER: 0.1685
2026-01-04 23:15:17,054: t15.2024.03.08 val PER: 0.2945
2026-01-04 23:15:17,054: t15.2024.03.15 val PER: 0.2645
2026-01-04 23:15:17,054: t15.2024.03.17 val PER: 0.2078
2026-01-04 23:15:17,054: t15.2024.05.10 val PER: 0.2140
2026-01-04 23:15:17,054: t15.2024.06.14 val PER: 0.1987
2026-01-04 23:15:17,054: t15.2024.07.19 val PER: 0.2973
2026-01-04 23:15:17,054: t15.2024.07.21 val PER: 0.1434
2026-01-04 23:15:17,054: t15.2024.07.28 val PER: 0.1882
2026-01-04 23:15:17,054: t15.2025.01.10 val PER: 0.3636
2026-01-04 23:15:17,054: t15.2025.01.12 val PER: 0.2117
2026-01-04 23:15:17,054: t15.2025.03.14 val PER: 0.3831
2026-01-04 23:15:17,055: t15.2025.03.16 val PER: 0.2500
2026-01-04 23:15:17,055: t15.2025.03.30 val PER: 0.3448
2026-01-04 23:15:17,055: t15.2025.04.13 val PER: 0.2796
2026-01-04 23:15:17,322: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_6500
2026-01-04 23:15:25,989: Train batch 6600: loss: 11.94 grad norm: 50.31 time: 0.045
2026-01-04 23:15:44,208: Train batch 6800: loss: 15.35 grad norm: 57.12 time: 0.048
2026-01-04 23:16:02,527: Train batch 7000: loss: 17.16 grad norm: 64.39 time: 0.061
2026-01-04 23:16:02,528: Running test after training batch: 7000
2026-01-04 23:16:02,688: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:16:07,694: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-04 23:16:07,723: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-04 23:16:09,325: Val batch 7000: PER (avg): 0.1929 CTC Loss (avg): 19.2067 WER(1gram): 55.08% (n=64) time: 6.797
2026-01-04 23:16:09,325: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-04 23:16:09,326: t15.2023.08.13 val PER: 0.1528
2026-01-04 23:16:09,326: t15.2023.08.18 val PER: 0.1417
2026-01-04 23:16:09,326: t15.2023.08.20 val PER: 0.1509
2026-01-04 23:16:09,326: t15.2023.08.25 val PER: 0.1069
2026-01-04 23:16:09,326: t15.2023.08.27 val PER: 0.2170
2026-01-04 23:16:09,326: t15.2023.09.01 val PER: 0.1169
2026-01-04 23:16:09,326: t15.2023.09.03 val PER: 0.1793
2026-01-04 23:16:09,326: t15.2023.09.24 val PER: 0.1553
2026-01-04 23:16:09,326: t15.2023.09.29 val PER: 0.1710
2026-01-04 23:16:09,326: t15.2023.10.01 val PER: 0.2048
2026-01-04 23:16:09,326: t15.2023.10.06 val PER: 0.1152
2026-01-04 23:16:09,326: t15.2023.10.08 val PER: 0.2855
2026-01-04 23:16:09,326: t15.2023.10.13 val PER: 0.2583
2026-01-04 23:16:09,326: t15.2023.10.15 val PER: 0.1905
2026-01-04 23:16:09,327: t15.2023.10.20 val PER: 0.2047
2026-01-04 23:16:09,327: t15.2023.10.22 val PER: 0.1437
2026-01-04 23:16:09,327: t15.2023.11.03 val PER: 0.2069
2026-01-04 23:16:09,327: t15.2023.11.04 val PER: 0.0444
2026-01-04 23:16:09,327: t15.2023.11.17 val PER: 0.0700
2026-01-04 23:16:09,327: t15.2023.11.19 val PER: 0.0539
2026-01-04 23:16:09,327: t15.2023.11.26 val PER: 0.1935
2026-01-04 23:16:09,327: t15.2023.12.03 val PER: 0.1586
2026-01-04 23:16:09,327: t15.2023.12.08 val PER: 0.1551
2026-01-04 23:16:09,327: t15.2023.12.10 val PER: 0.1367
2026-01-04 23:16:09,327: t15.2023.12.17 val PER: 0.1757
2026-01-04 23:16:09,327: t15.2023.12.29 val PER: 0.1956
2026-01-04 23:16:09,328: t15.2024.02.25 val PER: 0.1531
2026-01-04 23:16:09,328: t15.2024.03.08 val PER: 0.2788
2026-01-04 23:16:09,328: t15.2024.03.15 val PER: 0.2370
2026-01-04 23:16:09,328: t15.2024.03.17 val PER: 0.1869
2026-01-04 23:16:09,328: t15.2024.05.10 val PER: 0.2021
2026-01-04 23:16:09,328: t15.2024.06.14 val PER: 0.2050
2026-01-04 23:16:09,328: t15.2024.07.19 val PER: 0.3032
2026-01-04 23:16:09,328: t15.2024.07.21 val PER: 0.1338
2026-01-04 23:16:09,328: t15.2024.07.28 val PER: 0.1779
2026-01-04 23:16:09,328: t15.2025.01.10 val PER: 0.3388
2026-01-04 23:16:09,328: t15.2025.01.12 val PER: 0.2071
2026-01-04 23:16:09,328: t15.2025.03.14 val PER: 0.3447
2026-01-04 23:16:09,328: t15.2025.03.16 val PER: 0.2369
2026-01-04 23:16:09,328: t15.2025.03.30 val PER: 0.3621
2026-01-04 23:16:09,328: t15.2025.04.13 val PER: 0.2639
2026-01-04 23:16:09,587: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_7000
2026-01-04 23:16:28,248: Train batch 7200: loss: 14.24 grad norm: 57.56 time: 0.078
2026-01-04 23:16:46,444: Train batch 7400: loss: 13.41 grad norm: 54.72 time: 0.076
2026-01-04 23:16:55,804: Running test after training batch: 7500
2026-01-04 23:16:56,006: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:17:00,742: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 23:17:00,772: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-04 23:17:02,395: Val batch 7500: PER (avg): 0.1865 CTC Loss (avg): 18.5632 WER(1gram): 55.84% (n=64) time: 6.591
2026-01-04 23:17:02,396: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 23:17:02,396: t15.2023.08.13 val PER: 0.1486
2026-01-04 23:17:02,396: t15.2023.08.18 val PER: 0.1333
2026-01-04 23:17:02,396: t15.2023.08.20 val PER: 0.1454
2026-01-04 23:17:02,396: t15.2023.08.25 val PER: 0.1084
2026-01-04 23:17:02,396: t15.2023.08.27 val PER: 0.2058
2026-01-04 23:17:02,396: t15.2023.09.01 val PER: 0.1088
2026-01-04 23:17:02,396: t15.2023.09.03 val PER: 0.1876
2026-01-04 23:17:02,396: t15.2023.09.24 val PER: 0.1468
2026-01-04 23:17:02,397: t15.2023.09.29 val PER: 0.1589
2026-01-04 23:17:02,397: t15.2023.10.01 val PER: 0.1935
2026-01-04 23:17:02,397: t15.2023.10.06 val PER: 0.1141
2026-01-04 23:17:02,397: t15.2023.10.08 val PER: 0.2788
2026-01-04 23:17:02,397: t15.2023.10.13 val PER: 0.2397
2026-01-04 23:17:02,397: t15.2023.10.15 val PER: 0.1826
2026-01-04 23:17:02,397: t15.2023.10.20 val PER: 0.1913
2026-01-04 23:17:02,397: t15.2023.10.22 val PER: 0.1503
2026-01-04 23:17:02,397: t15.2023.11.03 val PER: 0.1947
2026-01-04 23:17:02,397: t15.2023.11.04 val PER: 0.0375
2026-01-04 23:17:02,397: t15.2023.11.17 val PER: 0.0715
2026-01-04 23:17:02,397: t15.2023.11.19 val PER: 0.0599
2026-01-04 23:17:02,397: t15.2023.11.26 val PER: 0.1891
2026-01-04 23:17:02,398: t15.2023.12.03 val PER: 0.1597
2026-01-04 23:17:02,398: t15.2023.12.08 val PER: 0.1425
2026-01-04 23:17:02,398: t15.2023.12.10 val PER: 0.1156
2026-01-04 23:17:02,398: t15.2023.12.17 val PER: 0.1726
2026-01-04 23:17:02,398: t15.2023.12.29 val PER: 0.1839
2026-01-04 23:17:02,398: t15.2024.02.25 val PER: 0.1348
2026-01-04 23:17:02,398: t15.2024.03.08 val PER: 0.2731
2026-01-04 23:17:02,398: t15.2024.03.15 val PER: 0.2445
2026-01-04 23:17:02,398: t15.2024.03.17 val PER: 0.1813
2026-01-04 23:17:02,398: t15.2024.05.10 val PER: 0.1961
2026-01-04 23:17:02,398: t15.2024.06.14 val PER: 0.1987
2026-01-04 23:17:02,398: t15.2024.07.19 val PER: 0.2887
2026-01-04 23:17:02,398: t15.2024.07.21 val PER: 0.1379
2026-01-04 23:17:02,398: t15.2024.07.28 val PER: 0.1676
2026-01-04 23:17:02,398: t15.2025.01.10 val PER: 0.3457
2026-01-04 23:17:02,399: t15.2025.01.12 val PER: 0.1971
2026-01-04 23:17:02,399: t15.2025.03.14 val PER: 0.3669
2026-01-04 23:17:02,399: t15.2025.03.16 val PER: 0.2382
2026-01-04 23:17:02,399: t15.2025.03.30 val PER: 0.3379
2026-01-04 23:17:02,399: t15.2025.04.13 val PER: 0.2468
2026-01-04 23:17:02,655: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_7500
2026-01-04 23:17:11,835: Train batch 7600: loss: 16.34 grad norm: 60.83 time: 0.069
2026-01-04 23:17:30,157: Train batch 7800: loss: 14.16 grad norm: 58.84 time: 0.056
2026-01-04 23:17:48,749: Train batch 8000: loss: 10.86 grad norm: 51.17 time: 0.072
2026-01-04 23:17:48,749: Running test after training batch: 8000
2026-01-04 23:17:48,848: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:17:53,558: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 23:17:53,587: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost ned
2026-01-04 23:17:55,212: Val batch 8000: PER (avg): 0.1822 CTC Loss (avg): 18.0015 WER(1gram): 54.57% (n=64) time: 6.463
2026-01-04 23:17:55,213: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-04 23:17:55,213: t15.2023.08.13 val PER: 0.1424
2026-01-04 23:17:55,213: t15.2023.08.18 val PER: 0.1249
2026-01-04 23:17:55,213: t15.2023.08.20 val PER: 0.1469
2026-01-04 23:17:55,213: t15.2023.08.25 val PER: 0.1084
2026-01-04 23:17:55,213: t15.2023.08.27 val PER: 0.2106
2026-01-04 23:17:55,213: t15.2023.09.01 val PER: 0.1006
2026-01-04 23:17:55,213: t15.2023.09.03 val PER: 0.1888
2026-01-04 23:17:55,213: t15.2023.09.24 val PER: 0.1493
2026-01-04 23:17:55,213: t15.2023.09.29 val PER: 0.1493
2026-01-04 23:17:55,214: t15.2023.10.01 val PER: 0.1975
2026-01-04 23:17:55,214: t15.2023.10.06 val PER: 0.1130
2026-01-04 23:17:55,214: t15.2023.10.08 val PER: 0.2706
2026-01-04 23:17:55,214: t15.2023.10.13 val PER: 0.2389
2026-01-04 23:17:55,214: t15.2023.10.15 val PER: 0.1826
2026-01-04 23:17:55,214: t15.2023.10.20 val PER: 0.1779
2026-01-04 23:17:55,214: t15.2023.10.22 val PER: 0.1359
2026-01-04 23:17:55,214: t15.2023.11.03 val PER: 0.2083
2026-01-04 23:17:55,214: t15.2023.11.04 val PER: 0.0375
2026-01-04 23:17:55,214: t15.2023.11.17 val PER: 0.0669
2026-01-04 23:17:55,214: t15.2023.11.19 val PER: 0.0599
2026-01-04 23:17:55,214: t15.2023.11.26 val PER: 0.1768
2026-01-04 23:17:55,215: t15.2023.12.03 val PER: 0.1555
2026-01-04 23:17:55,215: t15.2023.12.08 val PER: 0.1471
2026-01-04 23:17:55,215: t15.2023.12.10 val PER: 0.1117
2026-01-04 23:17:55,215: t15.2023.12.17 val PER: 0.1840
2026-01-04 23:17:55,215: t15.2023.12.29 val PER: 0.1750
2026-01-04 23:17:55,215: t15.2024.02.25 val PER: 0.1503
2026-01-04 23:17:55,215: t15.2024.03.08 val PER: 0.2632
2026-01-04 23:17:55,215: t15.2024.03.15 val PER: 0.2364
2026-01-04 23:17:55,215: t15.2024.03.17 val PER: 0.1729
2026-01-04 23:17:55,215: t15.2024.05.10 val PER: 0.1932
2026-01-04 23:17:55,215: t15.2024.06.14 val PER: 0.2003
2026-01-04 23:17:55,215: t15.2024.07.19 val PER: 0.2755
2026-01-04 23:17:55,215: t15.2024.07.21 val PER: 0.1152
2026-01-04 23:17:55,215: t15.2024.07.28 val PER: 0.1581
2026-01-04 23:17:55,215: t15.2025.01.10 val PER: 0.3333
2026-01-04 23:17:55,215: t15.2025.01.12 val PER: 0.1832
2026-01-04 23:17:55,216: t15.2025.03.14 val PER: 0.3536
2026-01-04 23:17:55,216: t15.2025.03.16 val PER: 0.2225
2026-01-04 23:17:55,216: t15.2025.03.30 val PER: 0.3529
2026-01-04 23:17:55,216: t15.2025.04.13 val PER: 0.2454
2026-01-04 23:17:55,476: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_8000
2026-01-04 23:18:13,817: Train batch 8200: loss: 9.32 grad norm: 46.45 time: 0.054
2026-01-04 23:18:31,889: Train batch 8400: loss: 10.03 grad norm: 48.44 time: 0.064
2026-01-04 23:18:41,160: Running test after training batch: 8500
2026-01-04 23:18:41,251: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:18:45,923: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 23:18:45,952: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost nett
2026-01-04 23:18:47,593: Val batch 8500: PER (avg): 0.1778 CTC Loss (avg): 17.5089 WER(1gram): 48.98% (n=64) time: 6.433
2026-01-04 23:18:47,594: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 23:18:47,594: t15.2023.08.13 val PER: 0.1289
2026-01-04 23:18:47,594: t15.2023.08.18 val PER: 0.1299
2026-01-04 23:18:47,594: t15.2023.08.20 val PER: 0.1430
2026-01-04 23:18:47,594: t15.2023.08.25 val PER: 0.1054
2026-01-04 23:18:47,594: t15.2023.08.27 val PER: 0.2138
2026-01-04 23:18:47,594: t15.2023.09.01 val PER: 0.0966
2026-01-04 23:18:47,594: t15.2023.09.03 val PER: 0.1829
2026-01-04 23:18:47,594: t15.2023.09.24 val PER: 0.1493
2026-01-04 23:18:47,594: t15.2023.09.29 val PER: 0.1512
2026-01-04 23:18:47,594: t15.2023.10.01 val PER: 0.1995
2026-01-04 23:18:47,594: t15.2023.10.06 val PER: 0.1109
2026-01-04 23:18:47,595: t15.2023.10.08 val PER: 0.2598
2026-01-04 23:18:47,595: t15.2023.10.13 val PER: 0.2374
2026-01-04 23:18:47,595: t15.2023.10.15 val PER: 0.1819
2026-01-04 23:18:47,595: t15.2023.10.20 val PER: 0.1846
2026-01-04 23:18:47,595: t15.2023.10.22 val PER: 0.1403
2026-01-04 23:18:47,595: t15.2023.11.03 val PER: 0.1940
2026-01-04 23:18:47,595: t15.2023.11.04 val PER: 0.0580
2026-01-04 23:18:47,595: t15.2023.11.17 val PER: 0.0560
2026-01-04 23:18:47,595: t15.2023.11.19 val PER: 0.0459
2026-01-04 23:18:47,595: t15.2023.11.26 val PER: 0.1710
2026-01-04 23:18:47,595: t15.2023.12.03 val PER: 0.1439
2026-01-04 23:18:47,595: t15.2023.12.08 val PER: 0.1405
2026-01-04 23:18:47,595: t15.2023.12.10 val PER: 0.1183
2026-01-04 23:18:47,595: t15.2023.12.17 val PER: 0.1518
2026-01-04 23:18:47,596: t15.2023.12.29 val PER: 0.1620
2026-01-04 23:18:47,596: t15.2024.02.25 val PER: 0.1475
2026-01-04 23:18:47,596: t15.2024.03.08 val PER: 0.2603
2026-01-04 23:18:47,596: t15.2024.03.15 val PER: 0.2276
2026-01-04 23:18:47,596: t15.2024.03.17 val PER: 0.1646
2026-01-04 23:18:47,596: t15.2024.05.10 val PER: 0.1902
2026-01-04 23:18:47,596: t15.2024.06.14 val PER: 0.1956
2026-01-04 23:18:47,596: t15.2024.07.19 val PER: 0.2736
2026-01-04 23:18:47,596: t15.2024.07.21 val PER: 0.1221
2026-01-04 23:18:47,596: t15.2024.07.28 val PER: 0.1647
2026-01-04 23:18:47,596: t15.2025.01.10 val PER: 0.3223
2026-01-04 23:18:47,596: t15.2025.01.12 val PER: 0.1809
2026-01-04 23:18:47,596: t15.2025.03.14 val PER: 0.3521
2026-01-04 23:18:47,596: t15.2025.03.16 val PER: 0.2081
2026-01-04 23:18:47,596: t15.2025.03.30 val PER: 0.3391
2026-01-04 23:18:47,597: t15.2025.04.13 val PER: 0.2397
2026-01-04 23:18:47,598: New best val WER(1gram) 54.57% --> 48.98%
2026-01-04 23:18:47,598: Checkpointing model
2026-01-04 23:18:48,225: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:18:48,494: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_8500
2026-01-04 23:18:57,397: Train batch 8600: loss: 15.60 grad norm: 61.32 time: 0.054
2026-01-04 23:19:15,261: Train batch 8800: loss: 15.21 grad norm: 61.24 time: 0.060
2026-01-04 23:19:33,160: Train batch 9000: loss: 15.45 grad norm: 63.01 time: 0.072
2026-01-04 23:19:33,160: Running test after training batch: 9000
2026-01-04 23:19:33,277: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:19:38,121: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 23:19:38,151: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nett
2026-01-04 23:19:39,816: Val batch 9000: PER (avg): 0.1747 CTC Loss (avg): 17.3397 WER(1gram): 50.76% (n=64) time: 6.656
2026-01-04 23:19:39,816: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 23:19:39,817: t15.2023.08.13 val PER: 0.1268
2026-01-04 23:19:39,817: t15.2023.08.18 val PER: 0.1224
2026-01-04 23:19:39,817: t15.2023.08.20 val PER: 0.1303
2026-01-04 23:19:39,817: t15.2023.08.25 val PER: 0.0994
2026-01-04 23:19:39,817: t15.2023.08.27 val PER: 0.2138
2026-01-04 23:19:39,817: t15.2023.09.01 val PER: 0.0893
2026-01-04 23:19:39,817: t15.2023.09.03 val PER: 0.1888
2026-01-04 23:19:39,817: t15.2023.09.24 val PER: 0.1481
2026-01-04 23:19:39,817: t15.2023.09.29 val PER: 0.1455
2026-01-04 23:19:39,817: t15.2023.10.01 val PER: 0.1889
2026-01-04 23:19:39,817: t15.2023.10.06 val PER: 0.0980
2026-01-04 23:19:39,817: t15.2023.10.08 val PER: 0.2625
2026-01-04 23:19:39,817: t15.2023.10.13 val PER: 0.2327
2026-01-04 23:19:39,817: t15.2023.10.15 val PER: 0.1793
2026-01-04 23:19:39,818: t15.2023.10.20 val PER: 0.2081
2026-01-04 23:19:39,818: t15.2023.10.22 val PER: 0.1314
2026-01-04 23:19:39,818: t15.2023.11.03 val PER: 0.1981
2026-01-04 23:19:39,818: t15.2023.11.04 val PER: 0.0341
2026-01-04 23:19:39,818: t15.2023.11.17 val PER: 0.0560
2026-01-04 23:19:39,818: t15.2023.11.19 val PER: 0.0439
2026-01-04 23:19:39,818: t15.2023.11.26 val PER: 0.1696
2026-01-04 23:19:39,818: t15.2023.12.03 val PER: 0.1471
2026-01-04 23:19:39,818: t15.2023.12.08 val PER: 0.1358
2026-01-04 23:19:39,818: t15.2023.12.10 val PER: 0.1078
2026-01-04 23:19:39,818: t15.2023.12.17 val PER: 0.1611
2026-01-04 23:19:39,818: t15.2023.12.29 val PER: 0.1654
2026-01-04 23:19:39,818: t15.2024.02.25 val PER: 0.1461
2026-01-04 23:19:39,818: t15.2024.03.08 val PER: 0.2575
2026-01-04 23:19:39,819: t15.2024.03.15 val PER: 0.2301
2026-01-04 23:19:39,819: t15.2024.03.17 val PER: 0.1702
2026-01-04 23:19:39,819: t15.2024.05.10 val PER: 0.1887
2026-01-04 23:19:39,819: t15.2024.06.14 val PER: 0.1877
2026-01-04 23:19:39,819: t15.2024.07.19 val PER: 0.2709
2026-01-04 23:19:39,819: t15.2024.07.21 val PER: 0.1110
2026-01-04 23:19:39,819: t15.2024.07.28 val PER: 0.1485
2026-01-04 23:19:39,819: t15.2025.01.10 val PER: 0.3182
2026-01-04 23:19:39,819: t15.2025.01.12 val PER: 0.1778
2026-01-04 23:19:39,819: t15.2025.03.14 val PER: 0.3624
2026-01-04 23:19:39,819: t15.2025.03.16 val PER: 0.2199
2026-01-04 23:19:39,819: t15.2025.03.30 val PER: 0.3287
2026-01-04 23:19:39,819: t15.2025.04.13 val PER: 0.2368
2026-01-04 23:19:40,093: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_9000
2026-01-04 23:19:58,215: Train batch 9200: loss: 10.88 grad norm: 50.42 time: 0.056
2026-01-04 23:20:16,010: Train batch 9400: loss: 7.37 grad norm: 45.81 time: 0.068
2026-01-04 23:20:25,023: Running test after training batch: 9500
2026-01-04 23:20:25,197: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:20:29,926: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 23:20:29,955: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost nett
2026-01-04 23:20:31,628: Val batch 9500: PER (avg): 0.1721 CTC Loss (avg): 17.0809 WER(1gram): 48.22% (n=64) time: 6.605
2026-01-04 23:20:31,629: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 23:20:31,629: t15.2023.08.13 val PER: 0.1299
2026-01-04 23:20:31,629: t15.2023.08.18 val PER: 0.1232
2026-01-04 23:20:31,629: t15.2023.08.20 val PER: 0.1390
2026-01-04 23:20:31,629: t15.2023.08.25 val PER: 0.1024
2026-01-04 23:20:31,629: t15.2023.08.27 val PER: 0.2058
2026-01-04 23:20:31,629: t15.2023.09.01 val PER: 0.0950
2026-01-04 23:20:31,629: t15.2023.09.03 val PER: 0.1853
2026-01-04 23:20:31,629: t15.2023.09.24 val PER: 0.1481
2026-01-04 23:20:31,630: t15.2023.09.29 val PER: 0.1404
2026-01-04 23:20:31,630: t15.2023.10.01 val PER: 0.1929
2026-01-04 23:20:31,630: t15.2023.10.06 val PER: 0.1066
2026-01-04 23:20:31,630: t15.2023.10.08 val PER: 0.2585
2026-01-04 23:20:31,630: t15.2023.10.13 val PER: 0.2304
2026-01-04 23:20:31,630: t15.2023.10.15 val PER: 0.1819
2026-01-04 23:20:31,630: t15.2023.10.20 val PER: 0.1879
2026-01-04 23:20:31,630: t15.2023.10.22 val PER: 0.1225
2026-01-04 23:20:31,630: t15.2023.11.03 val PER: 0.1886
2026-01-04 23:20:31,630: t15.2023.11.04 val PER: 0.0307
2026-01-04 23:20:31,630: t15.2023.11.17 val PER: 0.0638
2026-01-04 23:20:31,630: t15.2023.11.19 val PER: 0.0499
2026-01-04 23:20:31,630: t15.2023.11.26 val PER: 0.1594
2026-01-04 23:20:31,630: t15.2023.12.03 val PER: 0.1408
2026-01-04 23:20:31,630: t15.2023.12.08 val PER: 0.1391
2026-01-04 23:20:31,631: t15.2023.12.10 val PER: 0.1091
2026-01-04 23:20:31,631: t15.2023.12.17 val PER: 0.1466
2026-01-04 23:20:31,631: t15.2023.12.29 val PER: 0.1544
2026-01-04 23:20:31,631: t15.2024.02.25 val PER: 0.1320
2026-01-04 23:20:31,631: t15.2024.03.08 val PER: 0.2518
2026-01-04 23:20:31,631: t15.2024.03.15 val PER: 0.2276
2026-01-04 23:20:31,631: t15.2024.03.17 val PER: 0.1590
2026-01-04 23:20:31,631: t15.2024.05.10 val PER: 0.1813
2026-01-04 23:20:31,631: t15.2024.06.14 val PER: 0.1751
2026-01-04 23:20:31,631: t15.2024.07.19 val PER: 0.2538
2026-01-04 23:20:31,631: t15.2024.07.21 val PER: 0.1145
2026-01-04 23:20:31,631: t15.2024.07.28 val PER: 0.1544
2026-01-04 23:20:31,632: t15.2025.01.10 val PER: 0.3168
2026-01-04 23:20:31,632: t15.2025.01.12 val PER: 0.1809
2026-01-04 23:20:31,632: t15.2025.03.14 val PER: 0.3728
2026-01-04 23:20:31,632: t15.2025.03.16 val PER: 0.2029
2026-01-04 23:20:31,632: t15.2025.03.30 val PER: 0.3322
2026-01-04 23:20:31,632: t15.2025.04.13 val PER: 0.2297
2026-01-04 23:20:31,633: New best val WER(1gram) 48.98% --> 48.22%
2026-01-04 23:20:31,633: Checkpointing model
2026-01-04 23:20:32,271: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:20:32,568: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_9500
2026-01-04 23:20:41,631: Train batch 9600: loss: 8.10 grad norm: 45.58 time: 0.073
2026-01-04 23:21:00,066: Train batch 9800: loss: 12.44 grad norm: 56.84 time: 0.063
2026-01-04 23:21:18,957: Train batch 10000: loss: 5.32 grad norm: 38.01 time: 0.060
2026-01-04 23:21:18,957: Running test after training batch: 10000
2026-01-04 23:21:19,065: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:21:23,955: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 23:21:23,984: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sette
2026-01-04 23:21:25,664: Val batch 10000: PER (avg): 0.1684 CTC Loss (avg): 16.9280 WER(1gram): 50.25% (n=64) time: 6.707
2026-01-04 23:21:25,664: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 23:21:25,664: t15.2023.08.13 val PER: 0.1351
2026-01-04 23:21:25,665: t15.2023.08.18 val PER: 0.1165
2026-01-04 23:21:25,665: t15.2023.08.20 val PER: 0.1311
2026-01-04 23:21:25,665: t15.2023.08.25 val PER: 0.1069
2026-01-04 23:21:25,665: t15.2023.08.27 val PER: 0.2058
2026-01-04 23:21:25,669: t15.2023.09.01 val PER: 0.0909
2026-01-04 23:21:25,669: t15.2023.09.03 val PER: 0.1758
2026-01-04 23:21:25,669: t15.2023.09.24 val PER: 0.1493
2026-01-04 23:21:25,669: t15.2023.09.29 val PER: 0.1474
2026-01-04 23:21:25,669: t15.2023.10.01 val PER: 0.1823
2026-01-04 23:21:25,669: t15.2023.10.06 val PER: 0.1023
2026-01-04 23:21:25,669: t15.2023.10.08 val PER: 0.2544
2026-01-04 23:21:25,669: t15.2023.10.13 val PER: 0.2265
2026-01-04 23:21:25,669: t15.2023.10.15 val PER: 0.1727
2026-01-04 23:21:25,669: t15.2023.10.20 val PER: 0.1913
2026-01-04 23:21:25,669: t15.2023.10.22 val PER: 0.1269
2026-01-04 23:21:25,670: t15.2023.11.03 val PER: 0.1845
2026-01-04 23:21:25,670: t15.2023.11.04 val PER: 0.0375
2026-01-04 23:21:25,670: t15.2023.11.17 val PER: 0.0498
2026-01-04 23:21:25,670: t15.2023.11.19 val PER: 0.0399
2026-01-04 23:21:25,670: t15.2023.11.26 val PER: 0.1486
2026-01-04 23:21:25,670: t15.2023.12.03 val PER: 0.1387
2026-01-04 23:21:25,670: t15.2023.12.08 val PER: 0.1258
2026-01-04 23:21:25,670: t15.2023.12.10 val PER: 0.1038
2026-01-04 23:21:25,670: t15.2023.12.17 val PER: 0.1466
2026-01-04 23:21:25,670: t15.2023.12.29 val PER: 0.1482
2026-01-04 23:21:25,670: t15.2024.02.25 val PER: 0.1461
2026-01-04 23:21:25,671: t15.2024.03.08 val PER: 0.2404
2026-01-04 23:21:25,671: t15.2024.03.15 val PER: 0.2276
2026-01-04 23:21:25,671: t15.2024.03.17 val PER: 0.1527
2026-01-04 23:21:25,671: t15.2024.05.10 val PER: 0.1738
2026-01-04 23:21:25,671: t15.2024.06.14 val PER: 0.1798
2026-01-04 23:21:25,671: t15.2024.07.19 val PER: 0.2577
2026-01-04 23:21:25,671: t15.2024.07.21 val PER: 0.1138
2026-01-04 23:21:25,671: t15.2024.07.28 val PER: 0.1537
2026-01-04 23:21:25,671: t15.2025.01.10 val PER: 0.3044
2026-01-04 23:21:25,671: t15.2025.01.12 val PER: 0.1740
2026-01-04 23:21:25,672: t15.2025.03.14 val PER: 0.3506
2026-01-04 23:21:25,672: t15.2025.03.16 val PER: 0.2238
2026-01-04 23:21:25,672: t15.2025.03.30 val PER: 0.3161
2026-01-04 23:21:25,672: t15.2025.04.13 val PER: 0.2311
2026-01-04 23:21:25,958: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_10000
2026-01-04 23:21:44,359: Train batch 10200: loss: 6.31 grad norm: 38.83 time: 0.050
2026-01-04 23:22:03,008: Train batch 10400: loss: 9.07 grad norm: 57.96 time: 0.072
2026-01-04 23:22:12,274: Running test after training batch: 10500
2026-01-04 23:22:12,429: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:22:17,143: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:22:17,172: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-04 23:22:18,856: Val batch 10500: PER (avg): 0.1669 CTC Loss (avg): 16.6906 WER(1gram): 50.25% (n=64) time: 6.581
2026-01-04 23:22:18,856: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 23:22:18,856: t15.2023.08.13 val PER: 0.1206
2026-01-04 23:22:18,856: t15.2023.08.18 val PER: 0.1207
2026-01-04 23:22:18,856: t15.2023.08.20 val PER: 0.1231
2026-01-04 23:22:18,856: t15.2023.08.25 val PER: 0.1054
2026-01-04 23:22:18,856: t15.2023.08.27 val PER: 0.2106
2026-01-04 23:22:18,857: t15.2023.09.01 val PER: 0.0974
2026-01-04 23:22:18,857: t15.2023.09.03 val PER: 0.1770
2026-01-04 23:22:18,857: t15.2023.09.24 val PER: 0.1529
2026-01-04 23:22:18,857: t15.2023.09.29 val PER: 0.1455
2026-01-04 23:22:18,857: t15.2023.10.01 val PER: 0.1863
2026-01-04 23:22:18,857: t15.2023.10.06 val PER: 0.0904
2026-01-04 23:22:18,857: t15.2023.10.08 val PER: 0.2517
2026-01-04 23:22:18,857: t15.2023.10.13 val PER: 0.2211
2026-01-04 23:22:18,857: t15.2023.10.15 val PER: 0.1786
2026-01-04 23:22:18,857: t15.2023.10.20 val PER: 0.1913
2026-01-04 23:22:18,857: t15.2023.10.22 val PER: 0.1203
2026-01-04 23:22:18,857: t15.2023.11.03 val PER: 0.1934
2026-01-04 23:22:18,857: t15.2023.11.04 val PER: 0.0444
2026-01-04 23:22:18,857: t15.2023.11.17 val PER: 0.0560
2026-01-04 23:22:18,857: t15.2023.11.19 val PER: 0.0579
2026-01-04 23:22:18,857: t15.2023.11.26 val PER: 0.1442
2026-01-04 23:22:18,858: t15.2023.12.03 val PER: 0.1292
2026-01-04 23:22:18,858: t15.2023.12.08 val PER: 0.1205
2026-01-04 23:22:18,858: t15.2023.12.10 val PER: 0.0972
2026-01-04 23:22:18,858: t15.2023.12.17 val PER: 0.1372
2026-01-04 23:22:18,858: t15.2023.12.29 val PER: 0.1544
2026-01-04 23:22:18,858: t15.2024.02.25 val PER: 0.1222
2026-01-04 23:22:18,858: t15.2024.03.08 val PER: 0.2489
2026-01-04 23:22:18,858: t15.2024.03.15 val PER: 0.2233
2026-01-04 23:22:18,858: t15.2024.03.17 val PER: 0.1541
2026-01-04 23:22:18,858: t15.2024.05.10 val PER: 0.1768
2026-01-04 23:22:18,859: t15.2024.06.14 val PER: 0.1751
2026-01-04 23:22:18,859: t15.2024.07.19 val PER: 0.2630
2026-01-04 23:22:18,859: t15.2024.07.21 val PER: 0.1138
2026-01-04 23:22:18,859: t15.2024.07.28 val PER: 0.1390
2026-01-04 23:22:18,859: t15.2025.01.10 val PER: 0.3113
2026-01-04 23:22:18,859: t15.2025.01.12 val PER: 0.1632
2026-01-04 23:22:18,859: t15.2025.03.14 val PER: 0.3595
2026-01-04 23:22:18,859: t15.2025.03.16 val PER: 0.1990
2026-01-04 23:22:18,859: t15.2025.03.30 val PER: 0.3207
2026-01-04 23:22:18,859: t15.2025.04.13 val PER: 0.2368
2026-01-04 23:22:19,142: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_10500
2026-01-04 23:22:28,618: Train batch 10600: loss: 8.85 grad norm: 57.01 time: 0.072
2026-01-04 23:22:47,040: Train batch 10800: loss: 14.84 grad norm: 75.85 time: 0.064
2026-01-04 23:23:05,592: Train batch 11000: loss: 14.08 grad norm: 62.34 time: 0.057
2026-01-04 23:23:05,592: Running test after training batch: 11000
2026-01-04 23:23:05,696: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:23:10,716: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:23:10,746: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 23:23:12,408: Val batch 11000: PER (avg): 0.1624 CTC Loss (avg): 16.4326 WER(1gram): 49.49% (n=64) time: 6.816
2026-01-04 23:23:12,408: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 23:23:12,409: t15.2023.08.13 val PER: 0.1227
2026-01-04 23:23:12,409: t15.2023.08.18 val PER: 0.1182
2026-01-04 23:23:12,409: t15.2023.08.20 val PER: 0.1239
2026-01-04 23:23:12,409: t15.2023.08.25 val PER: 0.0858
2026-01-04 23:23:12,409: t15.2023.08.27 val PER: 0.1977
2026-01-04 23:23:12,409: t15.2023.09.01 val PER: 0.0836
2026-01-04 23:23:12,410: t15.2023.09.03 val PER: 0.1805
2026-01-04 23:23:12,410: t15.2023.09.24 val PER: 0.1420
2026-01-04 23:23:12,410: t15.2023.09.29 val PER: 0.1378
2026-01-04 23:23:12,410: t15.2023.10.01 val PER: 0.1876
2026-01-04 23:23:12,410: t15.2023.10.06 val PER: 0.0840
2026-01-04 23:23:12,410: t15.2023.10.08 val PER: 0.2544
2026-01-04 23:23:12,410: t15.2023.10.13 val PER: 0.2157
2026-01-04 23:23:12,410: t15.2023.10.15 val PER: 0.1707
2026-01-04 23:23:12,410: t15.2023.10.20 val PER: 0.1980
2026-01-04 23:23:12,410: t15.2023.10.22 val PER: 0.1225
2026-01-04 23:23:12,410: t15.2023.11.03 val PER: 0.1872
2026-01-04 23:23:12,410: t15.2023.11.04 val PER: 0.0444
2026-01-04 23:23:12,411: t15.2023.11.17 val PER: 0.0544
2026-01-04 23:23:12,411: t15.2023.11.19 val PER: 0.0379
2026-01-04 23:23:12,411: t15.2023.11.26 val PER: 0.1420
2026-01-04 23:23:12,411: t15.2023.12.03 val PER: 0.1261
2026-01-04 23:23:12,411: t15.2023.12.08 val PER: 0.1039
2026-01-04 23:23:12,411: t15.2023.12.10 val PER: 0.1025
2026-01-04 23:23:12,411: t15.2023.12.17 val PER: 0.1435
2026-01-04 23:23:12,411: t15.2023.12.29 val PER: 0.1489
2026-01-04 23:23:12,411: t15.2024.02.25 val PER: 0.1320
2026-01-04 23:23:12,411: t15.2024.03.08 val PER: 0.2248
2026-01-04 23:23:12,411: t15.2024.03.15 val PER: 0.2283
2026-01-04 23:23:12,411: t15.2024.03.17 val PER: 0.1527
2026-01-04 23:23:12,412: t15.2024.05.10 val PER: 0.1738
2026-01-04 23:23:12,412: t15.2024.06.14 val PER: 0.1672
2026-01-04 23:23:12,412: t15.2024.07.19 val PER: 0.2531
2026-01-04 23:23:12,412: t15.2024.07.21 val PER: 0.1014
2026-01-04 23:23:12,412: t15.2024.07.28 val PER: 0.1412
2026-01-04 23:23:12,412: t15.2025.01.10 val PER: 0.2948
2026-01-04 23:23:12,412: t15.2025.01.12 val PER: 0.1647
2026-01-04 23:23:12,412: t15.2025.03.14 val PER: 0.3550
2026-01-04 23:23:12,412: t15.2025.03.16 val PER: 0.1990
2026-01-04 23:23:12,412: t15.2025.03.30 val PER: 0.3149
2026-01-04 23:23:12,413: t15.2025.04.13 val PER: 0.2282
2026-01-04 23:23:12,695: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_11000
2026-01-04 23:23:31,100: Train batch 11200: loss: 10.73 grad norm: 51.40 time: 0.071
2026-01-04 23:23:49,522: Train batch 11400: loss: 9.57 grad norm: 55.03 time: 0.058
2026-01-04 23:23:58,800: Running test after training batch: 11500
2026-01-04 23:23:58,960: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:24:03,654: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 23:24:03,686: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost nett
2026-01-04 23:24:05,401: Val batch 11500: PER (avg): 0.1600 CTC Loss (avg): 16.3803 WER(1gram): 49.49% (n=64) time: 6.601
2026-01-04 23:24:05,402: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 23:24:05,402: t15.2023.08.13 val PER: 0.1185
2026-01-04 23:24:05,402: t15.2023.08.18 val PER: 0.1106
2026-01-04 23:24:05,402: t15.2023.08.20 val PER: 0.1191
2026-01-04 23:24:05,402: t15.2023.08.25 val PER: 0.0934
2026-01-04 23:24:05,402: t15.2023.08.27 val PER: 0.2010
2026-01-04 23:24:05,402: t15.2023.09.01 val PER: 0.0933
2026-01-04 23:24:05,402: t15.2023.09.03 val PER: 0.1710
2026-01-04 23:24:05,402: t15.2023.09.24 val PER: 0.1359
2026-01-04 23:24:05,402: t15.2023.09.29 val PER: 0.1353
2026-01-04 23:24:05,403: t15.2023.10.01 val PER: 0.1731
2026-01-04 23:24:05,403: t15.2023.10.06 val PER: 0.0883
2026-01-04 23:24:05,403: t15.2023.10.08 val PER: 0.2476
2026-01-04 23:24:05,403: t15.2023.10.13 val PER: 0.2118
2026-01-04 23:24:05,403: t15.2023.10.15 val PER: 0.1615
2026-01-04 23:24:05,403: t15.2023.10.20 val PER: 0.2013
2026-01-04 23:24:05,403: t15.2023.10.22 val PER: 0.1180
2026-01-04 23:24:05,403: t15.2023.11.03 val PER: 0.1852
2026-01-04 23:24:05,403: t15.2023.11.04 val PER: 0.0239
2026-01-04 23:24:05,404: t15.2023.11.17 val PER: 0.0451
2026-01-04 23:24:05,404: t15.2023.11.19 val PER: 0.0439
2026-01-04 23:24:05,404: t15.2023.11.26 val PER: 0.1312
2026-01-04 23:24:05,404: t15.2023.12.03 val PER: 0.1197
2026-01-04 23:24:05,404: t15.2023.12.08 val PER: 0.1165
2026-01-04 23:24:05,404: t15.2023.12.10 val PER: 0.1012
2026-01-04 23:24:05,404: t15.2023.12.17 val PER: 0.1403
2026-01-04 23:24:05,404: t15.2023.12.29 val PER: 0.1373
2026-01-04 23:24:05,404: t15.2024.02.25 val PER: 0.1236
2026-01-04 23:24:05,404: t15.2024.03.08 val PER: 0.2361
2026-01-04 23:24:05,404: t15.2024.03.15 val PER: 0.2133
2026-01-04 23:24:05,404: t15.2024.03.17 val PER: 0.1450
2026-01-04 23:24:05,404: t15.2024.05.10 val PER: 0.1738
2026-01-04 23:24:05,404: t15.2024.06.14 val PER: 0.1909
2026-01-04 23:24:05,404: t15.2024.07.19 val PER: 0.2492
2026-01-04 23:24:05,404: t15.2024.07.21 val PER: 0.1034
2026-01-04 23:24:05,405: t15.2024.07.28 val PER: 0.1456
2026-01-04 23:24:05,405: t15.2025.01.10 val PER: 0.3072
2026-01-04 23:24:05,405: t15.2025.01.12 val PER: 0.1724
2026-01-04 23:24:05,405: t15.2025.03.14 val PER: 0.3580
2026-01-04 23:24:05,405: t15.2025.03.16 val PER: 0.1963
2026-01-04 23:24:05,405: t15.2025.03.30 val PER: 0.3103
2026-01-04 23:24:05,405: t15.2025.04.13 val PER: 0.2211
2026-01-04 23:24:05,710: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_11500
2026-01-04 23:24:14,359: Train batch 11600: loss: 10.70 grad norm: 45.92 time: 0.061
2026-01-04 23:24:32,146: Train batch 11800: loss: 6.64 grad norm: 42.50 time: 0.046
2026-01-04 23:24:50,036: Train batch 12000: loss: 13.49 grad norm: 53.41 time: 0.072
2026-01-04 23:24:50,037: Running test after training batch: 12000
2026-01-04 23:24:50,138: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:24:55,107: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 23:24:55,138: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sette
2026-01-04 23:24:56,835: Val batch 12000: PER (avg): 0.1577 CTC Loss (avg): 16.1395 WER(1gram): 48.73% (n=64) time: 6.798
2026-01-04 23:24:56,835: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 23:24:56,835: t15.2023.08.13 val PER: 0.1154
2026-01-04 23:24:56,835: t15.2023.08.18 val PER: 0.1039
2026-01-04 23:24:56,835: t15.2023.08.20 val PER: 0.1152
2026-01-04 23:24:56,835: t15.2023.08.25 val PER: 0.0994
2026-01-04 23:24:56,836: t15.2023.08.27 val PER: 0.1881
2026-01-04 23:24:56,836: t15.2023.09.01 val PER: 0.0917
2026-01-04 23:24:56,836: t15.2023.09.03 val PER: 0.1544
2026-01-04 23:24:56,836: t15.2023.09.24 val PER: 0.1371
2026-01-04 23:24:56,836: t15.2023.09.29 val PER: 0.1378
2026-01-04 23:24:56,836: t15.2023.10.01 val PER: 0.1797
2026-01-04 23:24:56,836: t15.2023.10.06 val PER: 0.0893
2026-01-04 23:24:56,836: t15.2023.10.08 val PER: 0.2436
2026-01-04 23:24:56,836: t15.2023.10.13 val PER: 0.2196
2026-01-04 23:24:56,836: t15.2023.10.15 val PER: 0.1635
2026-01-04 23:24:56,837: t15.2023.10.20 val PER: 0.2047
2026-01-04 23:24:56,837: t15.2023.10.22 val PER: 0.1225
2026-01-04 23:24:56,837: t15.2023.11.03 val PER: 0.1777
2026-01-04 23:24:56,837: t15.2023.11.04 val PER: 0.0410
2026-01-04 23:24:56,837: t15.2023.11.17 val PER: 0.0435
2026-01-04 23:24:56,837: t15.2023.11.19 val PER: 0.0379
2026-01-04 23:24:56,837: t15.2023.11.26 val PER: 0.1326
2026-01-04 23:24:56,837: t15.2023.12.03 val PER: 0.1155
2026-01-04 23:24:56,837: t15.2023.12.08 val PER: 0.1079
2026-01-04 23:24:56,838: t15.2023.12.10 val PER: 0.1012
2026-01-04 23:24:56,838: t15.2023.12.17 val PER: 0.1362
2026-01-04 23:24:56,838: t15.2023.12.29 val PER: 0.1332
2026-01-04 23:24:56,838: t15.2024.02.25 val PER: 0.1264
2026-01-04 23:24:56,838: t15.2024.03.08 val PER: 0.2376
2026-01-04 23:24:56,838: t15.2024.03.15 val PER: 0.2133
2026-01-04 23:24:56,838: t15.2024.03.17 val PER: 0.1360
2026-01-04 23:24:56,838: t15.2024.05.10 val PER: 0.1694
2026-01-04 23:24:56,838: t15.2024.06.14 val PER: 0.1767
2026-01-04 23:24:56,838: t15.2024.07.19 val PER: 0.2472
2026-01-04 23:24:56,838: t15.2024.07.21 val PER: 0.1055
2026-01-04 23:24:56,838: t15.2024.07.28 val PER: 0.1441
2026-01-04 23:24:56,839: t15.2025.01.10 val PER: 0.2961
2026-01-04 23:24:56,839: t15.2025.01.12 val PER: 0.1594
2026-01-04 23:24:56,839: t15.2025.03.14 val PER: 0.3580
2026-01-04 23:24:56,839: t15.2025.03.16 val PER: 0.1990
2026-01-04 23:24:56,839: t15.2025.03.30 val PER: 0.3023
2026-01-04 23:24:56,839: t15.2025.04.13 val PER: 0.2211
2026-01-04 23:24:57,114: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_12000
2026-01-04 23:25:15,538: Train batch 12200: loss: 5.49 grad norm: 38.20 time: 0.064
2026-01-04 23:25:33,775: Train batch 12400: loss: 4.96 grad norm: 34.96 time: 0.040
2026-01-04 23:25:42,999: Running test after training batch: 12500
2026-01-04 23:25:43,190: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:25:47,878: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 23:25:47,910: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 23:25:49,673: Val batch 12500: PER (avg): 0.1556 CTC Loss (avg): 15.9261 WER(1gram): 47.72% (n=64) time: 6.673
2026-01-04 23:25:49,673: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 23:25:49,674: t15.2023.08.13 val PER: 0.1268
2026-01-04 23:25:49,674: t15.2023.08.18 val PER: 0.1081
2026-01-04 23:25:49,674: t15.2023.08.20 val PER: 0.1128
2026-01-04 23:25:49,674: t15.2023.08.25 val PER: 0.0919
2026-01-04 23:25:49,674: t15.2023.08.27 val PER: 0.2026
2026-01-04 23:25:49,674: t15.2023.09.01 val PER: 0.0852
2026-01-04 23:25:49,674: t15.2023.09.03 val PER: 0.1686
2026-01-04 23:25:49,674: t15.2023.09.24 val PER: 0.1347
2026-01-04 23:25:49,674: t15.2023.09.29 val PER: 0.1359
2026-01-04 23:25:49,674: t15.2023.10.01 val PER: 0.1684
2026-01-04 23:25:49,674: t15.2023.10.06 val PER: 0.0850
2026-01-04 23:25:49,674: t15.2023.10.08 val PER: 0.2585
2026-01-04 23:25:49,674: t15.2023.10.13 val PER: 0.2056
2026-01-04 23:25:49,674: t15.2023.10.15 val PER: 0.1595
2026-01-04 23:25:49,675: t15.2023.10.20 val PER: 0.1946
2026-01-04 23:25:49,675: t15.2023.10.22 val PER: 0.1136
2026-01-04 23:25:49,675: t15.2023.11.03 val PER: 0.1791
2026-01-04 23:25:49,675: t15.2023.11.04 val PER: 0.0410
2026-01-04 23:25:49,675: t15.2023.11.17 val PER: 0.0404
2026-01-04 23:25:49,675: t15.2023.11.19 val PER: 0.0379
2026-01-04 23:25:49,675: t15.2023.11.26 val PER: 0.1239
2026-01-04 23:25:49,675: t15.2023.12.03 val PER: 0.1218
2026-01-04 23:25:49,675: t15.2023.12.08 val PER: 0.1005
2026-01-04 23:25:49,675: t15.2023.12.10 val PER: 0.0946
2026-01-04 23:25:49,675: t15.2023.12.17 val PER: 0.1445
2026-01-04 23:25:49,675: t15.2023.12.29 val PER: 0.1332
2026-01-04 23:25:49,675: t15.2024.02.25 val PER: 0.1138
2026-01-04 23:25:49,676: t15.2024.03.08 val PER: 0.2404
2026-01-04 23:25:49,676: t15.2024.03.15 val PER: 0.2045
2026-01-04 23:25:49,676: t15.2024.03.17 val PER: 0.1444
2026-01-04 23:25:49,676: t15.2024.05.10 val PER: 0.1575
2026-01-04 23:25:49,676: t15.2024.06.14 val PER: 0.1688
2026-01-04 23:25:49,676: t15.2024.07.19 val PER: 0.2472
2026-01-04 23:25:49,676: t15.2024.07.21 val PER: 0.0993
2026-01-04 23:25:49,676: t15.2024.07.28 val PER: 0.1375
2026-01-04 23:25:49,676: t15.2025.01.10 val PER: 0.3017
2026-01-04 23:25:49,676: t15.2025.01.12 val PER: 0.1470
2026-01-04 23:25:49,676: t15.2025.03.14 val PER: 0.3743
2026-01-04 23:25:49,677: t15.2025.03.16 val PER: 0.1937
2026-01-04 23:25:49,677: t15.2025.03.30 val PER: 0.3092
2026-01-04 23:25:49,677: t15.2025.04.13 val PER: 0.2154
2026-01-04 23:25:49,678: New best val WER(1gram) 48.22% --> 47.72%
2026-01-04 23:25:49,678: Checkpointing model
2026-01-04 23:25:50,355: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:25:50,676: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_12500
2026-01-04 23:25:59,848: Train batch 12600: loss: 7.69 grad norm: 43.27 time: 0.057
2026-01-04 23:26:18,552: Train batch 12800: loss: 5.67 grad norm: 39.42 time: 0.052
2026-01-04 23:26:37,065: Train batch 13000: loss: 6.22 grad norm: 40.21 time: 0.067
2026-01-04 23:26:37,065: Running test after training batch: 13000
2026-01-04 23:26:37,193: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:26:41,882: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 23:26:41,915: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 23:26:43,690: Val batch 13000: PER (avg): 0.1552 CTC Loss (avg): 15.8400 WER(1gram): 47.72% (n=64) time: 6.624
2026-01-04 23:26:43,690: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 23:26:43,690: t15.2023.08.13 val PER: 0.1164
2026-01-04 23:26:43,690: t15.2023.08.18 val PER: 0.1123
2026-01-04 23:26:43,690: t15.2023.08.20 val PER: 0.1056
2026-01-04 23:26:43,690: t15.2023.08.25 val PER: 0.0873
2026-01-04 23:26:43,691: t15.2023.08.27 val PER: 0.1994
2026-01-04 23:26:43,691: t15.2023.09.01 val PER: 0.0901
2026-01-04 23:26:43,691: t15.2023.09.03 val PER: 0.1651
2026-01-04 23:26:43,691: t15.2023.09.24 val PER: 0.1299
2026-01-04 23:26:43,691: t15.2023.09.29 val PER: 0.1347
2026-01-04 23:26:43,691: t15.2023.10.01 val PER: 0.1737
2026-01-04 23:26:43,691: t15.2023.10.06 val PER: 0.0883
2026-01-04 23:26:43,691: t15.2023.10.08 val PER: 0.2558
2026-01-04 23:26:43,691: t15.2023.10.13 val PER: 0.2079
2026-01-04 23:26:43,691: t15.2023.10.15 val PER: 0.1602
2026-01-04 23:26:43,691: t15.2023.10.20 val PER: 0.1846
2026-01-04 23:26:43,691: t15.2023.10.22 val PER: 0.1047
2026-01-04 23:26:43,691: t15.2023.11.03 val PER: 0.1845
2026-01-04 23:26:43,691: t15.2023.11.04 val PER: 0.0410
2026-01-04 23:26:43,691: t15.2023.11.17 val PER: 0.0498
2026-01-04 23:26:43,692: t15.2023.11.19 val PER: 0.0419
2026-01-04 23:26:43,692: t15.2023.11.26 val PER: 0.1275
2026-01-04 23:26:43,692: t15.2023.12.03 val PER: 0.1250
2026-01-04 23:26:43,692: t15.2023.12.08 val PER: 0.1005
2026-01-04 23:26:43,692: t15.2023.12.10 val PER: 0.0907
2026-01-04 23:26:43,692: t15.2023.12.17 val PER: 0.1424
2026-01-04 23:26:43,692: t15.2023.12.29 val PER: 0.1393
2026-01-04 23:26:43,692: t15.2024.02.25 val PER: 0.1110
2026-01-04 23:26:43,692: t15.2024.03.08 val PER: 0.2347
2026-01-04 23:26:43,692: t15.2024.03.15 val PER: 0.2126
2026-01-04 23:26:43,692: t15.2024.03.17 val PER: 0.1409
2026-01-04 23:26:43,692: t15.2024.05.10 val PER: 0.1664
2026-01-04 23:26:43,692: t15.2024.06.14 val PER: 0.1703
2026-01-04 23:26:43,692: t15.2024.07.19 val PER: 0.2419
2026-01-04 23:26:43,692: t15.2024.07.21 val PER: 0.0917
2026-01-04 23:26:43,692: t15.2024.07.28 val PER: 0.1375
2026-01-04 23:26:43,692: t15.2025.01.10 val PER: 0.2810
2026-01-04 23:26:43,693: t15.2025.01.12 val PER: 0.1555
2026-01-04 23:26:43,693: t15.2025.03.14 val PER: 0.3609
2026-01-04 23:26:43,693: t15.2025.03.16 val PER: 0.1859
2026-01-04 23:26:43,693: t15.2025.03.30 val PER: 0.3080
2026-01-04 23:26:43,693: t15.2025.04.13 val PER: 0.2211
2026-01-04 23:26:43,988: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_13000
2026-01-04 23:27:02,312: Train batch 13200: loss: 12.08 grad norm: 58.88 time: 0.054
2026-01-04 23:27:20,604: Train batch 13400: loss: 8.64 grad norm: 49.91 time: 0.062
2026-01-04 23:27:29,897: Running test after training batch: 13500
2026-01-04 23:27:30,028: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:27:34,720: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 23:27:34,752: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost nett
2026-01-04 23:27:36,497: Val batch 13500: PER (avg): 0.1512 CTC Loss (avg): 15.5718 WER(1gram): 46.45% (n=64) time: 6.600
2026-01-04 23:27:36,498: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 23:27:36,498: t15.2023.08.13 val PER: 0.1164
2026-01-04 23:27:36,498: t15.2023.08.18 val PER: 0.1065
2026-01-04 23:27:36,498: t15.2023.08.20 val PER: 0.1064
2026-01-04 23:27:36,498: t15.2023.08.25 val PER: 0.0798
2026-01-04 23:27:36,498: t15.2023.08.27 val PER: 0.1945
2026-01-04 23:27:36,498: t15.2023.09.01 val PER: 0.0869
2026-01-04 23:27:36,498: t15.2023.09.03 val PER: 0.1568
2026-01-04 23:27:36,499: t15.2023.09.24 val PER: 0.1274
2026-01-04 23:27:36,499: t15.2023.09.29 val PER: 0.1238
2026-01-04 23:27:36,499: t15.2023.10.01 val PER: 0.1764
2026-01-04 23:27:36,499: t15.2023.10.06 val PER: 0.0883
2026-01-04 23:27:36,499: t15.2023.10.08 val PER: 0.2530
2026-01-04 23:27:36,499: t15.2023.10.13 val PER: 0.2033
2026-01-04 23:27:36,499: t15.2023.10.15 val PER: 0.1595
2026-01-04 23:27:36,499: t15.2023.10.20 val PER: 0.1846
2026-01-04 23:27:36,499: t15.2023.10.22 val PER: 0.1080
2026-01-04 23:27:36,499: t15.2023.11.03 val PER: 0.1750
2026-01-04 23:27:36,499: t15.2023.11.04 val PER: 0.0307
2026-01-04 23:27:36,499: t15.2023.11.17 val PER: 0.0420
2026-01-04 23:27:36,499: t15.2023.11.19 val PER: 0.0319
2026-01-04 23:27:36,500: t15.2023.11.26 val PER: 0.1145
2026-01-04 23:27:36,500: t15.2023.12.03 val PER: 0.1145
2026-01-04 23:27:36,500: t15.2023.12.08 val PER: 0.1039
2026-01-04 23:27:36,500: t15.2023.12.10 val PER: 0.0972
2026-01-04 23:27:36,500: t15.2023.12.17 val PER: 0.1268
2026-01-04 23:27:36,500: t15.2023.12.29 val PER: 0.1311
2026-01-04 23:27:36,500: t15.2024.02.25 val PER: 0.1110
2026-01-04 23:27:36,500: t15.2024.03.08 val PER: 0.2276
2026-01-04 23:27:36,500: t15.2024.03.15 val PER: 0.2014
2026-01-04 23:27:36,500: t15.2024.03.17 val PER: 0.1464
2026-01-04 23:27:36,500: t15.2024.05.10 val PER: 0.1634
2026-01-04 23:27:36,500: t15.2024.06.14 val PER: 0.1593
2026-01-04 23:27:36,500: t15.2024.07.19 val PER: 0.2380
2026-01-04 23:27:36,501: t15.2024.07.21 val PER: 0.0897
2026-01-04 23:27:36,501: t15.2024.07.28 val PER: 0.1331
2026-01-04 23:27:36,501: t15.2025.01.10 val PER: 0.2920
2026-01-04 23:27:36,501: t15.2025.01.12 val PER: 0.1532
2026-01-04 23:27:36,501: t15.2025.03.14 val PER: 0.3536
2026-01-04 23:27:36,501: t15.2025.03.16 val PER: 0.1859
2026-01-04 23:27:36,501: t15.2025.03.30 val PER: 0.3069
2026-01-04 23:27:36,501: t15.2025.04.13 val PER: 0.2140
2026-01-04 23:27:36,502: New best val WER(1gram) 47.72% --> 46.45%
2026-01-04 23:27:36,502: Checkpointing model
2026-01-04 23:27:37,155: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:27:37,460: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_13500
2026-01-04 23:27:46,570: Train batch 13600: loss: 12.81 grad norm: 63.38 time: 0.062
2026-01-04 23:28:05,272: Train batch 13800: loss: 8.69 grad norm: 55.23 time: 0.056
2026-01-04 23:28:24,030: Train batch 14000: loss: 11.45 grad norm: 55.96 time: 0.051
2026-01-04 23:28:24,031: Running test after training batch: 14000
2026-01-04 23:28:24,220: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:28:28,924: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:28:28,956: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 23:28:30,730: Val batch 14000: PER (avg): 0.1495 CTC Loss (avg): 15.5225 WER(1gram): 45.94% (n=64) time: 6.699
2026-01-04 23:28:30,731: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-04 23:28:30,731: t15.2023.08.13 val PER: 0.1102
2026-01-04 23:28:30,731: t15.2023.08.18 val PER: 0.1006
2026-01-04 23:28:30,731: t15.2023.08.20 val PER: 0.1080
2026-01-04 23:28:30,731: t15.2023.08.25 val PER: 0.0934
2026-01-04 23:28:30,731: t15.2023.08.27 val PER: 0.1865
2026-01-04 23:28:30,731: t15.2023.09.01 val PER: 0.0804
2026-01-04 23:28:30,731: t15.2023.09.03 val PER: 0.1734
2026-01-04 23:28:30,731: t15.2023.09.24 val PER: 0.1262
2026-01-04 23:28:30,731: t15.2023.09.29 val PER: 0.1270
2026-01-04 23:28:30,731: t15.2023.10.01 val PER: 0.1816
2026-01-04 23:28:30,732: t15.2023.10.06 val PER: 0.0797
2026-01-04 23:28:30,732: t15.2023.10.08 val PER: 0.2490
2026-01-04 23:28:30,732: t15.2023.10.13 val PER: 0.2002
2026-01-04 23:28:30,732: t15.2023.10.15 val PER: 0.1602
2026-01-04 23:28:30,732: t15.2023.10.20 val PER: 0.2081
2026-01-04 23:28:30,732: t15.2023.10.22 val PER: 0.1158
2026-01-04 23:28:30,732: t15.2023.11.03 val PER: 0.1744
2026-01-04 23:28:30,732: t15.2023.11.04 val PER: 0.0375
2026-01-04 23:28:30,732: t15.2023.11.17 val PER: 0.0451
2026-01-04 23:28:30,732: t15.2023.11.19 val PER: 0.0279
2026-01-04 23:28:30,732: t15.2023.11.26 val PER: 0.1188
2026-01-04 23:28:30,732: t15.2023.12.03 val PER: 0.1176
2026-01-04 23:28:30,732: t15.2023.12.08 val PER: 0.0999
2026-01-04 23:28:30,732: t15.2023.12.10 val PER: 0.0933
2026-01-04 23:28:30,732: t15.2023.12.17 val PER: 0.1279
2026-01-04 23:28:30,732: t15.2023.12.29 val PER: 0.1290
2026-01-04 23:28:30,733: t15.2024.02.25 val PER: 0.1067
2026-01-04 23:28:30,733: t15.2024.03.08 val PER: 0.2219
2026-01-04 23:28:30,733: t15.2024.03.15 val PER: 0.1939
2026-01-04 23:28:30,733: t15.2024.03.17 val PER: 0.1395
2026-01-04 23:28:30,733: t15.2024.05.10 val PER: 0.1620
2026-01-04 23:28:30,733: t15.2024.06.14 val PER: 0.1577
2026-01-04 23:28:30,733: t15.2024.07.19 val PER: 0.2287
2026-01-04 23:28:30,733: t15.2024.07.21 val PER: 0.0897
2026-01-04 23:28:30,733: t15.2024.07.28 val PER: 0.1279
2026-01-04 23:28:30,733: t15.2025.01.10 val PER: 0.2837
2026-01-04 23:28:30,733: t15.2025.01.12 val PER: 0.1501
2026-01-04 23:28:30,733: t15.2025.03.14 val PER: 0.3402
2026-01-04 23:28:30,733: t15.2025.03.16 val PER: 0.1846
2026-01-04 23:28:30,733: t15.2025.03.30 val PER: 0.2943
2026-01-04 23:28:30,733: t15.2025.04.13 val PER: 0.2183
2026-01-04 23:28:30,738: New best val WER(1gram) 46.45% --> 45.94%
2026-01-04 23:28:30,738: Checkpointing model
2026-01-04 23:28:31,420: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:28:31,716: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_14000
2026-01-04 23:28:50,272: Train batch 14200: loss: 7.85 grad norm: 49.80 time: 0.056
2026-01-04 23:29:08,889: Train batch 14400: loss: 5.61 grad norm: 39.69 time: 0.064
2026-01-04 23:29:18,175: Running test after training batch: 14500
2026-01-04 23:29:18,283: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:29:23,079: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 23:29:23,111: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 23:29:24,904: Val batch 14500: PER (avg): 0.1509 CTC Loss (avg): 15.6336 WER(1gram): 46.19% (n=64) time: 6.728
2026-01-04 23:29:24,904: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 23:29:24,904: t15.2023.08.13 val PER: 0.1143
2026-01-04 23:29:24,904: t15.2023.08.18 val PER: 0.1115
2026-01-04 23:29:24,904: t15.2023.08.20 val PER: 0.1041
2026-01-04 23:29:24,905: t15.2023.08.25 val PER: 0.0828
2026-01-04 23:29:24,905: t15.2023.08.27 val PER: 0.1929
2026-01-04 23:29:24,905: t15.2023.09.01 val PER: 0.0804
2026-01-04 23:29:24,905: t15.2023.09.03 val PER: 0.1568
2026-01-04 23:29:24,905: t15.2023.09.24 val PER: 0.1311
2026-01-04 23:29:24,905: t15.2023.09.29 val PER: 0.1315
2026-01-04 23:29:24,905: t15.2023.10.01 val PER: 0.1823
2026-01-04 23:29:24,905: t15.2023.10.06 val PER: 0.0829
2026-01-04 23:29:24,905: t15.2023.10.08 val PER: 0.2571
2026-01-04 23:29:24,905: t15.2023.10.13 val PER: 0.2002
2026-01-04 23:29:24,906: t15.2023.10.15 val PER: 0.1582
2026-01-04 23:29:24,906: t15.2023.10.20 val PER: 0.1846
2026-01-04 23:29:24,906: t15.2023.10.22 val PER: 0.1080
2026-01-04 23:29:24,906: t15.2023.11.03 val PER: 0.1777
2026-01-04 23:29:24,906: t15.2023.11.04 val PER: 0.0375
2026-01-04 23:29:24,906: t15.2023.11.17 val PER: 0.0513
2026-01-04 23:29:24,906: t15.2023.11.19 val PER: 0.0379
2026-01-04 23:29:24,906: t15.2023.11.26 val PER: 0.1188
2026-01-04 23:29:24,906: t15.2023.12.03 val PER: 0.1134
2026-01-04 23:29:24,906: t15.2023.12.08 val PER: 0.0959
2026-01-04 23:29:24,906: t15.2023.12.10 val PER: 0.0894
2026-01-04 23:29:24,906: t15.2023.12.17 val PER: 0.1424
2026-01-04 23:29:24,907: t15.2023.12.29 val PER: 0.1304
2026-01-04 23:29:24,907: t15.2024.02.25 val PER: 0.1180
2026-01-04 23:29:24,907: t15.2024.03.08 val PER: 0.2248
2026-01-04 23:29:24,907: t15.2024.03.15 val PER: 0.2001
2026-01-04 23:29:24,907: t15.2024.03.17 val PER: 0.1339
2026-01-04 23:29:24,907: t15.2024.05.10 val PER: 0.1590
2026-01-04 23:29:24,907: t15.2024.06.14 val PER: 0.1577
2026-01-04 23:29:24,907: t15.2024.07.19 val PER: 0.2380
2026-01-04 23:29:24,907: t15.2024.07.21 val PER: 0.0931
2026-01-04 23:29:24,907: t15.2024.07.28 val PER: 0.1382
2026-01-04 23:29:24,907: t15.2025.01.10 val PER: 0.2700
2026-01-04 23:29:24,908: t15.2025.01.12 val PER: 0.1501
2026-01-04 23:29:24,908: t15.2025.03.14 val PER: 0.3536
2026-01-04 23:29:24,908: t15.2025.03.16 val PER: 0.1872
2026-01-04 23:29:24,908: t15.2025.03.30 val PER: 0.2943
2026-01-04 23:29:24,908: t15.2025.04.13 val PER: 0.2154
2026-01-04 23:29:25,194: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_14500
2026-01-04 23:29:34,488: Train batch 14600: loss: 12.38 grad norm: 60.52 time: 0.058
2026-01-04 23:29:53,023: Train batch 14800: loss: 5.40 grad norm: 42.39 time: 0.051
2026-01-04 23:30:11,709: Train batch 15000: loss: 8.72 grad norm: 48.33 time: 0.052
2026-01-04 23:30:11,709: Running test after training batch: 15000
2026-01-04 23:30:11,810: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:30:16,531: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 23:30:16,564: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 23:30:18,356: Val batch 15000: PER (avg): 0.1475 CTC Loss (avg): 15.3175 WER(1gram): 45.69% (n=64) time: 6.646
2026-01-04 23:30:18,356: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 23:30:18,356: t15.2023.08.13 val PER: 0.1112
2026-01-04 23:30:18,356: t15.2023.08.18 val PER: 0.1023
2026-01-04 23:30:18,356: t15.2023.08.20 val PER: 0.1001
2026-01-04 23:30:18,357: t15.2023.08.25 val PER: 0.0934
2026-01-04 23:30:18,357: t15.2023.08.27 val PER: 0.1849
2026-01-04 23:30:18,357: t15.2023.09.01 val PER: 0.0787
2026-01-04 23:30:18,357: t15.2023.09.03 val PER: 0.1485
2026-01-04 23:30:18,357: t15.2023.09.24 val PER: 0.1299
2026-01-04 23:30:18,357: t15.2023.09.29 val PER: 0.1244
2026-01-04 23:30:18,357: t15.2023.10.01 val PER: 0.1731
2026-01-04 23:30:18,357: t15.2023.10.06 val PER: 0.0818
2026-01-04 23:30:18,357: t15.2023.10.08 val PER: 0.2476
2026-01-04 23:30:18,357: t15.2023.10.13 val PER: 0.1978
2026-01-04 23:30:18,357: t15.2023.10.15 val PER: 0.1543
2026-01-04 23:30:18,357: t15.2023.10.20 val PER: 0.2047
2026-01-04 23:30:18,357: t15.2023.10.22 val PER: 0.1147
2026-01-04 23:30:18,357: t15.2023.11.03 val PER: 0.1689
2026-01-04 23:30:18,357: t15.2023.11.04 val PER: 0.0375
2026-01-04 23:30:18,358: t15.2023.11.17 val PER: 0.0467
2026-01-04 23:30:18,358: t15.2023.11.19 val PER: 0.0399
2026-01-04 23:30:18,358: t15.2023.11.26 val PER: 0.1087
2026-01-04 23:30:18,358: t15.2023.12.03 val PER: 0.1197
2026-01-04 23:30:18,358: t15.2023.12.08 val PER: 0.0952
2026-01-04 23:30:18,358: t15.2023.12.10 val PER: 0.0920
2026-01-04 23:30:18,358: t15.2023.12.17 val PER: 0.1320
2026-01-04 23:30:18,358: t15.2023.12.29 val PER: 0.1352
2026-01-04 23:30:18,358: t15.2024.02.25 val PER: 0.1067
2026-01-04 23:30:18,358: t15.2024.03.08 val PER: 0.2205
2026-01-04 23:30:18,358: t15.2024.03.15 val PER: 0.2039
2026-01-04 23:30:18,358: t15.2024.03.17 val PER: 0.1269
2026-01-04 23:30:18,358: t15.2024.05.10 val PER: 0.1560
2026-01-04 23:30:18,358: t15.2024.06.14 val PER: 0.1625
2026-01-04 23:30:18,358: t15.2024.07.19 val PER: 0.2195
2026-01-04 23:30:18,358: t15.2024.07.21 val PER: 0.0938
2026-01-04 23:30:18,358: t15.2024.07.28 val PER: 0.1221
2026-01-04 23:30:18,359: t15.2025.01.10 val PER: 0.2934
2026-01-04 23:30:18,359: t15.2025.01.12 val PER: 0.1455
2026-01-04 23:30:18,359: t15.2025.03.14 val PER: 0.3491
2026-01-04 23:30:18,359: t15.2025.03.16 val PER: 0.1780
2026-01-04 23:30:18,359: t15.2025.03.30 val PER: 0.3000
2026-01-04 23:30:18,359: t15.2025.04.13 val PER: 0.2126
2026-01-04 23:30:18,360: New best val WER(1gram) 45.94% --> 45.69%
2026-01-04 23:30:18,360: Checkpointing model
2026-01-04 23:30:19,003: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:30:19,304: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_15000
2026-01-04 23:30:37,771: Train batch 15200: loss: 5.33 grad norm: 42.65 time: 0.058
2026-01-04 23:30:56,173: Train batch 15400: loss: 11.10 grad norm: 57.36 time: 0.050
2026-01-04 23:31:05,302: Running test after training batch: 15500
2026-01-04 23:31:05,421: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:31:10,326: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:31:10,359: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 23:31:12,191: Val batch 15500: PER (avg): 0.1471 CTC Loss (avg): 15.2088 WER(1gram): 45.43% (n=64) time: 6.888
2026-01-04 23:31:12,191: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 23:31:12,191: t15.2023.08.13 val PER: 0.1143
2026-01-04 23:31:12,192: t15.2023.08.18 val PER: 0.1031
2026-01-04 23:31:12,192: t15.2023.08.20 val PER: 0.1048
2026-01-04 23:31:12,192: t15.2023.08.25 val PER: 0.0889
2026-01-04 23:31:12,192: t15.2023.08.27 val PER: 0.1833
2026-01-04 23:31:12,192: t15.2023.09.01 val PER: 0.0804
2026-01-04 23:31:12,192: t15.2023.09.03 val PER: 0.1591
2026-01-04 23:31:12,192: t15.2023.09.24 val PER: 0.1286
2026-01-04 23:31:12,192: t15.2023.09.29 val PER: 0.1238
2026-01-04 23:31:12,192: t15.2023.10.01 val PER: 0.1757
2026-01-04 23:31:12,192: t15.2023.10.06 val PER: 0.0786
2026-01-04 23:31:12,192: t15.2023.10.08 val PER: 0.2395
2026-01-04 23:31:12,192: t15.2023.10.13 val PER: 0.1893
2026-01-04 23:31:12,192: t15.2023.10.15 val PER: 0.1536
2026-01-04 23:31:12,193: t15.2023.10.20 val PER: 0.1946
2026-01-04 23:31:12,193: t15.2023.10.22 val PER: 0.1125
2026-01-04 23:31:12,193: t15.2023.11.03 val PER: 0.1723
2026-01-04 23:31:12,193: t15.2023.11.04 val PER: 0.0410
2026-01-04 23:31:12,193: t15.2023.11.17 val PER: 0.0404
2026-01-04 23:31:12,193: t15.2023.11.19 val PER: 0.0319
2026-01-04 23:31:12,193: t15.2023.11.26 val PER: 0.1152
2026-01-04 23:31:12,193: t15.2023.12.03 val PER: 0.1145
2026-01-04 23:31:12,193: t15.2023.12.08 val PER: 0.0932
2026-01-04 23:31:12,193: t15.2023.12.10 val PER: 0.0854
2026-01-04 23:31:12,193: t15.2023.12.17 val PER: 0.1331
2026-01-04 23:31:12,193: t15.2023.12.29 val PER: 0.1345
2026-01-04 23:31:12,193: t15.2024.02.25 val PER: 0.1067
2026-01-04 23:31:12,193: t15.2024.03.08 val PER: 0.2219
2026-01-04 23:31:12,193: t15.2024.03.15 val PER: 0.1989
2026-01-04 23:31:12,193: t15.2024.03.17 val PER: 0.1325
2026-01-04 23:31:12,193: t15.2024.05.10 val PER: 0.1545
2026-01-04 23:31:12,194: t15.2024.06.14 val PER: 0.1656
2026-01-04 23:31:12,194: t15.2024.07.19 val PER: 0.2268
2026-01-04 23:31:12,194: t15.2024.07.21 val PER: 0.0917
2026-01-04 23:31:12,194: t15.2024.07.28 val PER: 0.1301
2026-01-04 23:31:12,194: t15.2025.01.10 val PER: 0.2769
2026-01-04 23:31:12,194: t15.2025.01.12 val PER: 0.1547
2026-01-04 23:31:12,194: t15.2025.03.14 val PER: 0.3402
2026-01-04 23:31:12,194: t15.2025.03.16 val PER: 0.1741
2026-01-04 23:31:12,194: t15.2025.03.30 val PER: 0.2897
2026-01-04 23:31:12,194: t15.2025.04.13 val PER: 0.2026
2026-01-04 23:31:12,195: New best val WER(1gram) 45.69% --> 45.43%
2026-01-04 23:31:12,195: Checkpointing model
2026-01-04 23:31:12,813: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:31:13,107: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_15500
2026-01-04 23:31:22,057: Train batch 15600: loss: 11.39 grad norm: 60.79 time: 0.063
2026-01-04 23:31:40,474: Train batch 15800: loss: 13.32 grad norm: 62.47 time: 0.066
2026-01-04 23:31:59,104: Train batch 16000: loss: 8.55 grad norm: 48.35 time: 0.054
2026-01-04 23:31:59,104: Running test after training batch: 16000
2026-01-04 23:31:59,299: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:32:03,999: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:32:04,032: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-04 23:32:05,854: Val batch 16000: PER (avg): 0.1477 CTC Loss (avg): 15.3603 WER(1gram): 45.94% (n=64) time: 6.749
2026-01-04 23:32:05,854: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 23:32:05,854: t15.2023.08.13 val PER: 0.1123
2026-01-04 23:32:05,854: t15.2023.08.18 val PER: 0.1048
2026-01-04 23:32:05,854: t15.2023.08.20 val PER: 0.1017
2026-01-04 23:32:05,855: t15.2023.08.25 val PER: 0.0858
2026-01-04 23:32:05,855: t15.2023.08.27 val PER: 0.1817
2026-01-04 23:32:05,855: t15.2023.09.01 val PER: 0.0812
2026-01-04 23:32:05,855: t15.2023.09.03 val PER: 0.1473
2026-01-04 23:32:05,855: t15.2023.09.24 val PER: 0.1299
2026-01-04 23:32:05,855: t15.2023.09.29 val PER: 0.1276
2026-01-04 23:32:05,855: t15.2023.10.01 val PER: 0.1750
2026-01-04 23:32:05,855: t15.2023.10.06 val PER: 0.0829
2026-01-04 23:32:05,855: t15.2023.10.08 val PER: 0.2463
2026-01-04 23:32:05,855: t15.2023.10.13 val PER: 0.1963
2026-01-04 23:32:05,855: t15.2023.10.15 val PER: 0.1523
2026-01-04 23:32:05,856: t15.2023.10.20 val PER: 0.1879
2026-01-04 23:32:05,856: t15.2023.10.22 val PER: 0.1036
2026-01-04 23:32:05,856: t15.2023.11.03 val PER: 0.1771
2026-01-04 23:32:05,856: t15.2023.11.04 val PER: 0.0410
2026-01-04 23:32:05,856: t15.2023.11.17 val PER: 0.0327
2026-01-04 23:32:05,856: t15.2023.11.19 val PER: 0.0359
2026-01-04 23:32:05,856: t15.2023.11.26 val PER: 0.1167
2026-01-04 23:32:05,856: t15.2023.12.03 val PER: 0.1113
2026-01-04 23:32:05,856: t15.2023.12.08 val PER: 0.0965
2026-01-04 23:32:05,857: t15.2023.12.10 val PER: 0.0920
2026-01-04 23:32:05,857: t15.2023.12.17 val PER: 0.1268
2026-01-04 23:32:05,857: t15.2023.12.29 val PER: 0.1270
2026-01-04 23:32:05,857: t15.2024.02.25 val PER: 0.0997
2026-01-04 23:32:05,857: t15.2024.03.08 val PER: 0.2347
2026-01-04 23:32:05,857: t15.2024.03.15 val PER: 0.1976
2026-01-04 23:32:05,857: t15.2024.03.17 val PER: 0.1318
2026-01-04 23:32:05,857: t15.2024.05.10 val PER: 0.1575
2026-01-04 23:32:05,857: t15.2024.06.14 val PER: 0.1640
2026-01-04 23:32:05,858: t15.2024.07.19 val PER: 0.2274
2026-01-04 23:32:05,858: t15.2024.07.21 val PER: 0.0897
2026-01-04 23:32:05,858: t15.2024.07.28 val PER: 0.1324
2026-01-04 23:32:05,858: t15.2025.01.10 val PER: 0.2837
2026-01-04 23:32:05,858: t15.2025.01.12 val PER: 0.1440
2026-01-04 23:32:05,858: t15.2025.03.14 val PER: 0.3491
2026-01-04 23:32:05,858: t15.2025.03.16 val PER: 0.1885
2026-01-04 23:32:05,858: t15.2025.03.30 val PER: 0.3034
2026-01-04 23:32:05,858: t15.2025.04.13 val PER: 0.2140
2026-01-04 23:32:06,163: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_16000
2026-01-04 23:32:24,660: Train batch 16200: loss: 6.34 grad norm: 44.77 time: 0.054
2026-01-04 23:32:43,105: Train batch 16400: loss: 10.59 grad norm: 62.63 time: 0.056
2026-01-04 23:32:52,347: Running test after training batch: 16500
2026-01-04 23:32:52,503: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:32:57,214: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:32:57,248: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 23:32:59,072: Val batch 16500: PER (avg): 0.1471 CTC Loss (avg): 15.2021 WER(1gram): 45.43% (n=64) time: 6.724
2026-01-04 23:32:59,072: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 23:32:59,072: t15.2023.08.13 val PER: 0.1102
2026-01-04 23:32:59,073: t15.2023.08.18 val PER: 0.1006
2026-01-04 23:32:59,073: t15.2023.08.20 val PER: 0.1048
2026-01-04 23:32:59,073: t15.2023.08.25 val PER: 0.0843
2026-01-04 23:32:59,073: t15.2023.08.27 val PER: 0.1897
2026-01-04 23:32:59,073: t15.2023.09.01 val PER: 0.0820
2026-01-04 23:32:59,073: t15.2023.09.03 val PER: 0.1544
2026-01-04 23:32:59,073: t15.2023.09.24 val PER: 0.1262
2026-01-04 23:32:59,073: t15.2023.09.29 val PER: 0.1264
2026-01-04 23:32:59,073: t15.2023.10.01 val PER: 0.1744
2026-01-04 23:32:59,073: t15.2023.10.06 val PER: 0.0818
2026-01-04 23:32:59,073: t15.2023.10.08 val PER: 0.2530
2026-01-04 23:32:59,073: t15.2023.10.13 val PER: 0.1939
2026-01-04 23:32:59,073: t15.2023.10.15 val PER: 0.1470
2026-01-04 23:32:59,073: t15.2023.10.20 val PER: 0.1879
2026-01-04 23:32:59,073: t15.2023.10.22 val PER: 0.1102
2026-01-04 23:32:59,074: t15.2023.11.03 val PER: 0.1811
2026-01-04 23:32:59,074: t15.2023.11.04 val PER: 0.0375
2026-01-04 23:32:59,074: t15.2023.11.17 val PER: 0.0404
2026-01-04 23:32:59,074: t15.2023.11.19 val PER: 0.0339
2026-01-04 23:32:59,074: t15.2023.11.26 val PER: 0.1051
2026-01-04 23:32:59,074: t15.2023.12.03 val PER: 0.1071
2026-01-04 23:32:59,074: t15.2023.12.08 val PER: 0.0939
2026-01-04 23:32:59,074: t15.2023.12.10 val PER: 0.0894
2026-01-04 23:32:59,075: t15.2023.12.17 val PER: 0.1258
2026-01-04 23:32:59,075: t15.2023.12.29 val PER: 0.1249
2026-01-04 23:32:59,075: t15.2024.02.25 val PER: 0.1096
2026-01-04 23:32:59,075: t15.2024.03.08 val PER: 0.2219
2026-01-04 23:32:59,075: t15.2024.03.15 val PER: 0.1995
2026-01-04 23:32:59,075: t15.2024.03.17 val PER: 0.1325
2026-01-04 23:32:59,075: t15.2024.05.10 val PER: 0.1486
2026-01-04 23:32:59,075: t15.2024.06.14 val PER: 0.1593
2026-01-04 23:32:59,075: t15.2024.07.19 val PER: 0.2287
2026-01-04 23:32:59,075: t15.2024.07.21 val PER: 0.0890
2026-01-04 23:32:59,075: t15.2024.07.28 val PER: 0.1324
2026-01-04 23:32:59,075: t15.2025.01.10 val PER: 0.2810
2026-01-04 23:32:59,075: t15.2025.01.12 val PER: 0.1424
2026-01-04 23:32:59,075: t15.2025.03.14 val PER: 0.3595
2026-01-04 23:32:59,076: t15.2025.03.16 val PER: 0.1806
2026-01-04 23:32:59,076: t15.2025.03.30 val PER: 0.3023
2026-01-04 23:32:59,076: t15.2025.04.13 val PER: 0.2197
2026-01-04 23:32:59,364: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_16500
2026-01-04 23:33:08,544: Train batch 16600: loss: 8.14 grad norm: 53.21 time: 0.051
2026-01-04 23:33:27,174: Train batch 16800: loss: 16.16 grad norm: 74.57 time: 0.061
2026-01-04 23:33:45,562: Train batch 17000: loss: 7.93 grad norm: 48.80 time: 0.080
2026-01-04 23:33:45,562: Running test after training batch: 17000
2026-01-04 23:33:45,669: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:33:50,535: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:33:50,570: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 23:33:52,442: Val batch 17000: PER (avg): 0.1472 CTC Loss (avg): 15.1029 WER(1gram): 46.45% (n=64) time: 6.880
2026-01-04 23:33:52,443: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 23:33:52,443: t15.2023.08.13 val PER: 0.1112
2026-01-04 23:33:52,443: t15.2023.08.18 val PER: 0.1031
2026-01-04 23:33:52,443: t15.2023.08.20 val PER: 0.1064
2026-01-04 23:33:52,443: t15.2023.08.25 val PER: 0.0889
2026-01-04 23:33:52,443: t15.2023.08.27 val PER: 0.1897
2026-01-04 23:33:52,444: t15.2023.09.01 val PER: 0.0804
2026-01-04 23:33:52,444: t15.2023.09.03 val PER: 0.1544
2026-01-04 23:33:52,444: t15.2023.09.24 val PER: 0.1286
2026-01-04 23:33:52,444: t15.2023.09.29 val PER: 0.1276
2026-01-04 23:33:52,444: t15.2023.10.01 val PER: 0.1697
2026-01-04 23:33:52,444: t15.2023.10.06 val PER: 0.0883
2026-01-04 23:33:52,444: t15.2023.10.08 val PER: 0.2463
2026-01-04 23:33:52,444: t15.2023.10.13 val PER: 0.1924
2026-01-04 23:33:52,444: t15.2023.10.15 val PER: 0.1477
2026-01-04 23:33:52,444: t15.2023.10.20 val PER: 0.1779
2026-01-04 23:33:52,444: t15.2023.10.22 val PER: 0.1047
2026-01-04 23:33:52,444: t15.2023.11.03 val PER: 0.1784
2026-01-04 23:33:52,444: t15.2023.11.04 val PER: 0.0273
2026-01-04 23:33:52,444: t15.2023.11.17 val PER: 0.0420
2026-01-04 23:33:52,445: t15.2023.11.19 val PER: 0.0339
2026-01-04 23:33:52,445: t15.2023.11.26 val PER: 0.1080
2026-01-04 23:33:52,445: t15.2023.12.03 val PER: 0.1113
2026-01-04 23:33:52,445: t15.2023.12.08 val PER: 0.0959
2026-01-04 23:33:52,445: t15.2023.12.10 val PER: 0.0854
2026-01-04 23:33:52,445: t15.2023.12.17 val PER: 0.1299
2026-01-04 23:33:52,445: t15.2023.12.29 val PER: 0.1208
2026-01-04 23:33:52,445: t15.2024.02.25 val PER: 0.1053
2026-01-04 23:33:52,445: t15.2024.03.08 val PER: 0.2248
2026-01-04 23:33:52,445: t15.2024.03.15 val PER: 0.2001
2026-01-04 23:33:52,445: t15.2024.03.17 val PER: 0.1339
2026-01-04 23:33:52,445: t15.2024.05.10 val PER: 0.1545
2026-01-04 23:33:52,445: t15.2024.06.14 val PER: 0.1577
2026-01-04 23:33:52,445: t15.2024.07.19 val PER: 0.2340
2026-01-04 23:33:52,445: t15.2024.07.21 val PER: 0.0903
2026-01-04 23:33:52,445: t15.2024.07.28 val PER: 0.1279
2026-01-04 23:33:52,445: t15.2025.01.10 val PER: 0.2727
2026-01-04 23:33:52,446: t15.2025.01.12 val PER: 0.1440
2026-01-04 23:33:52,446: t15.2025.03.14 val PER: 0.3550
2026-01-04 23:33:52,446: t15.2025.03.16 val PER: 0.1898
2026-01-04 23:33:52,446: t15.2025.03.30 val PER: 0.3046
2026-01-04 23:33:52,446: t15.2025.04.13 val PER: 0.2168
2026-01-04 23:33:52,732: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_17000
2026-01-04 23:34:10,700: Train batch 17200: loss: 9.37 grad norm: 48.20 time: 0.084
2026-01-04 23:34:28,764: Train batch 17400: loss: 11.13 grad norm: 58.89 time: 0.071
2026-01-04 23:34:37,672: Running test after training batch: 17500
2026-01-04 23:34:37,763: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:34:42,439: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:34:42,473: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 23:34:44,325: Val batch 17500: PER (avg): 0.1462 CTC Loss (avg): 15.1486 WER(1gram): 45.94% (n=64) time: 6.653
2026-01-04 23:34:44,325: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 23:34:44,325: t15.2023.08.13 val PER: 0.1102
2026-01-04 23:34:44,325: t15.2023.08.18 val PER: 0.1048
2026-01-04 23:34:44,326: t15.2023.08.20 val PER: 0.1088
2026-01-04 23:34:44,326: t15.2023.08.25 val PER: 0.0904
2026-01-04 23:34:44,326: t15.2023.08.27 val PER: 0.1833
2026-01-04 23:34:44,326: t15.2023.09.01 val PER: 0.0779
2026-01-04 23:34:44,326: t15.2023.09.03 val PER: 0.1520
2026-01-04 23:34:44,326: t15.2023.09.24 val PER: 0.1262
2026-01-04 23:34:44,326: t15.2023.09.29 val PER: 0.1238
2026-01-04 23:34:44,326: t15.2023.10.01 val PER: 0.1737
2026-01-04 23:34:44,326: t15.2023.10.06 val PER: 0.0818
2026-01-04 23:34:44,326: t15.2023.10.08 val PER: 0.2503
2026-01-04 23:34:44,326: t15.2023.10.13 val PER: 0.1924
2026-01-04 23:34:44,327: t15.2023.10.15 val PER: 0.1437
2026-01-04 23:34:44,327: t15.2023.10.20 val PER: 0.1913
2026-01-04 23:34:44,327: t15.2023.10.22 val PER: 0.1047
2026-01-04 23:34:44,327: t15.2023.11.03 val PER: 0.1777
2026-01-04 23:34:44,327: t15.2023.11.04 val PER: 0.0341
2026-01-04 23:34:44,327: t15.2023.11.17 val PER: 0.0420
2026-01-04 23:34:44,327: t15.2023.11.19 val PER: 0.0339
2026-01-04 23:34:44,327: t15.2023.11.26 val PER: 0.1094
2026-01-04 23:34:44,327: t15.2023.12.03 val PER: 0.1145
2026-01-04 23:34:44,327: t15.2023.12.08 val PER: 0.0925
2026-01-04 23:34:44,327: t15.2023.12.10 val PER: 0.0854
2026-01-04 23:34:44,327: t15.2023.12.17 val PER: 0.1341
2026-01-04 23:34:44,327: t15.2023.12.29 val PER: 0.1215
2026-01-04 23:34:44,327: t15.2024.02.25 val PER: 0.1081
2026-01-04 23:34:44,327: t15.2024.03.08 val PER: 0.2219
2026-01-04 23:34:44,327: t15.2024.03.15 val PER: 0.1957
2026-01-04 23:34:44,328: t15.2024.03.17 val PER: 0.1360
2026-01-04 23:34:44,328: t15.2024.05.10 val PER: 0.1486
2026-01-04 23:34:44,328: t15.2024.06.14 val PER: 0.1577
2026-01-04 23:34:44,328: t15.2024.07.19 val PER: 0.2235
2026-01-04 23:34:44,328: t15.2024.07.21 val PER: 0.0890
2026-01-04 23:34:44,328: t15.2024.07.28 val PER: 0.1324
2026-01-04 23:34:44,328: t15.2025.01.10 val PER: 0.2865
2026-01-04 23:34:44,328: t15.2025.01.12 val PER: 0.1363
2026-01-04 23:34:44,328: t15.2025.03.14 val PER: 0.3536
2026-01-04 23:34:44,329: t15.2025.03.16 val PER: 0.1832
2026-01-04 23:34:44,329: t15.2025.03.30 val PER: 0.2954
2026-01-04 23:34:44,329: t15.2025.04.13 val PER: 0.2168
2026-01-04 23:34:44,614: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_17500
2026-01-04 23:34:53,556: Train batch 17600: loss: 9.62 grad norm: 55.26 time: 0.051
2026-01-04 23:35:11,904: Train batch 17800: loss: 6.41 grad norm: 52.16 time: 0.041
2026-01-04 23:35:29,839: Train batch 18000: loss: 10.37 grad norm: 60.67 time: 0.061
2026-01-04 23:35:29,840: Running test after training batch: 18000
2026-01-04 23:35:29,936: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:35:34,639: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:35:34,673: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 23:35:36,530: Val batch 18000: PER (avg): 0.1455 CTC Loss (avg): 15.1206 WER(1gram): 45.69% (n=64) time: 6.690
2026-01-04 23:35:36,530: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 23:35:36,530: t15.2023.08.13 val PER: 0.1123
2026-01-04 23:35:36,531: t15.2023.08.18 val PER: 0.1031
2026-01-04 23:35:36,531: t15.2023.08.20 val PER: 0.1041
2026-01-04 23:35:36,531: t15.2023.08.25 val PER: 0.0873
2026-01-04 23:35:36,531: t15.2023.08.27 val PER: 0.1881
2026-01-04 23:35:36,531: t15.2023.09.01 val PER: 0.0812
2026-01-04 23:35:36,531: t15.2023.09.03 val PER: 0.1496
2026-01-04 23:35:36,531: t15.2023.09.24 val PER: 0.1226
2026-01-04 23:35:36,531: t15.2023.09.29 val PER: 0.1244
2026-01-04 23:35:36,531: t15.2023.10.01 val PER: 0.1684
2026-01-04 23:35:36,531: t15.2023.10.06 val PER: 0.0807
2026-01-04 23:35:36,531: t15.2023.10.08 val PER: 0.2490
2026-01-04 23:35:36,531: t15.2023.10.13 val PER: 0.1947
2026-01-04 23:35:36,531: t15.2023.10.15 val PER: 0.1457
2026-01-04 23:35:36,531: t15.2023.10.20 val PER: 0.1913
2026-01-04 23:35:36,531: t15.2023.10.22 val PER: 0.0991
2026-01-04 23:35:36,531: t15.2023.11.03 val PER: 0.1777
2026-01-04 23:35:36,531: t15.2023.11.04 val PER: 0.0341
2026-01-04 23:35:36,532: t15.2023.11.17 val PER: 0.0373
2026-01-04 23:35:36,532: t15.2023.11.19 val PER: 0.0319
2026-01-04 23:35:36,532: t15.2023.11.26 val PER: 0.1094
2026-01-04 23:35:36,532: t15.2023.12.03 val PER: 0.1103
2026-01-04 23:35:36,532: t15.2023.12.08 val PER: 0.0972
2026-01-04 23:35:36,532: t15.2023.12.10 val PER: 0.0854
2026-01-04 23:35:36,532: t15.2023.12.17 val PER: 0.1289
2026-01-04 23:35:36,532: t15.2023.12.29 val PER: 0.1249
2026-01-04 23:35:36,532: t15.2024.02.25 val PER: 0.1053
2026-01-04 23:35:36,532: t15.2024.03.08 val PER: 0.2219
2026-01-04 23:35:36,532: t15.2024.03.15 val PER: 0.1932
2026-01-04 23:35:36,532: t15.2024.03.17 val PER: 0.1325
2026-01-04 23:35:36,532: t15.2024.05.10 val PER: 0.1456
2026-01-04 23:35:36,532: t15.2024.06.14 val PER: 0.1546
2026-01-04 23:35:36,533: t15.2024.07.19 val PER: 0.2287
2026-01-04 23:35:36,533: t15.2024.07.21 val PER: 0.0869
2026-01-04 23:35:36,533: t15.2024.07.28 val PER: 0.1279
2026-01-04 23:35:36,533: t15.2025.01.10 val PER: 0.2810
2026-01-04 23:35:36,533: t15.2025.01.12 val PER: 0.1370
2026-01-04 23:35:36,533: t15.2025.03.14 val PER: 0.3595
2026-01-04 23:35:36,533: t15.2025.03.16 val PER: 0.1872
2026-01-04 23:35:36,533: t15.2025.03.30 val PER: 0.2897
2026-01-04 23:35:36,533: t15.2025.04.13 val PER: 0.2183
2026-01-04 23:35:36,823: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_18000
2026-01-04 23:35:55,008: Train batch 18200: loss: 7.64 grad norm: 48.93 time: 0.073
2026-01-04 23:36:13,110: Train batch 18400: loss: 4.97 grad norm: 42.48 time: 0.057
2026-01-04 23:36:22,296: Running test after training batch: 18500
2026-01-04 23:36:22,399: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:36:27,228: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:36:27,263: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 23:36:29,127: Val batch 18500: PER (avg): 0.1456 CTC Loss (avg): 15.0569 WER(1gram): 45.18% (n=64) time: 6.831
2026-01-04 23:36:29,128: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 23:36:29,128: t15.2023.08.13 val PER: 0.1102
2026-01-04 23:36:29,128: t15.2023.08.18 val PER: 0.1039
2026-01-04 23:36:29,128: t15.2023.08.20 val PER: 0.1048
2026-01-04 23:36:29,128: t15.2023.08.25 val PER: 0.0828
2026-01-04 23:36:29,128: t15.2023.08.27 val PER: 0.1849
2026-01-04 23:36:29,129: t15.2023.09.01 val PER: 0.0828
2026-01-04 23:36:29,129: t15.2023.09.03 val PER: 0.1568
2026-01-04 23:36:29,129: t15.2023.09.24 val PER: 0.1214
2026-01-04 23:36:29,129: t15.2023.09.29 val PER: 0.1251
2026-01-04 23:36:29,129: t15.2023.10.01 val PER: 0.1697
2026-01-04 23:36:29,129: t15.2023.10.06 val PER: 0.0840
2026-01-04 23:36:29,129: t15.2023.10.08 val PER: 0.2517
2026-01-04 23:36:29,129: t15.2023.10.13 val PER: 0.1901
2026-01-04 23:36:29,129: t15.2023.10.15 val PER: 0.1450
2026-01-04 23:36:29,129: t15.2023.10.20 val PER: 0.1846
2026-01-04 23:36:29,129: t15.2023.10.22 val PER: 0.1024
2026-01-04 23:36:29,130: t15.2023.11.03 val PER: 0.1784
2026-01-04 23:36:29,130: t15.2023.11.04 val PER: 0.0341
2026-01-04 23:36:29,130: t15.2023.11.17 val PER: 0.0389
2026-01-04 23:36:29,130: t15.2023.11.19 val PER: 0.0319
2026-01-04 23:36:29,130: t15.2023.11.26 val PER: 0.1109
2026-01-04 23:36:29,130: t15.2023.12.03 val PER: 0.1166
2026-01-04 23:36:29,130: t15.2023.12.08 val PER: 0.0985
2026-01-04 23:36:29,130: t15.2023.12.10 val PER: 0.0867
2026-01-04 23:36:29,130: t15.2023.12.17 val PER: 0.1268
2026-01-04 23:36:29,130: t15.2023.12.29 val PER: 0.1187
2026-01-04 23:36:29,131: t15.2024.02.25 val PER: 0.1067
2026-01-04 23:36:29,131: t15.2024.03.08 val PER: 0.2191
2026-01-04 23:36:29,131: t15.2024.03.15 val PER: 0.1932
2026-01-04 23:36:29,131: t15.2024.03.17 val PER: 0.1283
2026-01-04 23:36:29,131: t15.2024.05.10 val PER: 0.1441
2026-01-04 23:36:29,131: t15.2024.06.14 val PER: 0.1562
2026-01-04 23:36:29,131: t15.2024.07.19 val PER: 0.2268
2026-01-04 23:36:29,131: t15.2024.07.21 val PER: 0.0855
2026-01-04 23:36:29,131: t15.2024.07.28 val PER: 0.1287
2026-01-04 23:36:29,131: t15.2025.01.10 val PER: 0.2769
2026-01-04 23:36:29,131: t15.2025.01.12 val PER: 0.1440
2026-01-04 23:36:29,132: t15.2025.03.14 val PER: 0.3550
2026-01-04 23:36:29,132: t15.2025.03.16 val PER: 0.1872
2026-01-04 23:36:29,132: t15.2025.03.30 val PER: 0.2989
2026-01-04 23:36:29,132: t15.2025.04.13 val PER: 0.2154
2026-01-04 23:36:29,132: New best val WER(1gram) 45.43% --> 45.18%
2026-01-04 23:36:29,132: Checkpointing model
2026-01-04 23:36:29,788: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:36:30,091: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_18500
2026-01-04 23:36:39,118: Train batch 18600: loss: 12.02 grad norm: 61.27 time: 0.067
2026-01-04 23:36:57,277: Train batch 18800: loss: 7.97 grad norm: 51.47 time: 0.063
2026-01-04 23:37:15,456: Train batch 19000: loss: 7.83 grad norm: 44.15 time: 0.064
2026-01-04 23:37:15,457: Running test after training batch: 19000
2026-01-04 23:37:15,719: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:37:20,433: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:37:20,469: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 23:37:22,392: Val batch 19000: PER (avg): 0.1447 CTC Loss (avg): 15.0566 WER(1gram): 45.18% (n=64) time: 6.935
2026-01-04 23:37:22,392: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 23:37:22,393: t15.2023.08.13 val PER: 0.1112
2026-01-04 23:37:22,393: t15.2023.08.18 val PER: 0.0981
2026-01-04 23:37:22,393: t15.2023.08.20 val PER: 0.1033
2026-01-04 23:37:22,393: t15.2023.08.25 val PER: 0.0828
2026-01-04 23:37:22,393: t15.2023.08.27 val PER: 0.1849
2026-01-04 23:37:22,393: t15.2023.09.01 val PER: 0.0836
2026-01-04 23:37:22,393: t15.2023.09.03 val PER: 0.1508
2026-01-04 23:37:22,393: t15.2023.09.24 val PER: 0.1226
2026-01-04 23:37:22,393: t15.2023.09.29 val PER: 0.1174
2026-01-04 23:37:22,393: t15.2023.10.01 val PER: 0.1717
2026-01-04 23:37:22,393: t15.2023.10.06 val PER: 0.0797
2026-01-04 23:37:22,393: t15.2023.10.08 val PER: 0.2395
2026-01-04 23:37:22,393: t15.2023.10.13 val PER: 0.1901
2026-01-04 23:37:22,394: t15.2023.10.15 val PER: 0.1510
2026-01-04 23:37:22,394: t15.2023.10.20 val PER: 0.1846
2026-01-04 23:37:22,394: t15.2023.10.22 val PER: 0.1024
2026-01-04 23:37:22,394: t15.2023.11.03 val PER: 0.1771
2026-01-04 23:37:22,394: t15.2023.11.04 val PER: 0.0341
2026-01-04 23:37:22,394: t15.2023.11.17 val PER: 0.0389
2026-01-04 23:37:22,394: t15.2023.11.19 val PER: 0.0299
2026-01-04 23:37:22,394: t15.2023.11.26 val PER: 0.1123
2026-01-04 23:37:22,394: t15.2023.12.03 val PER: 0.1103
2026-01-04 23:37:22,394: t15.2023.12.08 val PER: 0.0939
2026-01-04 23:37:22,394: t15.2023.12.10 val PER: 0.0828
2026-01-04 23:37:22,394: t15.2023.12.17 val PER: 0.1268
2026-01-04 23:37:22,394: t15.2023.12.29 val PER: 0.1263
2026-01-04 23:37:22,395: t15.2024.02.25 val PER: 0.0997
2026-01-04 23:37:22,395: t15.2024.03.08 val PER: 0.2219
2026-01-04 23:37:22,395: t15.2024.03.15 val PER: 0.1895
2026-01-04 23:37:22,395: t15.2024.03.17 val PER: 0.1346
2026-01-04 23:37:22,395: t15.2024.05.10 val PER: 0.1456
2026-01-04 23:37:22,395: t15.2024.06.14 val PER: 0.1593
2026-01-04 23:37:22,395: t15.2024.07.19 val PER: 0.2274
2026-01-04 23:37:22,395: t15.2024.07.21 val PER: 0.0883
2026-01-04 23:37:22,395: t15.2024.07.28 val PER: 0.1309
2026-01-04 23:37:22,395: t15.2025.01.10 val PER: 0.2796
2026-01-04 23:37:22,395: t15.2025.01.12 val PER: 0.1339
2026-01-04 23:37:22,395: t15.2025.03.14 val PER: 0.3580
2026-01-04 23:37:22,395: t15.2025.03.16 val PER: 0.1846
2026-01-04 23:37:22,395: t15.2025.03.30 val PER: 0.2908
2026-01-04 23:37:22,395: t15.2025.04.13 val PER: 0.2140
2026-01-04 23:37:22,689: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_19000
2026-01-04 23:37:40,826: Train batch 19200: loss: 5.53 grad norm: 47.79 time: 0.063
2026-01-04 23:37:59,363: Train batch 19400: loss: 4.86 grad norm: 35.88 time: 0.053
2026-01-04 23:38:08,744: Running test after training batch: 19500
2026-01-04 23:38:08,896: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:38:13,602: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:38:13,638: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 23:38:15,529: Val batch 19500: PER (avg): 0.1447 CTC Loss (avg): 15.0000 WER(1gram): 44.92% (n=64) time: 6.785
2026-01-04 23:38:15,529: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 23:38:15,530: t15.2023.08.13 val PER: 0.1133
2026-01-04 23:38:15,530: t15.2023.08.18 val PER: 0.1039
2026-01-04 23:38:15,530: t15.2023.08.20 val PER: 0.1064
2026-01-04 23:38:15,530: t15.2023.08.25 val PER: 0.0798
2026-01-04 23:38:15,530: t15.2023.08.27 val PER: 0.1785
2026-01-04 23:38:15,530: t15.2023.09.01 val PER: 0.0836
2026-01-04 23:38:15,530: t15.2023.09.03 val PER: 0.1473
2026-01-04 23:38:15,530: t15.2023.09.24 val PER: 0.1238
2026-01-04 23:38:15,530: t15.2023.09.29 val PER: 0.1244
2026-01-04 23:38:15,530: t15.2023.10.01 val PER: 0.1717
2026-01-04 23:38:15,531: t15.2023.10.06 val PER: 0.0850
2026-01-04 23:38:15,531: t15.2023.10.08 val PER: 0.2503
2026-01-04 23:38:15,531: t15.2023.10.13 val PER: 0.1994
2026-01-04 23:38:15,531: t15.2023.10.15 val PER: 0.1483
2026-01-04 23:38:15,531: t15.2023.10.20 val PER: 0.1913
2026-01-04 23:38:15,531: t15.2023.10.22 val PER: 0.1058
2026-01-04 23:38:15,531: t15.2023.11.03 val PER: 0.1771
2026-01-04 23:38:15,531: t15.2023.11.04 val PER: 0.0410
2026-01-04 23:38:15,531: t15.2023.11.17 val PER: 0.0358
2026-01-04 23:38:15,531: t15.2023.11.19 val PER: 0.0279
2026-01-04 23:38:15,531: t15.2023.11.26 val PER: 0.1087
2026-01-04 23:38:15,531: t15.2023.12.03 val PER: 0.1124
2026-01-04 23:38:15,531: t15.2023.12.08 val PER: 0.0885
2026-01-04 23:38:15,531: t15.2023.12.10 val PER: 0.0854
2026-01-04 23:38:15,531: t15.2023.12.17 val PER: 0.1247
2026-01-04 23:38:15,531: t15.2023.12.29 val PER: 0.1201
2026-01-04 23:38:15,531: t15.2024.02.25 val PER: 0.1039
2026-01-04 23:38:15,532: t15.2024.03.08 val PER: 0.2233
2026-01-04 23:38:15,532: t15.2024.03.15 val PER: 0.1914
2026-01-04 23:38:15,532: t15.2024.03.17 val PER: 0.1283
2026-01-04 23:38:15,532: t15.2024.05.10 val PER: 0.1441
2026-01-04 23:38:15,532: t15.2024.06.14 val PER: 0.1577
2026-01-04 23:38:15,532: t15.2024.07.19 val PER: 0.2254
2026-01-04 23:38:15,532: t15.2024.07.21 val PER: 0.0855
2026-01-04 23:38:15,532: t15.2024.07.28 val PER: 0.1257
2026-01-04 23:38:15,532: t15.2025.01.10 val PER: 0.2755
2026-01-04 23:38:15,532: t15.2025.01.12 val PER: 0.1355
2026-01-04 23:38:15,532: t15.2025.03.14 val PER: 0.3536
2026-01-04 23:38:15,532: t15.2025.03.16 val PER: 0.1832
2026-01-04 23:38:15,532: t15.2025.03.30 val PER: 0.2943
2026-01-04 23:38:15,532: t15.2025.04.13 val PER: 0.2126
2026-01-04 23:38:15,534: New best val WER(1gram) 45.18% --> 44.92%
2026-01-04 23:38:15,534: Checkpointing model
2026-01-04 23:38:16,176: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:38:16,476: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_19500
2026-01-04 23:38:25,937: Train batch 19600: loss: 7.23 grad norm: 45.41 time: 0.058
2026-01-04 23:38:44,106: Train batch 19800: loss: 6.98 grad norm: 47.64 time: 0.055
2026-01-04 23:39:01,992: Running test after training batch: 19999
2026-01-04 23:39:02,080: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:39:06,797: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:39:06,833: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 23:39:08,740: Val batch 19999: PER (avg): 0.1450 CTC Loss (avg): 15.0120 WER(1gram): 45.94% (n=64) time: 6.747
2026-01-04 23:39:08,740: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 23:39:08,740: t15.2023.08.13 val PER: 0.1112
2026-01-04 23:39:08,740: t15.2023.08.18 val PER: 0.1014
2026-01-04 23:39:08,741: t15.2023.08.20 val PER: 0.1048
2026-01-04 23:39:08,741: t15.2023.08.25 val PER: 0.0843
2026-01-04 23:39:08,741: t15.2023.08.27 val PER: 0.1768
2026-01-04 23:39:08,741: t15.2023.09.01 val PER: 0.0860
2026-01-04 23:39:08,741: t15.2023.09.03 val PER: 0.1556
2026-01-04 23:39:08,741: t15.2023.09.24 val PER: 0.1262
2026-01-04 23:39:08,741: t15.2023.09.29 val PER: 0.1213
2026-01-04 23:39:08,741: t15.2023.10.01 val PER: 0.1711
2026-01-04 23:39:08,741: t15.2023.10.06 val PER: 0.0775
2026-01-04 23:39:08,741: t15.2023.10.08 val PER: 0.2449
2026-01-04 23:39:08,742: t15.2023.10.13 val PER: 0.2002
2026-01-04 23:39:08,742: t15.2023.10.15 val PER: 0.1470
2026-01-04 23:39:08,742: t15.2023.10.20 val PER: 0.1846
2026-01-04 23:39:08,742: t15.2023.10.22 val PER: 0.1047
2026-01-04 23:39:08,742: t15.2023.11.03 val PER: 0.1750
2026-01-04 23:39:08,742: t15.2023.11.04 val PER: 0.0341
2026-01-04 23:39:08,742: t15.2023.11.17 val PER: 0.0373
2026-01-04 23:39:08,742: t15.2023.11.19 val PER: 0.0299
2026-01-04 23:39:08,742: t15.2023.11.26 val PER: 0.1101
2026-01-04 23:39:08,742: t15.2023.12.03 val PER: 0.1134
2026-01-04 23:39:08,742: t15.2023.12.08 val PER: 0.0945
2026-01-04 23:39:08,742: t15.2023.12.10 val PER: 0.0854
2026-01-04 23:39:08,742: t15.2023.12.17 val PER: 0.1310
2026-01-04 23:39:08,742: t15.2023.12.29 val PER: 0.1215
2026-01-04 23:39:08,743: t15.2024.02.25 val PER: 0.1039
2026-01-04 23:39:08,743: t15.2024.03.08 val PER: 0.2233
2026-01-04 23:39:08,743: t15.2024.03.15 val PER: 0.1926
2026-01-04 23:39:08,743: t15.2024.03.17 val PER: 0.1269
2026-01-04 23:39:08,743: t15.2024.05.10 val PER: 0.1426
2026-01-04 23:39:08,743: t15.2024.06.14 val PER: 0.1577
2026-01-04 23:39:08,743: t15.2024.07.19 val PER: 0.2268
2026-01-04 23:39:08,743: t15.2024.07.21 val PER: 0.0862
2026-01-04 23:39:08,743: t15.2024.07.28 val PER: 0.1265
2026-01-04 23:39:08,743: t15.2025.01.10 val PER: 0.2713
2026-01-04 23:39:08,743: t15.2025.01.12 val PER: 0.1393
2026-01-04 23:39:08,743: t15.2025.03.14 val PER: 0.3550
2026-01-04 23:39:08,743: t15.2025.03.16 val PER: 0.1819
2026-01-04 23:39:08,743: t15.2025.03.30 val PER: 0.2931
2026-01-04 23:39:08,743: t15.2025.04.13 val PER: 0.2197
2026-01-04 23:39:09,038: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id15_wd1e-5/checkpoint/checkpoint_batch_19999
2026-01-04 23:39:09,069: Best avg val PER achieved: 0.14467
2026-01-04 23:39:09,070: Total training time: 36.05 minutes

=== RUN id20_wd1e-5.yaml ===
2026-01-04 23:39:14,545: Using device: cuda:0
2026-01-04 23:39:16,148: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-04 23:39:16,175: Using 45 sessions after filtering (from 45).
2026-01-04 23:39:16,586: Using torch.compile (if available)
2026-01-04 23:39:16,587: torch.compile not available (torch<2.0). Skipping.
2026-01-04 23:39:16,587: Initialized RNN decoding model
2026-01-04 23:39:16,587: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-04 23:39:16,587: Model has 44,907,305 parameters
2026-01-04 23:39:16,587: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-04 23:39:17,876: Successfully initialized datasets
2026-01-04 23:39:17,877: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-04 23:39:18,811: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.184
2026-01-04 23:39:18,811: Running test after training batch: 0
2026-01-04 23:39:18,924: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:39:24,351: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-04 23:39:25,063: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-04 23:39:58,892: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 40.081
2026-01-04 23:39:58,893: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-04 23:39:58,893: t15.2023.08.13 val PER: 1.3056
2026-01-04 23:39:58,893: t15.2023.08.18 val PER: 1.4208
2026-01-04 23:39:58,893: t15.2023.08.20 val PER: 1.3002
2026-01-04 23:39:58,893: t15.2023.08.25 val PER: 1.3389
2026-01-04 23:39:58,893: t15.2023.08.27 val PER: 1.2460
2026-01-04 23:39:58,893: t15.2023.09.01 val PER: 1.4537
2026-01-04 23:39:58,893: t15.2023.09.03 val PER: 1.3171
2026-01-04 23:39:58,893: t15.2023.09.24 val PER: 1.5461
2026-01-04 23:39:58,893: t15.2023.09.29 val PER: 1.4671
2026-01-04 23:39:58,893: t15.2023.10.01 val PER: 1.2147
2026-01-04 23:39:58,893: t15.2023.10.06 val PER: 1.4876
2026-01-04 23:39:58,894: t15.2023.10.08 val PER: 1.1827
2026-01-04 23:39:58,894: t15.2023.10.13 val PER: 1.3964
2026-01-04 23:39:58,894: t15.2023.10.15 val PER: 1.3889
2026-01-04 23:39:58,894: t15.2023.10.20 val PER: 1.4866
2026-01-04 23:39:58,894: t15.2023.10.22 val PER: 1.3942
2026-01-04 23:39:58,894: t15.2023.11.03 val PER: 1.5923
2026-01-04 23:39:58,894: t15.2023.11.04 val PER: 2.0171
2026-01-04 23:39:58,894: t15.2023.11.17 val PER: 1.9518
2026-01-04 23:39:58,894: t15.2023.11.19 val PER: 1.6707
2026-01-04 23:39:58,894: t15.2023.11.26 val PER: 1.5413
2026-01-04 23:39:58,894: t15.2023.12.03 val PER: 1.4254
2026-01-04 23:39:58,894: t15.2023.12.08 val PER: 1.4487
2026-01-04 23:39:58,894: t15.2023.12.10 val PER: 1.6899
2026-01-04 23:39:58,894: t15.2023.12.17 val PER: 1.3077
2026-01-04 23:39:58,894: t15.2023.12.29 val PER: 1.4063
2026-01-04 23:39:58,895: t15.2024.02.25 val PER: 1.4228
2026-01-04 23:39:58,895: t15.2024.03.08 val PER: 1.3257
2026-01-04 23:39:58,895: t15.2024.03.15 val PER: 1.3196
2026-01-04 23:39:58,895: t15.2024.03.17 val PER: 1.4052
2026-01-04 23:39:58,895: t15.2024.05.10 val PER: 1.3224
2026-01-04 23:39:58,895: t15.2024.06.14 val PER: 1.5315
2026-01-04 23:39:58,895: t15.2024.07.19 val PER: 1.0817
2026-01-04 23:39:58,895: t15.2024.07.21 val PER: 1.6290
2026-01-04 23:39:58,895: t15.2024.07.28 val PER: 1.6588
2026-01-04 23:39:58,895: t15.2025.01.10 val PER: 1.0923
2026-01-04 23:39:58,895: t15.2025.01.12 val PER: 1.7629
2026-01-04 23:39:58,896: t15.2025.03.14 val PER: 1.0414
2026-01-04 23:39:58,896: t15.2025.03.16 val PER: 1.6257
2026-01-04 23:39:58,896: t15.2025.03.30 val PER: 1.2874
2026-01-04 23:39:58,896: t15.2025.04.13 val PER: 1.5949
2026-01-04 23:39:58,897: New best val WER(1gram) inf% --> 100.00%
2026-01-04 23:39:58,897: Checkpointing model
2026-01-04 23:39:59,166: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:39:59,436: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_0
2026-01-04 23:40:17,925: Train batch 200: loss: 77.59 grad norm: 106.15 time: 0.054
2026-01-04 23:40:36,122: Train batch 400: loss: 53.57 grad norm: 93.33 time: 0.063
2026-01-04 23:40:45,182: Running test after training batch: 500
2026-01-04 23:40:45,333: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:40:50,165: WER debug example
  GT : you can see the code at this point as well
  PR : used and ease thus uhde at this ide is aisle
2026-01-04 23:40:50,199: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as tides
2026-01-04 23:40:52,382: Val batch 500: PER (avg): 0.5217 CTC Loss (avg): 55.5690 WER(1gram): 88.32% (n=64) time: 7.199
2026-01-04 23:40:52,382: WER lens: avg_true_words=6.16 avg_pred_words=5.67 max_pred_words=12
2026-01-04 23:40:52,382: t15.2023.08.13 val PER: 0.4626
2026-01-04 23:40:52,382: t15.2023.08.18 val PER: 0.4493
2026-01-04 23:40:52,383: t15.2023.08.20 val PER: 0.4424
2026-01-04 23:40:52,383: t15.2023.08.25 val PER: 0.4262
2026-01-04 23:40:52,383: t15.2023.08.27 val PER: 0.5241
2026-01-04 23:40:52,383: t15.2023.09.01 val PER: 0.4131
2026-01-04 23:40:52,383: t15.2023.09.03 val PER: 0.5071
2026-01-04 23:40:52,383: t15.2023.09.24 val PER: 0.4393
2026-01-04 23:40:52,383: t15.2023.09.29 val PER: 0.4652
2026-01-04 23:40:52,383: t15.2023.10.01 val PER: 0.5310
2026-01-04 23:40:52,383: t15.2023.10.06 val PER: 0.4403
2026-01-04 23:40:52,384: t15.2023.10.08 val PER: 0.5399
2026-01-04 23:40:52,384: t15.2023.10.13 val PER: 0.5795
2026-01-04 23:40:52,384: t15.2023.10.15 val PER: 0.4964
2026-01-04 23:40:52,384: t15.2023.10.20 val PER: 0.4664
2026-01-04 23:40:52,384: t15.2023.10.22 val PER: 0.4633
2026-01-04 23:40:52,384: t15.2023.11.03 val PER: 0.5183
2026-01-04 23:40:52,384: t15.2023.11.04 val PER: 0.2628
2026-01-04 23:40:52,384: t15.2023.11.17 val PER: 0.3593
2026-01-04 23:40:52,384: t15.2023.11.19 val PER: 0.3333
2026-01-04 23:40:52,384: t15.2023.11.26 val PER: 0.5536
2026-01-04 23:40:52,384: t15.2023.12.03 val PER: 0.5116
2026-01-04 23:40:52,384: t15.2023.12.08 val PER: 0.5300
2026-01-04 23:40:52,384: t15.2023.12.10 val PER: 0.4625
2026-01-04 23:40:52,384: t15.2023.12.17 val PER: 0.5520
2026-01-04 23:40:52,384: t15.2023.12.29 val PER: 0.5477
2026-01-04 23:40:52,384: t15.2024.02.25 val PER: 0.4846
2026-01-04 23:40:52,385: t15.2024.03.08 val PER: 0.6088
2026-01-04 23:40:52,385: t15.2024.03.15 val PER: 0.5553
2026-01-04 23:40:52,385: t15.2024.03.17 val PER: 0.5105
2026-01-04 23:40:52,385: t15.2024.05.10 val PER: 0.5587
2026-01-04 23:40:52,385: t15.2024.06.14 val PER: 0.5189
2026-01-04 23:40:52,385: t15.2024.07.19 val PER: 0.6790
2026-01-04 23:40:52,385: t15.2024.07.21 val PER: 0.4690
2026-01-04 23:40:52,385: t15.2024.07.28 val PER: 0.5228
2026-01-04 23:40:52,385: t15.2025.01.10 val PER: 0.7479
2026-01-04 23:40:52,385: t15.2025.01.12 val PER: 0.5612
2026-01-04 23:40:52,385: t15.2025.03.14 val PER: 0.7441
2026-01-04 23:40:52,385: t15.2025.03.16 val PER: 0.5916
2026-01-04 23:40:52,385: t15.2025.03.30 val PER: 0.7322
2026-01-04 23:40:52,385: t15.2025.04.13 val PER: 0.5749
2026-01-04 23:40:52,386: New best val WER(1gram) 100.00% --> 88.32%
2026-01-04 23:40:52,386: Checkpointing model
2026-01-04 23:40:53,016: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:40:53,293: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_500
2026-01-04 23:41:02,370: Train batch 600: loss: 49.26 grad norm: 83.33 time: 0.078
2026-01-04 23:41:20,622: Train batch 800: loss: 41.56 grad norm: 87.68 time: 0.057
2026-01-04 23:41:38,643: Train batch 1000: loss: 42.58 grad norm: 74.69 time: 0.065
2026-01-04 23:41:38,643: Running test after training batch: 1000
2026-01-04 23:41:38,784: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:41:43,553: WER debug example
  GT : you can see the code at this point as well
  PR : hued ent ease utt owed it this boyde is while
2026-01-04 23:41:43,585: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke that us it
2026-01-04 23:41:45,379: Val batch 1000: PER (avg): 0.4107 CTC Loss (avg): 42.3573 WER(1gram): 82.23% (n=64) time: 6.736
2026-01-04 23:41:45,380: WER lens: avg_true_words=6.16 avg_pred_words=5.53 max_pred_words=12
2026-01-04 23:41:45,380: t15.2023.08.13 val PER: 0.3846
2026-01-04 23:41:45,380: t15.2023.08.18 val PER: 0.3487
2026-01-04 23:41:45,380: t15.2023.08.20 val PER: 0.3463
2026-01-04 23:41:45,380: t15.2023.08.25 val PER: 0.2937
2026-01-04 23:41:45,381: t15.2023.08.27 val PER: 0.4196
2026-01-04 23:41:45,381: t15.2023.09.01 val PER: 0.3076
2026-01-04 23:41:45,381: t15.2023.09.03 val PER: 0.4014
2026-01-04 23:41:45,381: t15.2023.09.24 val PER: 0.3204
2026-01-04 23:41:45,381: t15.2023.09.29 val PER: 0.3701
2026-01-04 23:41:45,381: t15.2023.10.01 val PER: 0.4003
2026-01-04 23:41:45,381: t15.2023.10.06 val PER: 0.3143
2026-01-04 23:41:45,381: t15.2023.10.08 val PER: 0.4547
2026-01-04 23:41:45,381: t15.2023.10.13 val PER: 0.4631
2026-01-04 23:41:45,381: t15.2023.10.15 val PER: 0.3843
2026-01-04 23:41:45,381: t15.2023.10.20 val PER: 0.3725
2026-01-04 23:41:45,381: t15.2023.10.22 val PER: 0.3575
2026-01-04 23:41:45,381: t15.2023.11.03 val PER: 0.4030
2026-01-04 23:41:45,382: t15.2023.11.04 val PER: 0.1536
2026-01-04 23:41:45,382: t15.2023.11.17 val PER: 0.2519
2026-01-04 23:41:45,382: t15.2023.11.19 val PER: 0.1956
2026-01-04 23:41:45,382: t15.2023.11.26 val PER: 0.4551
2026-01-04 23:41:45,382: t15.2023.12.03 val PER: 0.4065
2026-01-04 23:41:45,382: t15.2023.12.08 val PER: 0.4095
2026-01-04 23:41:45,382: t15.2023.12.10 val PER: 0.3601
2026-01-04 23:41:45,382: t15.2023.12.17 val PER: 0.4179
2026-01-04 23:41:45,382: t15.2023.12.29 val PER: 0.4084
2026-01-04 23:41:45,382: t15.2024.02.25 val PER: 0.3511
2026-01-04 23:41:45,382: t15.2024.03.08 val PER: 0.5135
2026-01-04 23:41:45,382: t15.2024.03.15 val PER: 0.4422
2026-01-04 23:41:45,382: t15.2024.03.17 val PER: 0.4045
2026-01-04 23:41:45,382: t15.2024.05.10 val PER: 0.4294
2026-01-04 23:41:45,382: t15.2024.06.14 val PER: 0.4006
2026-01-04 23:41:45,382: t15.2024.07.19 val PER: 0.5326
2026-01-04 23:41:45,382: t15.2024.07.21 val PER: 0.3731
2026-01-04 23:41:45,383: t15.2024.07.28 val PER: 0.4125
2026-01-04 23:41:45,383: t15.2025.01.10 val PER: 0.6088
2026-01-04 23:41:45,383: t15.2025.01.12 val PER: 0.4596
2026-01-04 23:41:45,383: t15.2025.03.14 val PER: 0.6450
2026-01-04 23:41:45,383: t15.2025.03.16 val PER: 0.4830
2026-01-04 23:41:45,383: t15.2025.03.30 val PER: 0.6494
2026-01-04 23:41:45,383: t15.2025.04.13 val PER: 0.5050
2026-01-04 23:41:45,384: New best val WER(1gram) 88.32% --> 82.23%
2026-01-04 23:41:45,385: Checkpointing model
2026-01-04 23:41:46,039: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:41:46,314: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_1000
2026-01-04 23:42:04,350: Train batch 1200: loss: 32.89 grad norm: 74.26 time: 0.068
2026-01-04 23:42:22,673: Train batch 1400: loss: 36.66 grad norm: 83.55 time: 0.061
2026-01-04 23:42:31,795: Running test after training batch: 1500
2026-01-04 23:42:31,904: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:42:36,804: WER debug example
  GT : you can see the code at this point as well
  PR : yule kent e the good it this boyde is wheel
2026-01-04 23:42:36,835: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that cost it
2026-01-04 23:42:38,399: Val batch 1500: PER (avg): 0.3769 CTC Loss (avg): 37.2137 WER(1gram): 76.65% (n=64) time: 6.603
2026-01-04 23:42:38,399: WER lens: avg_true_words=6.16 avg_pred_words=5.14 max_pred_words=11
2026-01-04 23:42:38,400: t15.2023.08.13 val PER: 0.3347
2026-01-04 23:42:38,400: t15.2023.08.18 val PER: 0.3068
2026-01-04 23:42:38,400: t15.2023.08.20 val PER: 0.3129
2026-01-04 23:42:38,400: t15.2023.08.25 val PER: 0.2620
2026-01-04 23:42:38,400: t15.2023.08.27 val PER: 0.3810
2026-01-04 23:42:38,400: t15.2023.09.01 val PER: 0.2711
2026-01-04 23:42:38,400: t15.2023.09.03 val PER: 0.3694
2026-01-04 23:42:38,401: t15.2023.09.24 val PER: 0.2949
2026-01-04 23:42:38,401: t15.2023.09.29 val PER: 0.3376
2026-01-04 23:42:38,401: t15.2023.10.01 val PER: 0.3910
2026-01-04 23:42:38,401: t15.2023.10.06 val PER: 0.2659
2026-01-04 23:42:38,401: t15.2023.10.08 val PER: 0.4357
2026-01-04 23:42:38,401: t15.2023.10.13 val PER: 0.4430
2026-01-04 23:42:38,401: t15.2023.10.15 val PER: 0.3566
2026-01-04 23:42:38,401: t15.2023.10.20 val PER: 0.3322
2026-01-04 23:42:38,401: t15.2023.10.22 val PER: 0.3073
2026-01-04 23:42:38,401: t15.2023.11.03 val PER: 0.3623
2026-01-04 23:42:38,401: t15.2023.11.04 val PER: 0.1160
2026-01-04 23:42:38,401: t15.2023.11.17 val PER: 0.2302
2026-01-04 23:42:38,401: t15.2023.11.19 val PER: 0.1796
2026-01-04 23:42:38,401: t15.2023.11.26 val PER: 0.4109
2026-01-04 23:42:38,401: t15.2023.12.03 val PER: 0.3771
2026-01-04 23:42:38,401: t15.2023.12.08 val PER: 0.3582
2026-01-04 23:42:38,402: t15.2023.12.10 val PER: 0.2957
2026-01-04 23:42:38,402: t15.2023.12.17 val PER: 0.3669
2026-01-04 23:42:38,402: t15.2023.12.29 val PER: 0.3782
2026-01-04 23:42:38,402: t15.2024.02.25 val PER: 0.3132
2026-01-04 23:42:38,402: t15.2024.03.08 val PER: 0.4410
2026-01-04 23:42:38,402: t15.2024.03.15 val PER: 0.4196
2026-01-04 23:42:38,402: t15.2024.03.17 val PER: 0.3717
2026-01-04 23:42:38,402: t15.2024.05.10 val PER: 0.3848
2026-01-04 23:42:38,402: t15.2024.06.14 val PER: 0.3959
2026-01-04 23:42:38,402: t15.2024.07.19 val PER: 0.5089
2026-01-04 23:42:38,402: t15.2024.07.21 val PER: 0.3497
2026-01-04 23:42:38,402: t15.2024.07.28 val PER: 0.3757
2026-01-04 23:42:38,402: t15.2025.01.10 val PER: 0.5964
2026-01-04 23:42:38,402: t15.2025.01.12 val PER: 0.4196
2026-01-04 23:42:38,402: t15.2025.03.14 val PER: 0.6095
2026-01-04 23:42:38,402: t15.2025.03.16 val PER: 0.4594
2026-01-04 23:42:38,402: t15.2025.03.30 val PER: 0.6207
2026-01-04 23:42:38,403: t15.2025.04.13 val PER: 0.4665
2026-01-04 23:42:38,404: New best val WER(1gram) 82.23% --> 76.65%
2026-01-04 23:42:38,404: Checkpointing model
2026-01-04 23:42:39,032: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:42:39,304: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_1500
2026-01-04 23:42:48,421: Train batch 1600: loss: 37.26 grad norm: 80.49 time: 0.064
2026-01-04 23:43:06,262: Train batch 1800: loss: 35.05 grad norm: 69.04 time: 0.088
2026-01-04 23:43:24,091: Train batch 2000: loss: 33.79 grad norm: 73.62 time: 0.066
2026-01-04 23:43:24,091: Running test after training batch: 2000
2026-01-04 23:43:24,227: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:43:28,966: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this boyde is will
2026-01-04 23:43:28,998: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap thus kos id
2026-01-04 23:43:30,563: Val batch 2000: PER (avg): 0.3281 CTC Loss (avg): 33.0543 WER(1gram): 72.34% (n=64) time: 6.471
2026-01-04 23:43:30,563: WER lens: avg_true_words=6.16 avg_pred_words=5.73 max_pred_words=12
2026-01-04 23:43:30,563: t15.2023.08.13 val PER: 0.2952
2026-01-04 23:43:30,563: t15.2023.08.18 val PER: 0.2640
2026-01-04 23:43:30,563: t15.2023.08.20 val PER: 0.2534
2026-01-04 23:43:30,564: t15.2023.08.25 val PER: 0.2229
2026-01-04 23:43:30,564: t15.2023.08.27 val PER: 0.3537
2026-01-04 23:43:30,564: t15.2023.09.01 val PER: 0.2240
2026-01-04 23:43:30,564: t15.2023.09.03 val PER: 0.3183
2026-01-04 23:43:30,564: t15.2023.09.24 val PER: 0.2621
2026-01-04 23:43:30,564: t15.2023.09.29 val PER: 0.2706
2026-01-04 23:43:30,564: t15.2023.10.01 val PER: 0.3243
2026-01-04 23:43:30,564: t15.2023.10.06 val PER: 0.2304
2026-01-04 23:43:30,564: t15.2023.10.08 val PER: 0.4005
2026-01-04 23:43:30,564: t15.2023.10.13 val PER: 0.3778
2026-01-04 23:43:30,564: t15.2023.10.15 val PER: 0.3092
2026-01-04 23:43:30,564: t15.2023.10.20 val PER: 0.2886
2026-01-04 23:43:30,564: t15.2023.10.22 val PER: 0.2606
2026-01-04 23:43:30,564: t15.2023.11.03 val PER: 0.3209
2026-01-04 23:43:30,564: t15.2023.11.04 val PER: 0.0887
2026-01-04 23:43:30,565: t15.2023.11.17 val PER: 0.1882
2026-01-04 23:43:30,565: t15.2023.11.19 val PER: 0.1377
2026-01-04 23:43:30,565: t15.2023.11.26 val PER: 0.3754
2026-01-04 23:43:30,565: t15.2023.12.03 val PER: 0.3204
2026-01-04 23:43:30,565: t15.2023.12.08 val PER: 0.3156
2026-01-04 23:43:30,565: t15.2023.12.10 val PER: 0.2523
2026-01-04 23:43:30,565: t15.2023.12.17 val PER: 0.3160
2026-01-04 23:43:30,565: t15.2023.12.29 val PER: 0.3274
2026-01-04 23:43:30,566: t15.2024.02.25 val PER: 0.2767
2026-01-04 23:43:30,566: t15.2024.03.08 val PER: 0.4026
2026-01-04 23:43:30,566: t15.2024.03.15 val PER: 0.3508
2026-01-04 23:43:30,566: t15.2024.03.17 val PER: 0.3403
2026-01-04 23:43:30,566: t15.2024.05.10 val PER: 0.3388
2026-01-04 23:43:30,566: t15.2024.06.14 val PER: 0.3502
2026-01-04 23:43:30,566: t15.2024.07.19 val PER: 0.4634
2026-01-04 23:43:30,566: t15.2024.07.21 val PER: 0.2931
2026-01-04 23:43:30,566: t15.2024.07.28 val PER: 0.3294
2026-01-04 23:43:30,566: t15.2025.01.10 val PER: 0.5331
2026-01-04 23:43:30,566: t15.2025.01.12 val PER: 0.3811
2026-01-04 23:43:30,566: t15.2025.03.14 val PER: 0.5266
2026-01-04 23:43:30,566: t15.2025.03.16 val PER: 0.4005
2026-01-04 23:43:30,566: t15.2025.03.30 val PER: 0.5586
2026-01-04 23:43:30,566: t15.2025.04.13 val PER: 0.4108
2026-01-04 23:43:30,567: New best val WER(1gram) 76.65% --> 72.34%
2026-01-04 23:43:30,567: Checkpointing model
2026-01-04 23:43:31,212: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:43:31,480: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_2000
2026-01-04 23:43:49,263: Train batch 2200: loss: 28.43 grad norm: 67.87 time: 0.060
2026-01-04 23:44:07,378: Train batch 2400: loss: 29.02 grad norm: 63.04 time: 0.051
2026-01-04 23:44:16,377: Running test after training batch: 2500
2026-01-04 23:44:16,589: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:44:21,449: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt sze the code at this point is will
2026-01-04 23:44:21,477: WER debug example
  GT : how does it keep the cost down
  PR : houde des it keep the cost it
2026-01-04 23:44:23,116: Val batch 2500: PER (avg): 0.3045 CTC Loss (avg): 30.1706 WER(1gram): 66.50% (n=64) time: 6.738
2026-01-04 23:44:23,116: WER lens: avg_true_words=6.16 avg_pred_words=5.66 max_pred_words=11
2026-01-04 23:44:23,116: t15.2023.08.13 val PER: 0.2879
2026-01-04 23:44:23,116: t15.2023.08.18 val PER: 0.2305
2026-01-04 23:44:23,117: t15.2023.08.20 val PER: 0.2335
2026-01-04 23:44:23,117: t15.2023.08.25 val PER: 0.2063
2026-01-04 23:44:23,117: t15.2023.08.27 val PER: 0.3441
2026-01-04 23:44:23,117: t15.2023.09.01 val PER: 0.2070
2026-01-04 23:44:23,117: t15.2023.09.03 val PER: 0.2993
2026-01-04 23:44:23,117: t15.2023.09.24 val PER: 0.2342
2026-01-04 23:44:23,117: t15.2023.09.29 val PER: 0.2623
2026-01-04 23:44:23,117: t15.2023.10.01 val PER: 0.3052
2026-01-04 23:44:23,117: t15.2023.10.06 val PER: 0.2174
2026-01-04 23:44:23,117: t15.2023.10.08 val PER: 0.3789
2026-01-04 23:44:23,117: t15.2023.10.13 val PER: 0.3437
2026-01-04 23:44:23,118: t15.2023.10.15 val PER: 0.2874
2026-01-04 23:44:23,118: t15.2023.10.20 val PER: 0.2685
2026-01-04 23:44:23,118: t15.2023.10.22 val PER: 0.2327
2026-01-04 23:44:23,118: t15.2023.11.03 val PER: 0.2897
2026-01-04 23:44:23,118: t15.2023.11.04 val PER: 0.0751
2026-01-04 23:44:23,118: t15.2023.11.17 val PER: 0.1477
2026-01-04 23:44:23,118: t15.2023.11.19 val PER: 0.1178
2026-01-04 23:44:23,118: t15.2023.11.26 val PER: 0.3471
2026-01-04 23:44:23,118: t15.2023.12.03 val PER: 0.2742
2026-01-04 23:44:23,118: t15.2023.12.08 val PER: 0.2763
2026-01-04 23:44:23,118: t15.2023.12.10 val PER: 0.2405
2026-01-04 23:44:23,119: t15.2023.12.17 val PER: 0.2786
2026-01-04 23:44:23,119: t15.2023.12.29 val PER: 0.3027
2026-01-04 23:44:23,119: t15.2024.02.25 val PER: 0.2416
2026-01-04 23:44:23,119: t15.2024.03.08 val PER: 0.3713
2026-01-04 23:44:23,119: t15.2024.03.15 val PER: 0.3477
2026-01-04 23:44:23,119: t15.2024.03.17 val PER: 0.3159
2026-01-04 23:44:23,119: t15.2024.05.10 val PER: 0.3165
2026-01-04 23:44:23,119: t15.2024.06.14 val PER: 0.3438
2026-01-04 23:44:23,119: t15.2024.07.19 val PER: 0.4417
2026-01-04 23:44:23,119: t15.2024.07.21 val PER: 0.2703
2026-01-04 23:44:23,119: t15.2024.07.28 val PER: 0.2963
2026-01-04 23:44:23,119: t15.2025.01.10 val PER: 0.5096
2026-01-04 23:44:23,119: t15.2025.01.12 val PER: 0.3649
2026-01-04 23:44:23,119: t15.2025.03.14 val PER: 0.5000
2026-01-04 23:44:23,119: t15.2025.03.16 val PER: 0.3678
2026-01-04 23:44:23,120: t15.2025.03.30 val PER: 0.5241
2026-01-04 23:44:23,120: t15.2025.04.13 val PER: 0.3923
2026-01-04 23:44:23,121: New best val WER(1gram) 72.34% --> 66.50%
2026-01-04 23:44:23,121: Checkpointing model
2026-01-04 23:44:23,739: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:44:24,011: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_2500
2026-01-04 23:44:33,163: Train batch 2600: loss: 35.44 grad norm: 87.91 time: 0.055
2026-01-04 23:44:51,545: Train batch 2800: loss: 25.43 grad norm: 68.77 time: 0.081
2026-01-04 23:45:10,084: Train batch 3000: loss: 31.36 grad norm: 73.54 time: 0.083
2026-01-04 23:45:10,084: Running test after training batch: 3000
2026-01-04 23:45:10,216: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:45:14,962: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-04 23:45:14,992: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp the cost get
2026-01-04 23:45:16,644: Val batch 3000: PER (avg): 0.2804 CTC Loss (avg): 27.8530 WER(1gram): 64.97% (n=64) time: 6.560
2026-01-04 23:45:16,645: WER lens: avg_true_words=6.16 avg_pred_words=5.88 max_pred_words=11
2026-01-04 23:45:16,645: t15.2023.08.13 val PER: 0.2547
2026-01-04 23:45:16,645: t15.2023.08.18 val PER: 0.2196
2026-01-04 23:45:16,645: t15.2023.08.20 val PER: 0.2176
2026-01-04 23:45:16,645: t15.2023.08.25 val PER: 0.1867
2026-01-04 23:45:16,645: t15.2023.08.27 val PER: 0.3039
2026-01-04 23:45:16,645: t15.2023.09.01 val PER: 0.1891
2026-01-04 23:45:16,645: t15.2023.09.03 val PER: 0.2850
2026-01-04 23:45:16,645: t15.2023.09.24 val PER: 0.2221
2026-01-04 23:45:16,646: t15.2023.09.29 val PER: 0.2297
2026-01-04 23:45:16,646: t15.2023.10.01 val PER: 0.2933
2026-01-04 23:45:16,646: t15.2023.10.06 val PER: 0.1938
2026-01-04 23:45:16,646: t15.2023.10.08 val PER: 0.3532
2026-01-04 23:45:16,646: t15.2023.10.13 val PER: 0.3375
2026-01-04 23:45:16,646: t15.2023.10.15 val PER: 0.2617
2026-01-04 23:45:16,646: t15.2023.10.20 val PER: 0.2517
2026-01-04 23:45:16,646: t15.2023.10.22 val PER: 0.2171
2026-01-04 23:45:16,646: t15.2023.11.03 val PER: 0.2673
2026-01-04 23:45:16,646: t15.2023.11.04 val PER: 0.0683
2026-01-04 23:45:16,646: t15.2023.11.17 val PER: 0.1306
2026-01-04 23:45:16,646: t15.2023.11.19 val PER: 0.1257
2026-01-04 23:45:16,646: t15.2023.11.26 val PER: 0.3087
2026-01-04 23:45:16,647: t15.2023.12.03 val PER: 0.2647
2026-01-04 23:45:16,647: t15.2023.12.08 val PER: 0.2590
2026-01-04 23:45:16,647: t15.2023.12.10 val PER: 0.2089
2026-01-04 23:45:16,647: t15.2023.12.17 val PER: 0.2744
2026-01-04 23:45:16,647: t15.2023.12.29 val PER: 0.2848
2026-01-04 23:45:16,647: t15.2024.02.25 val PER: 0.2472
2026-01-04 23:45:16,647: t15.2024.03.08 val PER: 0.3442
2026-01-04 23:45:16,647: t15.2024.03.15 val PER: 0.3302
2026-01-04 23:45:16,647: t15.2024.03.17 val PER: 0.2873
2026-01-04 23:45:16,647: t15.2024.05.10 val PER: 0.3001
2026-01-04 23:45:16,647: t15.2024.06.14 val PER: 0.2981
2026-01-04 23:45:16,647: t15.2024.07.19 val PER: 0.4001
2026-01-04 23:45:16,648: t15.2024.07.21 val PER: 0.2324
2026-01-04 23:45:16,648: t15.2024.07.28 val PER: 0.2721
2026-01-04 23:45:16,648: t15.2025.01.10 val PER: 0.4807
2026-01-04 23:45:16,648: t15.2025.01.12 val PER: 0.3149
2026-01-04 23:45:16,648: t15.2025.03.14 val PER: 0.4364
2026-01-04 23:45:16,648: t15.2025.03.16 val PER: 0.3312
2026-01-04 23:45:16,648: t15.2025.03.30 val PER: 0.4966
2026-01-04 23:45:16,648: t15.2025.04.13 val PER: 0.3466
2026-01-04 23:45:16,649: New best val WER(1gram) 66.50% --> 64.97%
2026-01-04 23:45:16,649: Checkpointing model
2026-01-04 23:45:17,291: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:45:17,559: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_3000
2026-01-04 23:45:35,932: Train batch 3200: loss: 26.29 grad norm: 66.02 time: 0.076
2026-01-04 23:45:54,153: Train batch 3400: loss: 18.66 grad norm: 56.89 time: 0.049
2026-01-04 23:46:03,388: Running test after training batch: 3500
2026-01-04 23:46:03,531: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:46:08,556: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the goede at this point is will
2026-01-04 23:46:08,586: WER debug example
  GT : how does it keep the cost down
  PR : out des it yip thus cussed get
2026-01-04 23:46:10,156: Val batch 3500: PER (avg): 0.2666 CTC Loss (avg): 26.5824 WER(1gram): 64.72% (n=64) time: 6.768
2026-01-04 23:46:10,157: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=11
2026-01-04 23:46:10,157: t15.2023.08.13 val PER: 0.2370
2026-01-04 23:46:10,157: t15.2023.08.18 val PER: 0.2070
2026-01-04 23:46:10,157: t15.2023.08.20 val PER: 0.2160
2026-01-04 23:46:10,157: t15.2023.08.25 val PER: 0.1792
2026-01-04 23:46:10,157: t15.2023.08.27 val PER: 0.2862
2026-01-04 23:46:10,157: t15.2023.09.01 val PER: 0.1859
2026-01-04 23:46:10,157: t15.2023.09.03 val PER: 0.2530
2026-01-04 23:46:10,158: t15.2023.09.24 val PER: 0.2148
2026-01-04 23:46:10,158: t15.2023.09.29 val PER: 0.2246
2026-01-04 23:46:10,158: t15.2023.10.01 val PER: 0.2794
2026-01-04 23:46:10,158: t15.2023.10.06 val PER: 0.1776
2026-01-04 23:46:10,158: t15.2023.10.08 val PER: 0.3383
2026-01-04 23:46:10,158: t15.2023.10.13 val PER: 0.3111
2026-01-04 23:46:10,158: t15.2023.10.15 val PER: 0.2505
2026-01-04 23:46:10,158: t15.2023.10.20 val PER: 0.2483
2026-01-04 23:46:10,158: t15.2023.10.22 val PER: 0.2127
2026-01-04 23:46:10,158: t15.2023.11.03 val PER: 0.2578
2026-01-04 23:46:10,159: t15.2023.11.04 val PER: 0.0853
2026-01-04 23:46:10,159: t15.2023.11.17 val PER: 0.1182
2026-01-04 23:46:10,159: t15.2023.11.19 val PER: 0.1078
2026-01-04 23:46:10,159: t15.2023.11.26 val PER: 0.2790
2026-01-04 23:46:10,159: t15.2023.12.03 val PER: 0.2384
2026-01-04 23:46:10,159: t15.2023.12.08 val PER: 0.2437
2026-01-04 23:46:10,159: t15.2023.12.10 val PER: 0.2037
2026-01-04 23:46:10,159: t15.2023.12.17 val PER: 0.2620
2026-01-04 23:46:10,159: t15.2023.12.29 val PER: 0.2594
2026-01-04 23:46:10,159: t15.2024.02.25 val PER: 0.2191
2026-01-04 23:46:10,159: t15.2024.03.08 val PER: 0.3300
2026-01-04 23:46:10,159: t15.2024.03.15 val PER: 0.3139
2026-01-04 23:46:10,159: t15.2024.03.17 val PER: 0.2713
2026-01-04 23:46:10,160: t15.2024.05.10 val PER: 0.2675
2026-01-04 23:46:10,160: t15.2024.06.14 val PER: 0.2855
2026-01-04 23:46:10,160: t15.2024.07.19 val PER: 0.3935
2026-01-04 23:46:10,160: t15.2024.07.21 val PER: 0.2228
2026-01-04 23:46:10,160: t15.2024.07.28 val PER: 0.2779
2026-01-04 23:46:10,160: t15.2025.01.10 val PER: 0.4656
2026-01-04 23:46:10,160: t15.2025.01.12 val PER: 0.2864
2026-01-04 23:46:10,160: t15.2025.03.14 val PER: 0.4393
2026-01-04 23:46:10,160: t15.2025.03.16 val PER: 0.3325
2026-01-04 23:46:10,160: t15.2025.03.30 val PER: 0.4563
2026-01-04 23:46:10,160: t15.2025.04.13 val PER: 0.3481
2026-01-04 23:46:10,161: New best val WER(1gram) 64.97% --> 64.72%
2026-01-04 23:46:10,161: Checkpointing model
2026-01-04 23:46:10,779: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:46:11,051: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_3500
2026-01-04 23:46:20,192: Train batch 3600: loss: 22.66 grad norm: 64.65 time: 0.067
2026-01-04 23:46:38,099: Train batch 3800: loss: 25.69 grad norm: 71.59 time: 0.067
2026-01-04 23:46:56,395: Train batch 4000: loss: 19.64 grad norm: 55.30 time: 0.056
2026-01-04 23:46:56,395: Running test after training batch: 4000
2026-01-04 23:46:56,559: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:47:01,306: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 23:47:01,335: WER debug example
  GT : how does it keep the cost down
  PR : out dust it keep thus cost get
2026-01-04 23:47:02,969: Val batch 4000: PER (avg): 0.2500 CTC Loss (avg): 24.5639 WER(1gram): 63.71% (n=64) time: 6.573
2026-01-04 23:47:02,969: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-04 23:47:02,969: t15.2023.08.13 val PER: 0.2214
2026-01-04 23:47:02,969: t15.2023.08.18 val PER: 0.2003
2026-01-04 23:47:02,970: t15.2023.08.20 val PER: 0.2049
2026-01-04 23:47:02,970: t15.2023.08.25 val PER: 0.1581
2026-01-04 23:47:02,970: t15.2023.08.27 val PER: 0.2926
2026-01-04 23:47:02,970: t15.2023.09.01 val PER: 0.1680
2026-01-04 23:47:02,970: t15.2023.09.03 val PER: 0.2375
2026-01-04 23:47:02,970: t15.2023.09.24 val PER: 0.2027
2026-01-04 23:47:02,970: t15.2023.09.29 val PER: 0.2004
2026-01-04 23:47:02,970: t15.2023.10.01 val PER: 0.2550
2026-01-04 23:47:02,970: t15.2023.10.06 val PER: 0.1712
2026-01-04 23:47:02,971: t15.2023.10.08 val PER: 0.3329
2026-01-04 23:47:02,971: t15.2023.10.13 val PER: 0.3026
2026-01-04 23:47:02,971: t15.2023.10.15 val PER: 0.2432
2026-01-04 23:47:02,971: t15.2023.10.20 val PER: 0.2584
2026-01-04 23:47:02,971: t15.2023.10.22 val PER: 0.1982
2026-01-04 23:47:02,971: t15.2023.11.03 val PER: 0.2402
2026-01-04 23:47:02,971: t15.2023.11.04 val PER: 0.0648
2026-01-04 23:47:02,971: t15.2023.11.17 val PER: 0.1135
2026-01-04 23:47:02,971: t15.2023.11.19 val PER: 0.1058
2026-01-04 23:47:02,971: t15.2023.11.26 val PER: 0.2630
2026-01-04 23:47:02,971: t15.2023.12.03 val PER: 0.2290
2026-01-04 23:47:02,971: t15.2023.12.08 val PER: 0.2124
2026-01-04 23:47:02,972: t15.2023.12.10 val PER: 0.1761
2026-01-04 23:47:02,972: t15.2023.12.17 val PER: 0.2453
2026-01-04 23:47:02,972: t15.2023.12.29 val PER: 0.2588
2026-01-04 23:47:02,972: t15.2024.02.25 val PER: 0.2065
2026-01-04 23:47:02,972: t15.2024.03.08 val PER: 0.3314
2026-01-04 23:47:02,972: t15.2024.03.15 val PER: 0.2983
2026-01-04 23:47:02,972: t15.2024.03.17 val PER: 0.2643
2026-01-04 23:47:02,972: t15.2024.05.10 val PER: 0.2704
2026-01-04 23:47:02,972: t15.2024.06.14 val PER: 0.2681
2026-01-04 23:47:02,972: t15.2024.07.19 val PER: 0.3599
2026-01-04 23:47:02,972: t15.2024.07.21 val PER: 0.1883
2026-01-04 23:47:02,972: t15.2024.07.28 val PER: 0.2449
2026-01-04 23:47:02,972: t15.2025.01.10 val PER: 0.4242
2026-01-04 23:47:02,973: t15.2025.01.12 val PER: 0.2787
2026-01-04 23:47:02,973: t15.2025.03.14 val PER: 0.4201
2026-01-04 23:47:02,973: t15.2025.03.16 val PER: 0.3089
2026-01-04 23:47:02,973: t15.2025.03.30 val PER: 0.4103
2026-01-04 23:47:02,973: t15.2025.04.13 val PER: 0.3210
2026-01-04 23:47:02,974: New best val WER(1gram) 64.72% --> 63.71%
2026-01-04 23:47:02,974: Checkpointing model
2026-01-04 23:47:03,612: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:47:03,882: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_4000
2026-01-04 23:47:22,287: Train batch 4200: loss: 22.63 grad norm: 61.21 time: 0.080
2026-01-04 23:47:40,526: Train batch 4400: loss: 17.13 grad norm: 54.68 time: 0.066
2026-01-04 23:47:49,621: Running test after training batch: 4500
2026-01-04 23:47:49,720: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:47:54,748: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 23:47:54,778: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it yip thus cost get
2026-01-04 23:47:56,314: Val batch 4500: PER (avg): 0.2380 CTC Loss (avg): 23.1776 WER(1gram): 61.93% (n=64) time: 6.692
2026-01-04 23:47:56,315: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-04 23:47:56,315: t15.2023.08.13 val PER: 0.2131
2026-01-04 23:47:56,315: t15.2023.08.18 val PER: 0.1827
2026-01-04 23:47:56,315: t15.2023.08.20 val PER: 0.1954
2026-01-04 23:47:56,315: t15.2023.08.25 val PER: 0.1446
2026-01-04 23:47:56,315: t15.2023.08.27 val PER: 0.2540
2026-01-04 23:47:56,315: t15.2023.09.01 val PER: 0.1510
2026-01-04 23:47:56,315: t15.2023.09.03 val PER: 0.2387
2026-01-04 23:47:56,315: t15.2023.09.24 val PER: 0.1833
2026-01-04 23:47:56,316: t15.2023.09.29 val PER: 0.1978
2026-01-04 23:47:56,316: t15.2023.10.01 val PER: 0.2616
2026-01-04 23:47:56,316: t15.2023.10.06 val PER: 0.1582
2026-01-04 23:47:56,316: t15.2023.10.08 val PER: 0.3180
2026-01-04 23:47:56,316: t15.2023.10.13 val PER: 0.3080
2026-01-04 23:47:56,316: t15.2023.10.15 val PER: 0.2228
2026-01-04 23:47:56,316: t15.2023.10.20 val PER: 0.2383
2026-01-04 23:47:56,316: t15.2023.10.22 val PER: 0.1860
2026-01-04 23:47:56,316: t15.2023.11.03 val PER: 0.2408
2026-01-04 23:47:56,316: t15.2023.11.04 val PER: 0.0614
2026-01-04 23:47:56,316: t15.2023.11.17 val PER: 0.0995
2026-01-04 23:47:56,316: t15.2023.11.19 val PER: 0.0958
2026-01-04 23:47:56,317: t15.2023.11.26 val PER: 0.2623
2026-01-04 23:47:56,317: t15.2023.12.03 val PER: 0.2038
2026-01-04 23:47:56,317: t15.2023.12.08 val PER: 0.2117
2026-01-04 23:47:56,317: t15.2023.12.10 val PER: 0.1840
2026-01-04 23:47:56,317: t15.2023.12.17 val PER: 0.2266
2026-01-04 23:47:56,317: t15.2023.12.29 val PER: 0.2402
2026-01-04 23:47:56,317: t15.2024.02.25 val PER: 0.1966
2026-01-04 23:47:56,317: t15.2024.03.08 val PER: 0.3158
2026-01-04 23:47:56,317: t15.2024.03.15 val PER: 0.2908
2026-01-04 23:47:56,317: t15.2024.03.17 val PER: 0.2336
2026-01-04 23:47:56,317: t15.2024.05.10 val PER: 0.2437
2026-01-04 23:47:56,317: t15.2024.06.14 val PER: 0.2476
2026-01-04 23:47:56,317: t15.2024.07.19 val PER: 0.3441
2026-01-04 23:47:56,317: t15.2024.07.21 val PER: 0.1821
2026-01-04 23:47:56,317: t15.2024.07.28 val PER: 0.2213
2026-01-04 23:47:56,317: t15.2025.01.10 val PER: 0.4091
2026-01-04 23:47:56,317: t15.2025.01.12 val PER: 0.2640
2026-01-04 23:47:56,318: t15.2025.03.14 val PER: 0.3964
2026-01-04 23:47:56,318: t15.2025.03.16 val PER: 0.2919
2026-01-04 23:47:56,318: t15.2025.03.30 val PER: 0.4034
2026-01-04 23:47:56,318: t15.2025.04.13 val PER: 0.3024
2026-01-04 23:47:56,319: New best val WER(1gram) 63.71% --> 61.93%
2026-01-04 23:47:56,319: Checkpointing model
2026-01-04 23:47:56,937: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:47:57,213: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_4500
2026-01-04 23:48:06,651: Train batch 4600: loss: 19.62 grad norm: 62.73 time: 0.063
2026-01-04 23:48:25,026: Train batch 4800: loss: 13.80 grad norm: 54.60 time: 0.064
2026-01-04 23:48:43,261: Train batch 5000: loss: 30.79 grad norm: 81.43 time: 0.064
2026-01-04 23:48:43,261: Running test after training batch: 5000
2026-01-04 23:48:43,420: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:48:48,186: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 23:48:48,214: WER debug example
  GT : how does it keep the cost down
  PR : how dust it heap the cost get
2026-01-04 23:48:49,810: Val batch 5000: PER (avg): 0.2243 CTC Loss (avg): 22.0034 WER(1gram): 60.91% (n=64) time: 6.548
2026-01-04 23:48:49,810: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-04 23:48:49,811: t15.2023.08.13 val PER: 0.1913
2026-01-04 23:48:49,811: t15.2023.08.18 val PER: 0.1735
2026-01-04 23:48:49,811: t15.2023.08.20 val PER: 0.1763
2026-01-04 23:48:49,811: t15.2023.08.25 val PER: 0.1250
2026-01-04 23:48:49,811: t15.2023.08.27 val PER: 0.2363
2026-01-04 23:48:49,811: t15.2023.09.01 val PER: 0.1380
2026-01-04 23:48:49,811: t15.2023.09.03 val PER: 0.2197
2026-01-04 23:48:49,811: t15.2023.09.24 val PER: 0.1820
2026-01-04 23:48:49,811: t15.2023.09.29 val PER: 0.1857
2026-01-04 23:48:49,811: t15.2023.10.01 val PER: 0.2398
2026-01-04 23:48:49,811: t15.2023.10.06 val PER: 0.1518
2026-01-04 23:48:49,811: t15.2023.10.08 val PER: 0.3072
2026-01-04 23:48:49,811: t15.2023.10.13 val PER: 0.2832
2026-01-04 23:48:49,811: t15.2023.10.15 val PER: 0.2215
2026-01-04 23:48:49,812: t15.2023.10.20 val PER: 0.2349
2026-01-04 23:48:49,812: t15.2023.10.22 val PER: 0.1759
2026-01-04 23:48:49,812: t15.2023.11.03 val PER: 0.2259
2026-01-04 23:48:49,812: t15.2023.11.04 val PER: 0.0614
2026-01-04 23:48:49,812: t15.2023.11.17 val PER: 0.0778
2026-01-04 23:48:49,812: t15.2023.11.19 val PER: 0.0699
2026-01-04 23:48:49,812: t15.2023.11.26 val PER: 0.2268
2026-01-04 23:48:49,812: t15.2023.12.03 val PER: 0.1985
2026-01-04 23:48:49,812: t15.2023.12.08 val PER: 0.1924
2026-01-04 23:48:49,812: t15.2023.12.10 val PER: 0.1564
2026-01-04 23:48:49,813: t15.2023.12.17 val PER: 0.2297
2026-01-04 23:48:49,813: t15.2023.12.29 val PER: 0.2286
2026-01-04 23:48:49,813: t15.2024.02.25 val PER: 0.1910
2026-01-04 23:48:49,813: t15.2024.03.08 val PER: 0.3001
2026-01-04 23:48:49,813: t15.2024.03.15 val PER: 0.2808
2026-01-04 23:48:49,813: t15.2024.03.17 val PER: 0.2343
2026-01-04 23:48:49,813: t15.2024.05.10 val PER: 0.2422
2026-01-04 23:48:49,813: t15.2024.06.14 val PER: 0.2539
2026-01-04 23:48:49,813: t15.2024.07.19 val PER: 0.3217
2026-01-04 23:48:49,813: t15.2024.07.21 val PER: 0.1752
2026-01-04 23:48:49,813: t15.2024.07.28 val PER: 0.2140
2026-01-04 23:48:49,813: t15.2025.01.10 val PER: 0.3705
2026-01-04 23:48:49,813: t15.2025.01.12 val PER: 0.2371
2026-01-04 23:48:49,813: t15.2025.03.14 val PER: 0.3876
2026-01-04 23:48:49,813: t15.2025.03.16 val PER: 0.2657
2026-01-04 23:48:49,814: t15.2025.03.30 val PER: 0.4000
2026-01-04 23:48:49,814: t15.2025.04.13 val PER: 0.2953
2026-01-04 23:48:49,814: New best val WER(1gram) 61.93% --> 60.91%
2026-01-04 23:48:49,814: Checkpointing model
2026-01-04 23:48:50,473: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:48:50,763: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_5000
2026-01-04 23:49:09,244: Train batch 5200: loss: 16.34 grad norm: 56.98 time: 0.052
2026-01-04 23:49:27,651: Train batch 5400: loss: 17.55 grad norm: 60.55 time: 0.068
2026-01-04 23:49:36,794: Running test after training batch: 5500
2026-01-04 23:49:36,971: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:49:41,752: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 23:49:41,780: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost tet
2026-01-04 23:49:43,355: Val batch 5500: PER (avg): 0.2148 CTC Loss (avg): 21.0542 WER(1gram): 56.85% (n=64) time: 6.561
2026-01-04 23:49:43,356: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-04 23:49:43,356: t15.2023.08.13 val PER: 0.1819
2026-01-04 23:49:43,356: t15.2023.08.18 val PER: 0.1593
2026-01-04 23:49:43,356: t15.2023.08.20 val PER: 0.1700
2026-01-04 23:49:43,356: t15.2023.08.25 val PER: 0.1190
2026-01-04 23:49:43,356: t15.2023.08.27 val PER: 0.2428
2026-01-04 23:49:43,356: t15.2023.09.01 val PER: 0.1242
2026-01-04 23:49:43,356: t15.2023.09.03 val PER: 0.2209
2026-01-04 23:49:43,356: t15.2023.09.24 val PER: 0.1772
2026-01-04 23:49:43,356: t15.2023.09.29 val PER: 0.1729
2026-01-04 23:49:43,356: t15.2023.10.01 val PER: 0.2292
2026-01-04 23:49:43,357: t15.2023.10.06 val PER: 0.1442
2026-01-04 23:49:43,357: t15.2023.10.08 val PER: 0.2936
2026-01-04 23:49:43,357: t15.2023.10.13 val PER: 0.2816
2026-01-04 23:49:43,357: t15.2023.10.15 val PER: 0.2030
2026-01-04 23:49:43,357: t15.2023.10.20 val PER: 0.2215
2026-01-04 23:49:43,357: t15.2023.10.22 val PER: 0.1670
2026-01-04 23:49:43,357: t15.2023.11.03 val PER: 0.2232
2026-01-04 23:49:43,357: t15.2023.11.04 val PER: 0.0683
2026-01-04 23:49:43,357: t15.2023.11.17 val PER: 0.0840
2026-01-04 23:49:43,357: t15.2023.11.19 val PER: 0.0679
2026-01-04 23:49:43,357: t15.2023.11.26 val PER: 0.2210
2026-01-04 23:49:43,357: t15.2023.12.03 val PER: 0.1838
2026-01-04 23:49:43,357: t15.2023.12.08 val PER: 0.1891
2026-01-04 23:49:43,357: t15.2023.12.10 val PER: 0.1537
2026-01-04 23:49:43,357: t15.2023.12.17 val PER: 0.2173
2026-01-04 23:49:43,357: t15.2023.12.29 val PER: 0.2148
2026-01-04 23:49:43,357: t15.2024.02.25 val PER: 0.1812
2026-01-04 23:49:43,358: t15.2024.03.08 val PER: 0.2987
2026-01-04 23:49:43,358: t15.2024.03.15 val PER: 0.2577
2026-01-04 23:49:43,358: t15.2024.03.17 val PER: 0.2169
2026-01-04 23:49:43,358: t15.2024.05.10 val PER: 0.2363
2026-01-04 23:49:43,358: t15.2024.06.14 val PER: 0.2429
2026-01-04 23:49:43,358: t15.2024.07.19 val PER: 0.3184
2026-01-04 23:49:43,358: t15.2024.07.21 val PER: 0.1566
2026-01-04 23:49:43,358: t15.2024.07.28 val PER: 0.2029
2026-01-04 23:49:43,358: t15.2025.01.10 val PER: 0.3843
2026-01-04 23:49:43,358: t15.2025.01.12 val PER: 0.2340
2026-01-04 23:49:43,358: t15.2025.03.14 val PER: 0.3565
2026-01-04 23:49:43,358: t15.2025.03.16 val PER: 0.2657
2026-01-04 23:49:43,358: t15.2025.03.30 val PER: 0.3586
2026-01-04 23:49:43,358: t15.2025.04.13 val PER: 0.2853
2026-01-04 23:49:43,359: New best val WER(1gram) 60.91% --> 56.85%
2026-01-04 23:49:43,359: Checkpointing model
2026-01-04 23:49:43,971: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:49:44,239: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_5500
2026-01-04 23:49:53,476: Train batch 5600: loss: 19.73 grad norm: 66.08 time: 0.062
2026-01-04 23:50:12,092: Train batch 5800: loss: 13.16 grad norm: 54.97 time: 0.083
2026-01-04 23:50:30,843: Train batch 6000: loss: 14.13 grad norm: 53.43 time: 0.049
2026-01-04 23:50:30,843: Running test after training batch: 6000
2026-01-04 23:50:31,048: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:50:35,805: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 23:50:35,834: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost nett
2026-01-04 23:50:37,446: Val batch 6000: PER (avg): 0.2110 CTC Loss (avg): 20.7296 WER(1gram): 56.35% (n=64) time: 6.603
2026-01-04 23:50:37,447: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 23:50:37,447: t15.2023.08.13 val PER: 0.1694
2026-01-04 23:50:37,447: t15.2023.08.18 val PER: 0.1685
2026-01-04 23:50:37,447: t15.2023.08.20 val PER: 0.1716
2026-01-04 23:50:37,447: t15.2023.08.25 val PER: 0.1190
2026-01-04 23:50:37,447: t15.2023.08.27 val PER: 0.2363
2026-01-04 23:50:37,447: t15.2023.09.01 val PER: 0.1299
2026-01-04 23:50:37,447: t15.2023.09.03 val PER: 0.2126
2026-01-04 23:50:37,447: t15.2023.09.24 val PER: 0.1650
2026-01-04 23:50:37,447: t15.2023.09.29 val PER: 0.1742
2026-01-04 23:50:37,447: t15.2023.10.01 val PER: 0.2219
2026-01-04 23:50:37,448: t15.2023.10.06 val PER: 0.1367
2026-01-04 23:50:37,448: t15.2023.10.08 val PER: 0.2774
2026-01-04 23:50:37,448: t15.2023.10.13 val PER: 0.2762
2026-01-04 23:50:37,448: t15.2023.10.15 val PER: 0.2116
2026-01-04 23:50:37,448: t15.2023.10.20 val PER: 0.2282
2026-01-04 23:50:37,448: t15.2023.10.22 val PER: 0.1626
2026-01-04 23:50:37,448: t15.2023.11.03 val PER: 0.2259
2026-01-04 23:50:37,448: t15.2023.11.04 val PER: 0.0648
2026-01-04 23:50:37,448: t15.2023.11.17 val PER: 0.0684
2026-01-04 23:50:37,448: t15.2023.11.19 val PER: 0.0838
2026-01-04 23:50:37,448: t15.2023.11.26 val PER: 0.2159
2026-01-04 23:50:37,448: t15.2023.12.03 val PER: 0.1733
2026-01-04 23:50:37,449: t15.2023.12.08 val PER: 0.1871
2026-01-04 23:50:37,449: t15.2023.12.10 val PER: 0.1537
2026-01-04 23:50:37,449: t15.2023.12.17 val PER: 0.2037
2026-01-04 23:50:37,449: t15.2023.12.29 val PER: 0.2128
2026-01-04 23:50:37,449: t15.2024.02.25 val PER: 0.1601
2026-01-04 23:50:37,449: t15.2024.03.08 val PER: 0.2916
2026-01-04 23:50:37,449: t15.2024.03.15 val PER: 0.2670
2026-01-04 23:50:37,449: t15.2024.03.17 val PER: 0.2029
2026-01-04 23:50:37,449: t15.2024.05.10 val PER: 0.2169
2026-01-04 23:50:37,449: t15.2024.06.14 val PER: 0.2192
2026-01-04 23:50:37,449: t15.2024.07.19 val PER: 0.3125
2026-01-04 23:50:37,449: t15.2024.07.21 val PER: 0.1607
2026-01-04 23:50:37,449: t15.2024.07.28 val PER: 0.2022
2026-01-04 23:50:37,449: t15.2025.01.10 val PER: 0.3705
2026-01-04 23:50:37,449: t15.2025.01.12 val PER: 0.2202
2026-01-04 23:50:37,449: t15.2025.03.14 val PER: 0.3772
2026-01-04 23:50:37,450: t15.2025.03.16 val PER: 0.2618
2026-01-04 23:50:37,450: t15.2025.03.30 val PER: 0.3586
2026-01-04 23:50:37,450: t15.2025.04.13 val PER: 0.2682
2026-01-04 23:50:37,450: New best val WER(1gram) 56.85% --> 56.35%
2026-01-04 23:50:37,450: Checkpointing model
2026-01-04 23:50:38,080: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:50:38,351: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_6000
2026-01-04 23:50:56,271: Train batch 6200: loss: 16.49 grad norm: 59.36 time: 0.070
2026-01-04 23:51:14,268: Train batch 6400: loss: 18.74 grad norm: 67.14 time: 0.062
2026-01-04 23:51:23,406: Running test after training batch: 6500
2026-01-04 23:51:23,503: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:51:28,229: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 23:51:28,257: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost sent
2026-01-04 23:51:29,843: Val batch 6500: PER (avg): 0.2039 CTC Loss (avg): 19.9951 WER(1gram): 53.55% (n=64) time: 6.436
2026-01-04 23:51:29,843: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 23:51:29,843: t15.2023.08.13 val PER: 0.1809
2026-01-04 23:51:29,843: t15.2023.08.18 val PER: 0.1500
2026-01-04 23:51:29,844: t15.2023.08.20 val PER: 0.1573
2026-01-04 23:51:29,844: t15.2023.08.25 val PER: 0.1130
2026-01-04 23:51:29,844: t15.2023.08.27 val PER: 0.2251
2026-01-04 23:51:29,844: t15.2023.09.01 val PER: 0.1250
2026-01-04 23:51:29,844: t15.2023.09.03 val PER: 0.2126
2026-01-04 23:51:29,844: t15.2023.09.24 val PER: 0.1650
2026-01-04 23:51:29,844: t15.2023.09.29 val PER: 0.1704
2026-01-04 23:51:29,844: t15.2023.10.01 val PER: 0.2219
2026-01-04 23:51:29,844: t15.2023.10.06 val PER: 0.1292
2026-01-04 23:51:29,844: t15.2023.10.08 val PER: 0.2923
2026-01-04 23:51:29,844: t15.2023.10.13 val PER: 0.2653
2026-01-04 23:51:29,845: t15.2023.10.15 val PER: 0.2057
2026-01-04 23:51:29,845: t15.2023.10.20 val PER: 0.2215
2026-01-04 23:51:29,845: t15.2023.10.22 val PER: 0.1581
2026-01-04 23:51:29,845: t15.2023.11.03 val PER: 0.2205
2026-01-04 23:51:29,845: t15.2023.11.04 val PER: 0.0410
2026-01-04 23:51:29,845: t15.2023.11.17 val PER: 0.0591
2026-01-04 23:51:29,845: t15.2023.11.19 val PER: 0.0739
2026-01-04 23:51:29,845: t15.2023.11.26 val PER: 0.2007
2026-01-04 23:51:29,845: t15.2023.12.03 val PER: 0.1723
2026-01-04 23:51:29,845: t15.2023.12.08 val PER: 0.1664
2026-01-04 23:51:29,845: t15.2023.12.10 val PER: 0.1472
2026-01-04 23:51:29,845: t15.2023.12.17 val PER: 0.1944
2026-01-04 23:51:29,845: t15.2023.12.29 val PER: 0.1984
2026-01-04 23:51:29,846: t15.2024.02.25 val PER: 0.1826
2026-01-04 23:51:29,846: t15.2024.03.08 val PER: 0.2916
2026-01-04 23:51:29,846: t15.2024.03.15 val PER: 0.2514
2026-01-04 23:51:29,846: t15.2024.03.17 val PER: 0.1939
2026-01-04 23:51:29,846: t15.2024.05.10 val PER: 0.2199
2026-01-04 23:51:29,846: t15.2024.06.14 val PER: 0.2066
2026-01-04 23:51:29,846: t15.2024.07.19 val PER: 0.3072
2026-01-04 23:51:29,846: t15.2024.07.21 val PER: 0.1483
2026-01-04 23:51:29,846: t15.2024.07.28 val PER: 0.1919
2026-01-04 23:51:29,846: t15.2025.01.10 val PER: 0.3636
2026-01-04 23:51:29,846: t15.2025.01.12 val PER: 0.2094
2026-01-04 23:51:29,846: t15.2025.03.14 val PER: 0.3935
2026-01-04 23:51:29,846: t15.2025.03.16 val PER: 0.2304
2026-01-04 23:51:29,846: t15.2025.03.30 val PER: 0.3586
2026-01-04 23:51:29,846: t15.2025.04.13 val PER: 0.2653
2026-01-04 23:51:29,847: New best val WER(1gram) 56.35% --> 53.55%
2026-01-04 23:51:29,847: Checkpointing model
2026-01-04 23:51:30,450: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:51:30,716: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_6500
2026-01-04 23:51:39,654: Train batch 6600: loss: 12.14 grad norm: 50.92 time: 0.045
2026-01-04 23:51:57,706: Train batch 6800: loss: 16.02 grad norm: 58.72 time: 0.048
2026-01-04 23:52:15,790: Train batch 7000: loss: 17.05 grad norm: 62.14 time: 0.061
2026-01-04 23:52:15,791: Running test after training batch: 7000
2026-01-04 23:52:15,957: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:52:20,891: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:52:20,920: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost nett
2026-01-04 23:52:22,553: Val batch 7000: PER (avg): 0.1953 CTC Loss (avg): 19.1778 WER(1gram): 51.52% (n=64) time: 6.762
2026-01-04 23:52:22,554: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 23:52:22,554: t15.2023.08.13 val PER: 0.1622
2026-01-04 23:52:22,554: t15.2023.08.18 val PER: 0.1408
2026-01-04 23:52:22,554: t15.2023.08.20 val PER: 0.1549
2026-01-04 23:52:22,554: t15.2023.08.25 val PER: 0.1024
2026-01-04 23:52:22,554: t15.2023.08.27 val PER: 0.2138
2026-01-04 23:52:22,554: t15.2023.09.01 val PER: 0.1104
2026-01-04 23:52:22,554: t15.2023.09.03 val PER: 0.1829
2026-01-04 23:52:22,554: t15.2023.09.24 val PER: 0.1505
2026-01-04 23:52:22,554: t15.2023.09.29 val PER: 0.1710
2026-01-04 23:52:22,554: t15.2023.10.01 val PER: 0.2048
2026-01-04 23:52:22,555: t15.2023.10.06 val PER: 0.1087
2026-01-04 23:52:22,555: t15.2023.10.08 val PER: 0.2828
2026-01-04 23:52:22,555: t15.2023.10.13 val PER: 0.2583
2026-01-04 23:52:22,555: t15.2023.10.15 val PER: 0.1997
2026-01-04 23:52:22,555: t15.2023.10.20 val PER: 0.1980
2026-01-04 23:52:22,555: t15.2023.10.22 val PER: 0.1414
2026-01-04 23:52:22,555: t15.2023.11.03 val PER: 0.2028
2026-01-04 23:52:22,555: t15.2023.11.04 val PER: 0.0444
2026-01-04 23:52:22,555: t15.2023.11.17 val PER: 0.0607
2026-01-04 23:52:22,555: t15.2023.11.19 val PER: 0.0599
2026-01-04 23:52:22,555: t15.2023.11.26 val PER: 0.1899
2026-01-04 23:52:22,555: t15.2023.12.03 val PER: 0.1639
2026-01-04 23:52:22,555: t15.2023.12.08 val PER: 0.1565
2026-01-04 23:52:22,555: t15.2023.12.10 val PER: 0.1393
2026-01-04 23:52:22,556: t15.2023.12.17 val PER: 0.1715
2026-01-04 23:52:22,556: t15.2023.12.29 val PER: 0.2032
2026-01-04 23:52:22,556: t15.2024.02.25 val PER: 0.1643
2026-01-04 23:52:22,556: t15.2024.03.08 val PER: 0.2802
2026-01-04 23:52:22,556: t15.2024.03.15 val PER: 0.2445
2026-01-04 23:52:22,556: t15.2024.03.17 val PER: 0.2008
2026-01-04 23:52:22,556: t15.2024.05.10 val PER: 0.2051
2026-01-04 23:52:22,556: t15.2024.06.14 val PER: 0.2240
2026-01-04 23:52:22,556: t15.2024.07.19 val PER: 0.3032
2026-01-04 23:52:22,557: t15.2024.07.21 val PER: 0.1476
2026-01-04 23:52:22,557: t15.2024.07.28 val PER: 0.1838
2026-01-04 23:52:22,557: t15.2025.01.10 val PER: 0.3595
2026-01-04 23:52:22,557: t15.2025.01.12 val PER: 0.2017
2026-01-04 23:52:22,557: t15.2025.03.14 val PER: 0.3683
2026-01-04 23:52:22,557: t15.2025.03.16 val PER: 0.2291
2026-01-04 23:52:22,557: t15.2025.03.30 val PER: 0.3575
2026-01-04 23:52:22,557: t15.2025.04.13 val PER: 0.2639
2026-01-04 23:52:22,558: New best val WER(1gram) 53.55% --> 51.52%
2026-01-04 23:52:22,558: Checkpointing model
2026-01-04 23:52:23,191: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:52:23,459: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_7000
2026-01-04 23:52:41,867: Train batch 7200: loss: 14.34 grad norm: 56.89 time: 0.078
2026-01-04 23:52:59,988: Train batch 7400: loss: 13.93 grad norm: 56.37 time: 0.075
2026-01-04 23:53:09,022: Running test after training batch: 7500
2026-01-04 23:53:09,127: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:53:13,892: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 23:53:13,921: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cussed nit
2026-01-04 23:53:15,588: Val batch 7500: PER (avg): 0.1897 CTC Loss (avg): 18.7465 WER(1gram): 54.31% (n=64) time: 6.566
2026-01-04 23:53:15,588: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 23:53:15,589: t15.2023.08.13 val PER: 0.1622
2026-01-04 23:53:15,589: t15.2023.08.18 val PER: 0.1383
2026-01-04 23:53:15,589: t15.2023.08.20 val PER: 0.1446
2026-01-04 23:53:15,589: t15.2023.08.25 val PER: 0.1024
2026-01-04 23:53:15,589: t15.2023.08.27 val PER: 0.2090
2026-01-04 23:53:15,589: t15.2023.09.01 val PER: 0.1088
2026-01-04 23:53:15,589: t15.2023.09.03 val PER: 0.1900
2026-01-04 23:53:15,589: t15.2023.09.24 val PER: 0.1614
2026-01-04 23:53:15,589: t15.2023.09.29 val PER: 0.1570
2026-01-04 23:53:15,589: t15.2023.10.01 val PER: 0.2008
2026-01-04 23:53:15,589: t15.2023.10.06 val PER: 0.1066
2026-01-04 23:53:15,590: t15.2023.10.08 val PER: 0.2706
2026-01-04 23:53:15,590: t15.2023.10.13 val PER: 0.2475
2026-01-04 23:53:15,590: t15.2023.10.15 val PER: 0.1938
2026-01-04 23:53:15,590: t15.2023.10.20 val PER: 0.2013
2026-01-04 23:53:15,590: t15.2023.10.22 val PER: 0.1425
2026-01-04 23:53:15,590: t15.2023.11.03 val PER: 0.2083
2026-01-04 23:53:15,590: t15.2023.11.04 val PER: 0.0478
2026-01-04 23:53:15,590: t15.2023.11.17 val PER: 0.0715
2026-01-04 23:53:15,590: t15.2023.11.19 val PER: 0.0519
2026-01-04 23:53:15,590: t15.2023.11.26 val PER: 0.1957
2026-01-04 23:53:15,590: t15.2023.12.03 val PER: 0.1576
2026-01-04 23:53:15,590: t15.2023.12.08 val PER: 0.1425
2026-01-04 23:53:15,590: t15.2023.12.10 val PER: 0.1301
2026-01-04 23:53:15,590: t15.2023.12.17 val PER: 0.1674
2026-01-04 23:53:15,590: t15.2023.12.29 val PER: 0.1922
2026-01-04 23:53:15,590: t15.2024.02.25 val PER: 0.1362
2026-01-04 23:53:15,590: t15.2024.03.08 val PER: 0.2632
2026-01-04 23:53:15,590: t15.2024.03.15 val PER: 0.2427
2026-01-04 23:53:15,591: t15.2024.03.17 val PER: 0.1792
2026-01-04 23:53:15,591: t15.2024.05.10 val PER: 0.1976
2026-01-04 23:53:15,591: t15.2024.06.14 val PER: 0.2003
2026-01-04 23:53:15,591: t15.2024.07.19 val PER: 0.2914
2026-01-04 23:53:15,591: t15.2024.07.21 val PER: 0.1414
2026-01-04 23:53:15,591: t15.2024.07.28 val PER: 0.1743
2026-01-04 23:53:15,591: t15.2025.01.10 val PER: 0.3485
2026-01-04 23:53:15,591: t15.2025.01.12 val PER: 0.1909
2026-01-04 23:53:15,591: t15.2025.03.14 val PER: 0.3817
2026-01-04 23:53:15,591: t15.2025.03.16 val PER: 0.2500
2026-01-04 23:53:15,591: t15.2025.03.30 val PER: 0.3621
2026-01-04 23:53:15,591: t15.2025.04.13 val PER: 0.2454
2026-01-04 23:53:15,845: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_7500
2026-01-04 23:53:25,008: Train batch 7600: loss: 16.17 grad norm: 59.05 time: 0.069
2026-01-04 23:53:43,318: Train batch 7800: loss: 14.86 grad norm: 64.84 time: 0.055
2026-01-04 23:54:01,760: Train batch 8000: loss: 11.39 grad norm: 52.68 time: 0.072
2026-01-04 23:54:01,761: Running test after training batch: 8000
2026-01-04 23:54:01,877: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:54:06,650: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-04 23:54:06,679: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nit
2026-01-04 23:54:08,348: Val batch 8000: PER (avg): 0.1850 CTC Loss (avg): 18.1897 WER(1gram): 53.55% (n=64) time: 6.588
2026-01-04 23:54:08,349: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 23:54:08,349: t15.2023.08.13 val PER: 0.1518
2026-01-04 23:54:08,349: t15.2023.08.18 val PER: 0.1341
2026-01-04 23:54:08,349: t15.2023.08.20 val PER: 0.1422
2026-01-04 23:54:08,349: t15.2023.08.25 val PER: 0.1054
2026-01-04 23:54:08,349: t15.2023.08.27 val PER: 0.2122
2026-01-04 23:54:08,349: t15.2023.09.01 val PER: 0.0966
2026-01-04 23:54:08,349: t15.2023.09.03 val PER: 0.1936
2026-01-04 23:54:08,349: t15.2023.09.24 val PER: 0.1566
2026-01-04 23:54:08,349: t15.2023.09.29 val PER: 0.1493
2026-01-04 23:54:08,349: t15.2023.10.01 val PER: 0.2048
2026-01-04 23:54:08,350: t15.2023.10.06 val PER: 0.1109
2026-01-04 23:54:08,350: t15.2023.10.08 val PER: 0.2612
2026-01-04 23:54:08,350: t15.2023.10.13 val PER: 0.2475
2026-01-04 23:54:08,350: t15.2023.10.15 val PER: 0.1931
2026-01-04 23:54:08,350: t15.2023.10.20 val PER: 0.2148
2026-01-04 23:54:08,350: t15.2023.10.22 val PER: 0.1470
2026-01-04 23:54:08,350: t15.2023.11.03 val PER: 0.2015
2026-01-04 23:54:08,350: t15.2023.11.04 val PER: 0.0410
2026-01-04 23:54:08,350: t15.2023.11.17 val PER: 0.0607
2026-01-04 23:54:08,350: t15.2023.11.19 val PER: 0.0539
2026-01-04 23:54:08,350: t15.2023.11.26 val PER: 0.1732
2026-01-04 23:54:08,350: t15.2023.12.03 val PER: 0.1544
2026-01-04 23:54:08,350: t15.2023.12.08 val PER: 0.1458
2026-01-04 23:54:08,350: t15.2023.12.10 val PER: 0.1288
2026-01-04 23:54:08,350: t15.2023.12.17 val PER: 0.1726
2026-01-04 23:54:08,351: t15.2023.12.29 val PER: 0.1764
2026-01-04 23:54:08,351: t15.2024.02.25 val PER: 0.1475
2026-01-04 23:54:08,351: t15.2024.03.08 val PER: 0.2788
2026-01-04 23:54:08,351: t15.2024.03.15 val PER: 0.2383
2026-01-04 23:54:08,351: t15.2024.03.17 val PER: 0.1729
2026-01-04 23:54:08,351: t15.2024.05.10 val PER: 0.1813
2026-01-04 23:54:08,351: t15.2024.06.14 val PER: 0.2035
2026-01-04 23:54:08,351: t15.2024.07.19 val PER: 0.2868
2026-01-04 23:54:08,351: t15.2024.07.21 val PER: 0.1234
2026-01-04 23:54:08,351: t15.2024.07.28 val PER: 0.1603
2026-01-04 23:54:08,351: t15.2025.01.10 val PER: 0.3333
2026-01-04 23:54:08,351: t15.2025.01.12 val PER: 0.1932
2026-01-04 23:54:08,351: t15.2025.03.14 val PER: 0.3757
2026-01-04 23:54:08,352: t15.2025.03.16 val PER: 0.2264
2026-01-04 23:54:08,352: t15.2025.03.30 val PER: 0.3494
2026-01-04 23:54:08,352: t15.2025.04.13 val PER: 0.2482
2026-01-04 23:54:08,606: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_8000
2026-01-04 23:54:26,945: Train batch 8200: loss: 9.43 grad norm: 45.23 time: 0.054
2026-01-04 23:54:45,381: Train batch 8400: loss: 9.75 grad norm: 47.95 time: 0.064
2026-01-04 23:54:54,659: Running test after training batch: 8500
2026-01-04 23:54:54,754: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:54:59,472: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 23:54:59,501: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost nett
2026-01-04 23:55:01,157: Val batch 8500: PER (avg): 0.1801 CTC Loss (avg): 17.8237 WER(1gram): 51.02% (n=64) time: 6.497
2026-01-04 23:55:01,157: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 23:55:01,157: t15.2023.08.13 val PER: 0.1403
2026-01-04 23:55:01,157: t15.2023.08.18 val PER: 0.1282
2026-01-04 23:55:01,158: t15.2023.08.20 val PER: 0.1430
2026-01-04 23:55:01,158: t15.2023.08.25 val PER: 0.1220
2026-01-04 23:55:01,158: t15.2023.08.27 val PER: 0.2122
2026-01-04 23:55:01,158: t15.2023.09.01 val PER: 0.0982
2026-01-04 23:55:01,158: t15.2023.09.03 val PER: 0.1841
2026-01-04 23:55:01,158: t15.2023.09.24 val PER: 0.1626
2026-01-04 23:55:01,158: t15.2023.09.29 val PER: 0.1500
2026-01-04 23:55:01,158: t15.2023.10.01 val PER: 0.1968
2026-01-04 23:55:01,158: t15.2023.10.06 val PER: 0.1044
2026-01-04 23:55:01,158: t15.2023.10.08 val PER: 0.2666
2026-01-04 23:55:01,158: t15.2023.10.13 val PER: 0.2475
2026-01-04 23:55:01,158: t15.2023.10.15 val PER: 0.1819
2026-01-04 23:55:01,158: t15.2023.10.20 val PER: 0.1913
2026-01-04 23:55:01,158: t15.2023.10.22 val PER: 0.1470
2026-01-04 23:55:01,158: t15.2023.11.03 val PER: 0.1934
2026-01-04 23:55:01,159: t15.2023.11.04 val PER: 0.0375
2026-01-04 23:55:01,159: t15.2023.11.17 val PER: 0.0591
2026-01-04 23:55:01,159: t15.2023.11.19 val PER: 0.0479
2026-01-04 23:55:01,159: t15.2023.11.26 val PER: 0.1739
2026-01-04 23:55:01,159: t15.2023.12.03 val PER: 0.1387
2026-01-04 23:55:01,159: t15.2023.12.08 val PER: 0.1478
2026-01-04 23:55:01,159: t15.2023.12.10 val PER: 0.1078
2026-01-04 23:55:01,159: t15.2023.12.17 val PER: 0.1622
2026-01-04 23:55:01,159: t15.2023.12.29 val PER: 0.1736
2026-01-04 23:55:01,159: t15.2024.02.25 val PER: 0.1419
2026-01-04 23:55:01,159: t15.2024.03.08 val PER: 0.2603
2026-01-04 23:55:01,159: t15.2024.03.15 val PER: 0.2283
2026-01-04 23:55:01,159: t15.2024.03.17 val PER: 0.1709
2026-01-04 23:55:01,159: t15.2024.05.10 val PER: 0.1872
2026-01-04 23:55:01,159: t15.2024.06.14 val PER: 0.1814
2026-01-04 23:55:01,160: t15.2024.07.19 val PER: 0.2835
2026-01-04 23:55:01,160: t15.2024.07.21 val PER: 0.1214
2026-01-04 23:55:01,160: t15.2024.07.28 val PER: 0.1676
2026-01-04 23:55:01,160: t15.2025.01.10 val PER: 0.3251
2026-01-04 23:55:01,160: t15.2025.01.12 val PER: 0.1878
2026-01-04 23:55:01,160: t15.2025.03.14 val PER: 0.3521
2026-01-04 23:55:01,160: t15.2025.03.16 val PER: 0.2160
2026-01-04 23:55:01,160: t15.2025.03.30 val PER: 0.3345
2026-01-04 23:55:01,160: t15.2025.04.13 val PER: 0.2368
2026-01-04 23:55:01,161: New best val WER(1gram) 51.52% --> 51.02%
2026-01-04 23:55:01,161: Checkpointing model
2026-01-04 23:55:01,793: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:55:02,075: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_8500
2026-01-04 23:55:11,282: Train batch 8600: loss: 16.08 grad norm: 62.71 time: 0.054
2026-01-04 23:55:29,852: Train batch 8800: loss: 15.13 grad norm: 57.21 time: 0.060
2026-01-04 23:55:48,220: Train batch 9000: loss: 16.09 grad norm: 64.05 time: 0.072
2026-01-04 23:55:48,220: Running test after training batch: 9000
2026-01-04 23:55:48,348: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:55:53,247: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:55:53,277: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost nit
2026-01-04 23:55:54,961: Val batch 9000: PER (avg): 0.1740 CTC Loss (avg): 17.2253 WER(1gram): 50.00% (n=64) time: 6.740
2026-01-04 23:55:54,961: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-04 23:55:54,962: t15.2023.08.13 val PER: 0.1383
2026-01-04 23:55:54,962: t15.2023.08.18 val PER: 0.1232
2026-01-04 23:55:54,962: t15.2023.08.20 val PER: 0.1319
2026-01-04 23:55:54,962: t15.2023.08.25 val PER: 0.1024
2026-01-04 23:55:54,962: t15.2023.08.27 val PER: 0.2106
2026-01-04 23:55:54,962: t15.2023.09.01 val PER: 0.0950
2026-01-04 23:55:54,962: t15.2023.09.03 val PER: 0.1817
2026-01-04 23:55:54,962: t15.2023.09.24 val PER: 0.1481
2026-01-04 23:55:54,962: t15.2023.09.29 val PER: 0.1461
2026-01-04 23:55:54,962: t15.2023.10.01 val PER: 0.1902
2026-01-04 23:55:54,962: t15.2023.10.06 val PER: 0.0926
2026-01-04 23:55:54,962: t15.2023.10.08 val PER: 0.2639
2026-01-04 23:55:54,962: t15.2023.10.13 val PER: 0.2312
2026-01-04 23:55:54,962: t15.2023.10.15 val PER: 0.1753
2026-01-04 23:55:54,962: t15.2023.10.20 val PER: 0.2148
2026-01-04 23:55:54,963: t15.2023.10.22 val PER: 0.1303
2026-01-04 23:55:54,963: t15.2023.11.03 val PER: 0.2042
2026-01-04 23:55:54,963: t15.2023.11.04 val PER: 0.0341
2026-01-04 23:55:54,963: t15.2023.11.17 val PER: 0.0482
2026-01-04 23:55:54,963: t15.2023.11.19 val PER: 0.0499
2026-01-04 23:55:54,963: t15.2023.11.26 val PER: 0.1580
2026-01-04 23:55:54,963: t15.2023.12.03 val PER: 0.1345
2026-01-04 23:55:54,963: t15.2023.12.08 val PER: 0.1272
2026-01-04 23:55:54,963: t15.2023.12.10 val PER: 0.1104
2026-01-04 23:55:54,963: t15.2023.12.17 val PER: 0.1601
2026-01-04 23:55:54,963: t15.2023.12.29 val PER: 0.1647
2026-01-04 23:55:54,963: t15.2024.02.25 val PER: 0.1376
2026-01-04 23:55:54,963: t15.2024.03.08 val PER: 0.2632
2026-01-04 23:55:54,963: t15.2024.03.15 val PER: 0.2301
2026-01-04 23:55:54,964: t15.2024.03.17 val PER: 0.1681
2026-01-04 23:55:54,964: t15.2024.05.10 val PER: 0.1828
2026-01-04 23:55:54,964: t15.2024.06.14 val PER: 0.1893
2026-01-04 23:55:54,964: t15.2024.07.19 val PER: 0.2709
2026-01-04 23:55:54,964: t15.2024.07.21 val PER: 0.1221
2026-01-04 23:55:54,964: t15.2024.07.28 val PER: 0.1544
2026-01-04 23:55:54,964: t15.2025.01.10 val PER: 0.3154
2026-01-04 23:55:54,964: t15.2025.01.12 val PER: 0.1786
2026-01-04 23:55:54,964: t15.2025.03.14 val PER: 0.3609
2026-01-04 23:55:54,964: t15.2025.03.16 val PER: 0.2147
2026-01-04 23:55:54,964: t15.2025.03.30 val PER: 0.3172
2026-01-04 23:55:54,964: t15.2025.04.13 val PER: 0.2325
2026-01-04 23:55:54,965: New best val WER(1gram) 51.02% --> 50.00%
2026-01-04 23:55:54,965: Checkpointing model
2026-01-04 23:55:55,587: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:55:55,882: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_9000
2026-01-04 23:56:14,639: Train batch 9200: loss: 10.84 grad norm: 49.27 time: 0.055
2026-01-04 23:56:33,198: Train batch 9400: loss: 7.79 grad norm: 47.24 time: 0.068
2026-01-04 23:56:42,505: Running test after training batch: 9500
2026-01-04 23:56:42,665: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:56:47,847: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:56:47,876: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-04 23:56:49,574: Val batch 9500: PER (avg): 0.1729 CTC Loss (avg): 17.1634 WER(1gram): 49.24% (n=64) time: 7.068
2026-01-04 23:56:49,574: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 23:56:49,574: t15.2023.08.13 val PER: 0.1268
2026-01-04 23:56:49,575: t15.2023.08.18 val PER: 0.1182
2026-01-04 23:56:49,575: t15.2023.08.20 val PER: 0.1334
2026-01-04 23:56:49,575: t15.2023.08.25 val PER: 0.1009
2026-01-04 23:56:49,575: t15.2023.08.27 val PER: 0.2026
2026-01-04 23:56:49,575: t15.2023.09.01 val PER: 0.0869
2026-01-04 23:56:49,575: t15.2023.09.03 val PER: 0.1746
2026-01-04 23:56:49,575: t15.2023.09.24 val PER: 0.1541
2026-01-04 23:56:49,575: t15.2023.09.29 val PER: 0.1449
2026-01-04 23:56:49,576: t15.2023.10.01 val PER: 0.1915
2026-01-04 23:56:49,576: t15.2023.10.06 val PER: 0.1012
2026-01-04 23:56:49,576: t15.2023.10.08 val PER: 0.2639
2026-01-04 23:56:49,576: t15.2023.10.13 val PER: 0.2258
2026-01-04 23:56:49,576: t15.2023.10.15 val PER: 0.1753
2026-01-04 23:56:49,576: t15.2023.10.20 val PER: 0.2114
2026-01-04 23:56:49,576: t15.2023.10.22 val PER: 0.1303
2026-01-04 23:56:49,576: t15.2023.11.03 val PER: 0.1940
2026-01-04 23:56:49,576: t15.2023.11.04 val PER: 0.0375
2026-01-04 23:56:49,576: t15.2023.11.17 val PER: 0.0482
2026-01-04 23:56:49,576: t15.2023.11.19 val PER: 0.0479
2026-01-04 23:56:49,576: t15.2023.11.26 val PER: 0.1572
2026-01-04 23:56:49,576: t15.2023.12.03 val PER: 0.1366
2026-01-04 23:56:49,576: t15.2023.12.08 val PER: 0.1418
2026-01-04 23:56:49,577: t15.2023.12.10 val PER: 0.1170
2026-01-04 23:56:49,577: t15.2023.12.17 val PER: 0.1486
2026-01-04 23:56:49,577: t15.2023.12.29 val PER: 0.1537
2026-01-04 23:56:49,577: t15.2024.02.25 val PER: 0.1390
2026-01-04 23:56:49,577: t15.2024.03.08 val PER: 0.2674
2026-01-04 23:56:49,577: t15.2024.03.15 val PER: 0.2314
2026-01-04 23:56:49,577: t15.2024.03.17 val PER: 0.1639
2026-01-04 23:56:49,577: t15.2024.05.10 val PER: 0.1842
2026-01-04 23:56:49,577: t15.2024.06.14 val PER: 0.1782
2026-01-04 23:56:49,577: t15.2024.07.19 val PER: 0.2716
2026-01-04 23:56:49,577: t15.2024.07.21 val PER: 0.1097
2026-01-04 23:56:49,577: t15.2024.07.28 val PER: 0.1529
2026-01-04 23:56:49,577: t15.2025.01.10 val PER: 0.3223
2026-01-04 23:56:49,577: t15.2025.01.12 val PER: 0.1824
2026-01-04 23:56:49,577: t15.2025.03.14 val PER: 0.3713
2026-01-04 23:56:49,578: t15.2025.03.16 val PER: 0.2068
2026-01-04 23:56:49,578: t15.2025.03.30 val PER: 0.3241
2026-01-04 23:56:49,578: t15.2025.04.13 val PER: 0.2411
2026-01-04 23:56:49,578: New best val WER(1gram) 50.00% --> 49.24%
2026-01-04 23:56:49,579: Checkpointing model
2026-01-04 23:56:50,229: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:56:50,543: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_9500
2026-01-04 23:56:59,397: Train batch 9600: loss: 8.70 grad norm: 47.00 time: 0.073
2026-01-04 23:57:17,322: Train batch 9800: loss: 12.75 grad norm: 58.43 time: 0.063
2026-01-04 23:57:35,507: Train batch 10000: loss: 5.76 grad norm: 37.27 time: 0.061
2026-01-04 23:57:35,507: Running test after training batch: 10000
2026-01-04 23:57:35,604: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:57:40,407: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:57:40,438: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 23:57:42,130: Val batch 10000: PER (avg): 0.1681 CTC Loss (avg): 16.7244 WER(1gram): 50.00% (n=64) time: 6.623
2026-01-04 23:57:42,130: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 23:57:42,131: t15.2023.08.13 val PER: 0.1237
2026-01-04 23:57:42,131: t15.2023.08.18 val PER: 0.1224
2026-01-04 23:57:42,131: t15.2023.08.20 val PER: 0.1263
2026-01-04 23:57:42,131: t15.2023.08.25 val PER: 0.0919
2026-01-04 23:57:42,131: t15.2023.08.27 val PER: 0.2058
2026-01-04 23:57:42,131: t15.2023.09.01 val PER: 0.0909
2026-01-04 23:57:42,131: t15.2023.09.03 val PER: 0.1722
2026-01-04 23:57:42,131: t15.2023.09.24 val PER: 0.1505
2026-01-04 23:57:42,131: t15.2023.09.29 val PER: 0.1436
2026-01-04 23:57:42,131: t15.2023.10.01 val PER: 0.1803
2026-01-04 23:57:42,131: t15.2023.10.06 val PER: 0.1076
2026-01-04 23:57:42,131: t15.2023.10.08 val PER: 0.2585
2026-01-04 23:57:42,131: t15.2023.10.13 val PER: 0.2203
2026-01-04 23:57:42,131: t15.2023.10.15 val PER: 0.1661
2026-01-04 23:57:42,131: t15.2023.10.20 val PER: 0.2013
2026-01-04 23:57:42,132: t15.2023.10.22 val PER: 0.1269
2026-01-04 23:57:42,132: t15.2023.11.03 val PER: 0.1934
2026-01-04 23:57:42,132: t15.2023.11.04 val PER: 0.0375
2026-01-04 23:57:42,132: t15.2023.11.17 val PER: 0.0404
2026-01-04 23:57:42,132: t15.2023.11.19 val PER: 0.0439
2026-01-04 23:57:42,132: t15.2023.11.26 val PER: 0.1500
2026-01-04 23:57:42,132: t15.2023.12.03 val PER: 0.1292
2026-01-04 23:57:42,132: t15.2023.12.08 val PER: 0.1232
2026-01-04 23:57:42,132: t15.2023.12.10 val PER: 0.1117
2026-01-04 23:57:42,132: t15.2023.12.17 val PER: 0.1590
2026-01-04 23:57:42,133: t15.2023.12.29 val PER: 0.1551
2026-01-04 23:57:42,133: t15.2024.02.25 val PER: 0.1475
2026-01-04 23:57:42,133: t15.2024.03.08 val PER: 0.2432
2026-01-04 23:57:42,133: t15.2024.03.15 val PER: 0.2226
2026-01-04 23:57:42,133: t15.2024.03.17 val PER: 0.1604
2026-01-04 23:57:42,133: t15.2024.05.10 val PER: 0.1738
2026-01-04 23:57:42,133: t15.2024.06.14 val PER: 0.1956
2026-01-04 23:57:42,133: t15.2024.07.19 val PER: 0.2591
2026-01-04 23:57:42,133: t15.2024.07.21 val PER: 0.1138
2026-01-04 23:57:42,133: t15.2024.07.28 val PER: 0.1493
2026-01-04 23:57:42,133: t15.2025.01.10 val PER: 0.3154
2026-01-04 23:57:42,133: t15.2025.01.12 val PER: 0.1801
2026-01-04 23:57:42,133: t15.2025.03.14 val PER: 0.3388
2026-01-04 23:57:42,133: t15.2025.03.16 val PER: 0.2029
2026-01-04 23:57:42,133: t15.2025.03.30 val PER: 0.3103
2026-01-04 23:57:42,134: t15.2025.04.13 val PER: 0.2325
2026-01-04 23:57:42,424: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_10000
2026-01-04 23:58:01,056: Train batch 10200: loss: 6.26 grad norm: 38.48 time: 0.050
2026-01-04 23:58:20,161: Train batch 10400: loss: 9.26 grad norm: 50.22 time: 0.072
2026-01-04 23:58:29,453: Running test after training batch: 10500
2026-01-04 23:58:29,604: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:58:34,341: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:58:34,373: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-04 23:58:36,104: Val batch 10500: PER (avg): 0.1665 CTC Loss (avg): 16.5933 WER(1gram): 47.21% (n=64) time: 6.650
2026-01-04 23:58:36,105: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 23:58:36,105: t15.2023.08.13 val PER: 0.1175
2026-01-04 23:58:36,105: t15.2023.08.18 val PER: 0.1165
2026-01-04 23:58:36,105: t15.2023.08.20 val PER: 0.1311
2026-01-04 23:58:36,105: t15.2023.08.25 val PER: 0.1084
2026-01-04 23:58:36,105: t15.2023.08.27 val PER: 0.2042
2026-01-04 23:58:36,105: t15.2023.09.01 val PER: 0.0917
2026-01-04 23:58:36,105: t15.2023.09.03 val PER: 0.1841
2026-01-04 23:58:36,105: t15.2023.09.24 val PER: 0.1456
2026-01-04 23:58:36,105: t15.2023.09.29 val PER: 0.1544
2026-01-04 23:58:36,105: t15.2023.10.01 val PER: 0.1830
2026-01-04 23:58:36,105: t15.2023.10.06 val PER: 0.0947
2026-01-04 23:58:36,105: t15.2023.10.08 val PER: 0.2476
2026-01-04 23:58:36,106: t15.2023.10.13 val PER: 0.2141
2026-01-04 23:58:36,106: t15.2023.10.15 val PER: 0.1694
2026-01-04 23:58:36,106: t15.2023.10.20 val PER: 0.2047
2026-01-04 23:58:36,106: t15.2023.10.22 val PER: 0.1359
2026-01-04 23:58:36,106: t15.2023.11.03 val PER: 0.1906
2026-01-04 23:58:36,106: t15.2023.11.04 val PER: 0.0410
2026-01-04 23:58:36,106: t15.2023.11.17 val PER: 0.0467
2026-01-04 23:58:36,106: t15.2023.11.19 val PER: 0.0399
2026-01-04 23:58:36,106: t15.2023.11.26 val PER: 0.1406
2026-01-04 23:58:36,106: t15.2023.12.03 val PER: 0.1324
2026-01-04 23:58:36,106: t15.2023.12.08 val PER: 0.1238
2026-01-04 23:58:36,106: t15.2023.12.10 val PER: 0.1078
2026-01-04 23:58:36,106: t15.2023.12.17 val PER: 0.1476
2026-01-04 23:58:36,106: t15.2023.12.29 val PER: 0.1558
2026-01-04 23:58:36,106: t15.2024.02.25 val PER: 0.1222
2026-01-04 23:58:36,107: t15.2024.03.08 val PER: 0.2432
2026-01-04 23:58:36,107: t15.2024.03.15 val PER: 0.2226
2026-01-04 23:58:36,107: t15.2024.03.17 val PER: 0.1569
2026-01-04 23:58:36,107: t15.2024.05.10 val PER: 0.1694
2026-01-04 23:58:36,107: t15.2024.06.14 val PER: 0.1703
2026-01-04 23:58:36,107: t15.2024.07.19 val PER: 0.2597
2026-01-04 23:58:36,107: t15.2024.07.21 val PER: 0.1048
2026-01-04 23:58:36,107: t15.2024.07.28 val PER: 0.1426
2026-01-04 23:58:36,107: t15.2025.01.10 val PER: 0.3237
2026-01-04 23:58:36,107: t15.2025.01.12 val PER: 0.1640
2026-01-04 23:58:36,108: t15.2025.03.14 val PER: 0.3506
2026-01-04 23:58:36,108: t15.2025.03.16 val PER: 0.2107
2026-01-04 23:58:36,109: t15.2025.03.30 val PER: 0.3138
2026-01-04 23:58:36,109: t15.2025.04.13 val PER: 0.2311
2026-01-04 23:58:36,110: New best val WER(1gram) 49.24% --> 47.21%
2026-01-04 23:58:36,110: Checkpointing model
2026-01-04 23:58:36,797: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-04 23:58:37,091: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_10500
2026-01-04 23:58:46,555: Train batch 10600: loss: 9.55 grad norm: 58.62 time: 0.072
2026-01-04 23:59:05,039: Train batch 10800: loss: 14.68 grad norm: 64.56 time: 0.064
2026-01-04 23:59:23,576: Train batch 11000: loss: 14.63 grad norm: 62.87 time: 0.057
2026-01-04 23:59:23,576: Running test after training batch: 11000
2026-01-04 23:59:23,737: WER debug GT example: You can see the code at this point as well.
2026-01-04 23:59:28,437: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 23:59:28,468: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 23:59:30,171: Val batch 11000: PER (avg): 0.1643 CTC Loss (avg): 16.4709 WER(1gram): 47.72% (n=64) time: 6.595
2026-01-04 23:59:30,172: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 23:59:30,172: t15.2023.08.13 val PER: 0.1279
2026-01-04 23:59:30,172: t15.2023.08.18 val PER: 0.1190
2026-01-04 23:59:30,172: t15.2023.08.20 val PER: 0.1255
2026-01-04 23:59:30,172: t15.2023.08.25 val PER: 0.0994
2026-01-04 23:59:30,172: t15.2023.08.27 val PER: 0.1994
2026-01-04 23:59:30,173: t15.2023.09.01 val PER: 0.0869
2026-01-04 23:59:30,173: t15.2023.09.03 val PER: 0.1770
2026-01-04 23:59:30,173: t15.2023.09.24 val PER: 0.1468
2026-01-04 23:59:30,173: t15.2023.09.29 val PER: 0.1391
2026-01-04 23:59:30,173: t15.2023.10.01 val PER: 0.1876
2026-01-04 23:59:30,173: t15.2023.10.06 val PER: 0.0958
2026-01-04 23:59:30,173: t15.2023.10.08 val PER: 0.2598
2026-01-04 23:59:30,173: t15.2023.10.13 val PER: 0.2172
2026-01-04 23:59:30,173: t15.2023.10.15 val PER: 0.1668
2026-01-04 23:59:30,173: t15.2023.10.20 val PER: 0.2013
2026-01-04 23:59:30,173: t15.2023.10.22 val PER: 0.1303
2026-01-04 23:59:30,173: t15.2023.11.03 val PER: 0.1920
2026-01-04 23:59:30,174: t15.2023.11.04 val PER: 0.0307
2026-01-04 23:59:30,174: t15.2023.11.17 val PER: 0.0467
2026-01-04 23:59:30,174: t15.2023.11.19 val PER: 0.0499
2026-01-04 23:59:30,174: t15.2023.11.26 val PER: 0.1442
2026-01-04 23:59:30,174: t15.2023.12.03 val PER: 0.1313
2026-01-04 23:59:30,174: t15.2023.12.08 val PER: 0.1212
2026-01-04 23:59:30,174: t15.2023.12.10 val PER: 0.1051
2026-01-04 23:59:30,174: t15.2023.12.17 val PER: 0.1507
2026-01-04 23:59:30,174: t15.2023.12.29 val PER: 0.1400
2026-01-04 23:59:30,175: t15.2024.02.25 val PER: 0.1278
2026-01-04 23:59:30,175: t15.2024.03.08 val PER: 0.2304
2026-01-04 23:59:30,175: t15.2024.03.15 val PER: 0.2176
2026-01-04 23:59:30,175: t15.2024.03.17 val PER: 0.1548
2026-01-04 23:59:30,175: t15.2024.05.10 val PER: 0.1694
2026-01-04 23:59:30,175: t15.2024.06.14 val PER: 0.1735
2026-01-04 23:59:30,175: t15.2024.07.19 val PER: 0.2577
2026-01-04 23:59:30,175: t15.2024.07.21 val PER: 0.0972
2026-01-04 23:59:30,175: t15.2024.07.28 val PER: 0.1478
2026-01-04 23:59:30,175: t15.2025.01.10 val PER: 0.3113
2026-01-04 23:59:30,175: t15.2025.01.12 val PER: 0.1701
2026-01-04 23:59:30,175: t15.2025.03.14 val PER: 0.3521
2026-01-04 23:59:30,175: t15.2025.03.16 val PER: 0.1937
2026-01-04 23:59:30,175: t15.2025.03.30 val PER: 0.3046
2026-01-04 23:59:30,175: t15.2025.04.13 val PER: 0.2240
2026-01-04 23:59:30,457: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_11000
2026-01-04 23:59:48,785: Train batch 11200: loss: 10.38 grad norm: 52.23 time: 0.070
2026-01-05 00:00:07,092: Train batch 11400: loss: 9.51 grad norm: 53.23 time: 0.056
2026-01-05 00:00:16,370: Running test after training batch: 11500
2026-01-05 00:00:16,537: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:00:21,277: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:00:21,309: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-05 00:00:23,045: Val batch 11500: PER (avg): 0.1608 CTC Loss (avg): 16.4182 WER(1gram): 48.22% (n=64) time: 6.674
2026-01-05 00:00:23,045: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 00:00:23,045: t15.2023.08.13 val PER: 0.1216
2026-01-05 00:00:23,045: t15.2023.08.18 val PER: 0.1123
2026-01-05 00:00:23,045: t15.2023.08.20 val PER: 0.1176
2026-01-05 00:00:23,046: t15.2023.08.25 val PER: 0.0994
2026-01-05 00:00:23,046: t15.2023.08.27 val PER: 0.1913
2026-01-05 00:00:23,046: t15.2023.09.01 val PER: 0.0877
2026-01-05 00:00:23,046: t15.2023.09.03 val PER: 0.1627
2026-01-05 00:00:23,046: t15.2023.09.24 val PER: 0.1347
2026-01-05 00:00:23,046: t15.2023.09.29 val PER: 0.1347
2026-01-05 00:00:23,046: t15.2023.10.01 val PER: 0.1757
2026-01-05 00:00:23,046: t15.2023.10.06 val PER: 0.0850
2026-01-05 00:00:23,046: t15.2023.10.08 val PER: 0.2679
2026-01-05 00:00:23,046: t15.2023.10.13 val PER: 0.2095
2026-01-05 00:00:23,046: t15.2023.10.15 val PER: 0.1648
2026-01-05 00:00:23,046: t15.2023.10.20 val PER: 0.1812
2026-01-05 00:00:23,047: t15.2023.10.22 val PER: 0.1247
2026-01-05 00:00:23,047: t15.2023.11.03 val PER: 0.1927
2026-01-05 00:00:23,047: t15.2023.11.04 val PER: 0.0341
2026-01-05 00:00:23,047: t15.2023.11.17 val PER: 0.0389
2026-01-05 00:00:23,047: t15.2023.11.19 val PER: 0.0459
2026-01-05 00:00:23,047: t15.2023.11.26 val PER: 0.1304
2026-01-05 00:00:23,047: t15.2023.12.03 val PER: 0.1261
2026-01-05 00:00:23,047: t15.2023.12.08 val PER: 0.1198
2026-01-05 00:00:23,047: t15.2023.12.10 val PER: 0.0999
2026-01-05 00:00:23,047: t15.2023.12.17 val PER: 0.1435
2026-01-05 00:00:23,047: t15.2023.12.29 val PER: 0.1434
2026-01-05 00:00:23,047: t15.2024.02.25 val PER: 0.1222
2026-01-05 00:00:23,047: t15.2024.03.08 val PER: 0.2333
2026-01-05 00:00:23,047: t15.2024.03.15 val PER: 0.2220
2026-01-05 00:00:23,047: t15.2024.03.17 val PER: 0.1534
2026-01-05 00:00:23,047: t15.2024.05.10 val PER: 0.1634
2026-01-05 00:00:23,048: t15.2024.06.14 val PER: 0.1830
2026-01-05 00:00:23,048: t15.2024.07.19 val PER: 0.2492
2026-01-05 00:00:23,048: t15.2024.07.21 val PER: 0.1014
2026-01-05 00:00:23,048: t15.2024.07.28 val PER: 0.1449
2026-01-05 00:00:23,048: t15.2025.01.10 val PER: 0.3196
2026-01-05 00:00:23,048: t15.2025.01.12 val PER: 0.1624
2026-01-05 00:00:23,048: t15.2025.03.14 val PER: 0.3565
2026-01-05 00:00:23,048: t15.2025.03.16 val PER: 0.1990
2026-01-05 00:00:23,048: t15.2025.03.30 val PER: 0.3034
2026-01-05 00:00:23,048: t15.2025.04.13 val PER: 0.2211
2026-01-05 00:00:23,334: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_11500
2026-01-05 00:00:32,087: Train batch 11600: loss: 10.71 grad norm: 49.80 time: 0.061
2026-01-05 00:00:49,881: Train batch 11800: loss: 6.66 grad norm: 41.65 time: 0.044
2026-01-05 00:01:07,943: Train batch 12000: loss: 13.29 grad norm: 54.06 time: 0.071
2026-01-05 00:01:07,944: Running test after training batch: 12000
2026-01-05 00:01:08,036: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:01:12,760: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:01:12,792: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:01:14,525: Val batch 12000: PER (avg): 0.1590 CTC Loss (avg): 16.0814 WER(1gram): 48.48% (n=64) time: 6.581
2026-01-05 00:01:14,525: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 00:01:14,526: t15.2023.08.13 val PER: 0.1195
2026-01-05 00:01:14,526: t15.2023.08.18 val PER: 0.1123
2026-01-05 00:01:14,526: t15.2023.08.20 val PER: 0.1223
2026-01-05 00:01:14,526: t15.2023.08.25 val PER: 0.0979
2026-01-05 00:01:14,527: t15.2023.08.27 val PER: 0.1929
2026-01-05 00:01:14,530: t15.2023.09.01 val PER: 0.0852
2026-01-05 00:01:14,530: t15.2023.09.03 val PER: 0.1663
2026-01-05 00:01:14,530: t15.2023.09.24 val PER: 0.1371
2026-01-05 00:01:14,531: t15.2023.09.29 val PER: 0.1372
2026-01-05 00:01:14,531: t15.2023.10.01 val PER: 0.1764
2026-01-05 00:01:14,531: t15.2023.10.06 val PER: 0.0850
2026-01-05 00:01:14,531: t15.2023.10.08 val PER: 0.2476
2026-01-05 00:01:14,531: t15.2023.10.13 val PER: 0.2126
2026-01-05 00:01:14,531: t15.2023.10.15 val PER: 0.1661
2026-01-05 00:01:14,531: t15.2023.10.20 val PER: 0.1946
2026-01-05 00:01:14,531: t15.2023.10.22 val PER: 0.1236
2026-01-05 00:01:14,532: t15.2023.11.03 val PER: 0.1798
2026-01-05 00:01:14,532: t15.2023.11.04 val PER: 0.0375
2026-01-05 00:01:14,532: t15.2023.11.17 val PER: 0.0420
2026-01-05 00:01:14,532: t15.2023.11.19 val PER: 0.0459
2026-01-05 00:01:14,532: t15.2023.11.26 val PER: 0.1254
2026-01-05 00:01:14,532: t15.2023.12.03 val PER: 0.1145
2026-01-05 00:01:14,532: t15.2023.12.08 val PER: 0.1099
2026-01-05 00:01:14,532: t15.2023.12.10 val PER: 0.0946
2026-01-05 00:01:14,532: t15.2023.12.17 val PER: 0.1372
2026-01-05 00:01:14,532: t15.2023.12.29 val PER: 0.1386
2026-01-05 00:01:14,533: t15.2024.02.25 val PER: 0.1180
2026-01-05 00:01:14,533: t15.2024.03.08 val PER: 0.2390
2026-01-05 00:01:14,533: t15.2024.03.15 val PER: 0.2145
2026-01-05 00:01:14,533: t15.2024.03.17 val PER: 0.1492
2026-01-05 00:01:14,533: t15.2024.05.10 val PER: 0.1842
2026-01-05 00:01:14,533: t15.2024.06.14 val PER: 0.1877
2026-01-05 00:01:14,533: t15.2024.07.19 val PER: 0.2604
2026-01-05 00:01:14,533: t15.2024.07.21 val PER: 0.1041
2026-01-05 00:01:14,533: t15.2024.07.28 val PER: 0.1324
2026-01-05 00:01:14,533: t15.2025.01.10 val PER: 0.3058
2026-01-05 00:01:14,533: t15.2025.01.12 val PER: 0.1493
2026-01-05 00:01:14,534: t15.2025.03.14 val PER: 0.3550
2026-01-05 00:01:14,534: t15.2025.03.16 val PER: 0.2029
2026-01-05 00:01:14,534: t15.2025.03.30 val PER: 0.3092
2026-01-05 00:01:14,534: t15.2025.04.13 val PER: 0.2254
2026-01-05 00:01:14,815: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_12000
2026-01-05 00:01:33,196: Train batch 12200: loss: 6.05 grad norm: 43.66 time: 0.067
2026-01-05 00:01:51,491: Train batch 12400: loss: 4.74 grad norm: 39.01 time: 0.041
2026-01-05 00:02:00,915: Running test after training batch: 12500
2026-01-05 00:02:01,076: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:02:05,871: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:02:05,904: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:02:07,670: Val batch 12500: PER (avg): 0.1565 CTC Loss (avg): 16.0200 WER(1gram): 47.21% (n=64) time: 6.754
2026-01-05 00:02:07,670: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 00:02:07,670: t15.2023.08.13 val PER: 0.1185
2026-01-05 00:02:07,670: t15.2023.08.18 val PER: 0.1090
2026-01-05 00:02:07,670: t15.2023.08.20 val PER: 0.1080
2026-01-05 00:02:07,670: t15.2023.08.25 val PER: 0.0934
2026-01-05 00:02:07,670: t15.2023.08.27 val PER: 0.1865
2026-01-05 00:02:07,670: t15.2023.09.01 val PER: 0.0844
2026-01-05 00:02:07,671: t15.2023.09.03 val PER: 0.1627
2026-01-05 00:02:07,671: t15.2023.09.24 val PER: 0.1335
2026-01-05 00:02:07,671: t15.2023.09.29 val PER: 0.1385
2026-01-05 00:02:07,671: t15.2023.10.01 val PER: 0.1764
2026-01-05 00:02:07,671: t15.2023.10.06 val PER: 0.0840
2026-01-05 00:02:07,671: t15.2023.10.08 val PER: 0.2544
2026-01-05 00:02:07,671: t15.2023.10.13 val PER: 0.2126
2026-01-05 00:02:07,671: t15.2023.10.15 val PER: 0.1529
2026-01-05 00:02:07,671: t15.2023.10.20 val PER: 0.1879
2026-01-05 00:02:07,671: t15.2023.10.22 val PER: 0.1192
2026-01-05 00:02:07,671: t15.2023.11.03 val PER: 0.1852
2026-01-05 00:02:07,671: t15.2023.11.04 val PER: 0.0341
2026-01-05 00:02:07,671: t15.2023.11.17 val PER: 0.0451
2026-01-05 00:02:07,672: t15.2023.11.19 val PER: 0.0499
2026-01-05 00:02:07,672: t15.2023.11.26 val PER: 0.1319
2026-01-05 00:02:07,672: t15.2023.12.03 val PER: 0.1282
2026-01-05 00:02:07,672: t15.2023.12.08 val PER: 0.1005
2026-01-05 00:02:07,672: t15.2023.12.10 val PER: 0.1025
2026-01-05 00:02:07,672: t15.2023.12.17 val PER: 0.1518
2026-01-05 00:02:07,672: t15.2023.12.29 val PER: 0.1414
2026-01-05 00:02:07,672: t15.2024.02.25 val PER: 0.1081
2026-01-05 00:02:07,672: t15.2024.03.08 val PER: 0.2262
2026-01-05 00:02:07,672: t15.2024.03.15 val PER: 0.2120
2026-01-05 00:02:07,672: t15.2024.03.17 val PER: 0.1485
2026-01-05 00:02:07,672: t15.2024.05.10 val PER: 0.1738
2026-01-05 00:02:07,672: t15.2024.06.14 val PER: 0.1640
2026-01-05 00:02:07,672: t15.2024.07.19 val PER: 0.2472
2026-01-05 00:02:07,672: t15.2024.07.21 val PER: 0.0959
2026-01-05 00:02:07,672: t15.2024.07.28 val PER: 0.1316
2026-01-05 00:02:07,672: t15.2025.01.10 val PER: 0.3072
2026-01-05 00:02:07,673: t15.2025.01.12 val PER: 0.1524
2026-01-05 00:02:07,673: t15.2025.03.14 val PER: 0.3506
2026-01-05 00:02:07,673: t15.2025.03.16 val PER: 0.1963
2026-01-05 00:02:07,673: t15.2025.03.30 val PER: 0.2989
2026-01-05 00:02:07,673: t15.2025.04.13 val PER: 0.2268
2026-01-05 00:02:07,976: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_12500
2026-01-05 00:02:17,257: Train batch 12600: loss: 7.91 grad norm: 44.90 time: 0.057
2026-01-05 00:02:36,193: Train batch 12800: loss: 5.70 grad norm: 40.49 time: 0.052
2026-01-05 00:02:55,125: Train batch 13000: loss: 6.30 grad norm: 41.86 time: 0.067
2026-01-05 00:02:55,126: Running test after training batch: 13000
2026-01-05 00:02:55,260: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:02:59,977: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:03:00,010: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:03:01,776: Val batch 13000: PER (avg): 0.1551 CTC Loss (avg): 15.9140 WER(1gram): 45.94% (n=64) time: 6.650
2026-01-05 00:03:01,776: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 00:03:01,777: t15.2023.08.13 val PER: 0.1154
2026-01-05 00:03:01,777: t15.2023.08.18 val PER: 0.1081
2026-01-05 00:03:01,777: t15.2023.08.20 val PER: 0.1128
2026-01-05 00:03:01,777: t15.2023.08.25 val PER: 0.0934
2026-01-05 00:03:01,777: t15.2023.08.27 val PER: 0.1929
2026-01-05 00:03:01,777: t15.2023.09.01 val PER: 0.0795
2026-01-05 00:03:01,777: t15.2023.09.03 val PER: 0.1639
2026-01-05 00:03:01,777: t15.2023.09.24 val PER: 0.1238
2026-01-05 00:03:01,777: t15.2023.09.29 val PER: 0.1315
2026-01-05 00:03:01,777: t15.2023.10.01 val PER: 0.1744
2026-01-05 00:03:01,777: t15.2023.10.06 val PER: 0.0926
2026-01-05 00:03:01,777: t15.2023.10.08 val PER: 0.2544
2026-01-05 00:03:01,777: t15.2023.10.13 val PER: 0.2017
2026-01-05 00:03:01,778: t15.2023.10.15 val PER: 0.1575
2026-01-05 00:03:01,778: t15.2023.10.20 val PER: 0.1812
2026-01-05 00:03:01,778: t15.2023.10.22 val PER: 0.1158
2026-01-05 00:03:01,778: t15.2023.11.03 val PER: 0.1791
2026-01-05 00:03:01,778: t15.2023.11.04 val PER: 0.0375
2026-01-05 00:03:01,778: t15.2023.11.17 val PER: 0.0389
2026-01-05 00:03:01,778: t15.2023.11.19 val PER: 0.0439
2026-01-05 00:03:01,778: t15.2023.11.26 val PER: 0.1275
2026-01-05 00:03:01,778: t15.2023.12.03 val PER: 0.1313
2026-01-05 00:03:01,778: t15.2023.12.08 val PER: 0.1085
2026-01-05 00:03:01,779: t15.2023.12.10 val PER: 0.1012
2026-01-05 00:03:01,779: t15.2023.12.17 val PER: 0.1559
2026-01-05 00:03:01,779: t15.2023.12.29 val PER: 0.1393
2026-01-05 00:03:01,779: t15.2024.02.25 val PER: 0.1124
2026-01-05 00:03:01,779: t15.2024.03.08 val PER: 0.2304
2026-01-05 00:03:01,779: t15.2024.03.15 val PER: 0.2076
2026-01-05 00:03:01,779: t15.2024.03.17 val PER: 0.1353
2026-01-05 00:03:01,779: t15.2024.05.10 val PER: 0.1649
2026-01-05 00:03:01,779: t15.2024.06.14 val PER: 0.1593
2026-01-05 00:03:01,779: t15.2024.07.19 val PER: 0.2518
2026-01-05 00:03:01,779: t15.2024.07.21 val PER: 0.1000
2026-01-05 00:03:01,779: t15.2024.07.28 val PER: 0.1324
2026-01-05 00:03:01,779: t15.2025.01.10 val PER: 0.3003
2026-01-05 00:03:01,780: t15.2025.01.12 val PER: 0.1416
2026-01-05 00:03:01,780: t15.2025.03.14 val PER: 0.3447
2026-01-05 00:03:01,780: t15.2025.03.16 val PER: 0.1950
2026-01-05 00:03:01,780: t15.2025.03.30 val PER: 0.3161
2026-01-05 00:03:01,780: t15.2025.04.13 val PER: 0.2183
2026-01-05 00:03:01,781: New best val WER(1gram) 47.21% --> 45.94%
2026-01-05 00:03:01,781: Checkpointing model
2026-01-05 00:03:02,420: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-05 00:03:02,723: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_13000
2026-01-05 00:03:20,557: Train batch 13200: loss: 12.42 grad norm: 57.81 time: 0.054
2026-01-05 00:03:38,343: Train batch 13400: loss: 8.53 grad norm: 49.55 time: 0.061
2026-01-05 00:03:47,301: Running test after training batch: 13500
2026-01-05 00:03:47,422: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:03:52,187: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:03:52,220: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:03:53,994: Val batch 13500: PER (avg): 0.1534 CTC Loss (avg): 15.5757 WER(1gram): 46.70% (n=64) time: 6.693
2026-01-05 00:03:53,995: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 00:03:53,995: t15.2023.08.13 val PER: 0.1227
2026-01-05 00:03:53,995: t15.2023.08.18 val PER: 0.1039
2026-01-05 00:03:53,995: t15.2023.08.20 val PER: 0.1112
2026-01-05 00:03:53,995: t15.2023.08.25 val PER: 0.0934
2026-01-05 00:03:53,996: t15.2023.08.27 val PER: 0.1881
2026-01-05 00:03:53,996: t15.2023.09.01 val PER: 0.0844
2026-01-05 00:03:53,996: t15.2023.09.03 val PER: 0.1698
2026-01-05 00:03:53,996: t15.2023.09.24 val PER: 0.1311
2026-01-05 00:03:53,996: t15.2023.09.29 val PER: 0.1353
2026-01-05 00:03:53,996: t15.2023.10.01 val PER: 0.1717
2026-01-05 00:03:53,996: t15.2023.10.06 val PER: 0.0861
2026-01-05 00:03:53,996: t15.2023.10.08 val PER: 0.2544
2026-01-05 00:03:53,996: t15.2023.10.13 val PER: 0.2079
2026-01-05 00:03:53,997: t15.2023.10.15 val PER: 0.1536
2026-01-05 00:03:53,997: t15.2023.10.20 val PER: 0.1812
2026-01-05 00:03:53,997: t15.2023.10.22 val PER: 0.1292
2026-01-05 00:03:53,997: t15.2023.11.03 val PER: 0.1784
2026-01-05 00:03:53,997: t15.2023.11.04 val PER: 0.0375
2026-01-05 00:03:53,997: t15.2023.11.17 val PER: 0.0451
2026-01-05 00:03:53,997: t15.2023.11.19 val PER: 0.0339
2026-01-05 00:03:53,997: t15.2023.11.26 val PER: 0.1304
2026-01-05 00:03:53,997: t15.2023.12.03 val PER: 0.1166
2026-01-05 00:03:53,997: t15.2023.12.08 val PER: 0.1065
2026-01-05 00:03:53,997: t15.2023.12.10 val PER: 0.0972
2026-01-05 00:03:53,997: t15.2023.12.17 val PER: 0.1289
2026-01-05 00:03:53,997: t15.2023.12.29 val PER: 0.1263
2026-01-05 00:03:53,997: t15.2024.02.25 val PER: 0.1067
2026-01-05 00:03:53,997: t15.2024.03.08 val PER: 0.2191
2026-01-05 00:03:53,998: t15.2024.03.15 val PER: 0.2070
2026-01-05 00:03:53,998: t15.2024.03.17 val PER: 0.1360
2026-01-05 00:03:53,998: t15.2024.05.10 val PER: 0.1456
2026-01-05 00:03:53,998: t15.2024.06.14 val PER: 0.1640
2026-01-05 00:03:53,998: t15.2024.07.19 val PER: 0.2459
2026-01-05 00:03:53,998: t15.2024.07.21 val PER: 0.0972
2026-01-05 00:03:53,998: t15.2024.07.28 val PER: 0.1360
2026-01-05 00:03:53,998: t15.2025.01.10 val PER: 0.3072
2026-01-05 00:03:53,999: t15.2025.01.12 val PER: 0.1386
2026-01-05 00:03:53,999: t15.2025.03.14 val PER: 0.3624
2026-01-05 00:03:53,999: t15.2025.03.16 val PER: 0.1846
2026-01-05 00:03:53,999: t15.2025.03.30 val PER: 0.3138
2026-01-05 00:03:53,999: t15.2025.04.13 val PER: 0.2154
2026-01-05 00:03:54,295: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_13500
2026-01-05 00:04:03,472: Train batch 13600: loss: 12.45 grad norm: 63.79 time: 0.062
2026-01-05 00:04:21,982: Train batch 13800: loss: 8.79 grad norm: 54.79 time: 0.056
2026-01-05 00:04:40,309: Train batch 14000: loss: 11.91 grad norm: 58.95 time: 0.050
2026-01-05 00:04:40,309: Running test after training batch: 14000
2026-01-05 00:04:40,487: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:04:45,253: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:04:45,286: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:04:47,075: Val batch 14000: PER (avg): 0.1519 CTC Loss (avg): 15.5578 WER(1gram): 45.94% (n=64) time: 6.766
2026-01-05 00:04:47,075: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 00:04:47,076: t15.2023.08.13 val PER: 0.1123
2026-01-05 00:04:47,076: t15.2023.08.18 val PER: 0.1014
2026-01-05 00:04:47,076: t15.2023.08.20 val PER: 0.1096
2026-01-05 00:04:47,076: t15.2023.08.25 val PER: 0.1024
2026-01-05 00:04:47,076: t15.2023.08.27 val PER: 0.1865
2026-01-05 00:04:47,076: t15.2023.09.01 val PER: 0.0763
2026-01-05 00:04:47,076: t15.2023.09.03 val PER: 0.1746
2026-01-05 00:04:47,076: t15.2023.09.24 val PER: 0.1226
2026-01-05 00:04:47,076: t15.2023.09.29 val PER: 0.1385
2026-01-05 00:04:47,076: t15.2023.10.01 val PER: 0.1737
2026-01-05 00:04:47,076: t15.2023.10.06 val PER: 0.0926
2026-01-05 00:04:47,077: t15.2023.10.08 val PER: 0.2530
2026-01-05 00:04:47,077: t15.2023.10.13 val PER: 0.2048
2026-01-05 00:04:47,077: t15.2023.10.15 val PER: 0.1483
2026-01-05 00:04:47,077: t15.2023.10.20 val PER: 0.1846
2026-01-05 00:04:47,077: t15.2023.10.22 val PER: 0.1203
2026-01-05 00:04:47,077: t15.2023.11.03 val PER: 0.1757
2026-01-05 00:04:47,077: t15.2023.11.04 val PER: 0.0307
2026-01-05 00:04:47,077: t15.2023.11.17 val PER: 0.0404
2026-01-05 00:04:47,077: t15.2023.11.19 val PER: 0.0399
2026-01-05 00:04:47,077: t15.2023.11.26 val PER: 0.1290
2026-01-05 00:04:47,077: t15.2023.12.03 val PER: 0.1155
2026-01-05 00:04:47,077: t15.2023.12.08 val PER: 0.1032
2026-01-05 00:04:47,077: t15.2023.12.10 val PER: 0.0959
2026-01-05 00:04:47,078: t15.2023.12.17 val PER: 0.1424
2026-01-05 00:04:47,078: t15.2023.12.29 val PER: 0.1311
2026-01-05 00:04:47,078: t15.2024.02.25 val PER: 0.1067
2026-01-05 00:04:47,078: t15.2024.03.08 val PER: 0.2219
2026-01-05 00:04:47,078: t15.2024.03.15 val PER: 0.2089
2026-01-05 00:04:47,078: t15.2024.03.17 val PER: 0.1423
2026-01-05 00:04:47,078: t15.2024.05.10 val PER: 0.1545
2026-01-05 00:04:47,078: t15.2024.06.14 val PER: 0.1546
2026-01-05 00:04:47,078: t15.2024.07.19 val PER: 0.2307
2026-01-05 00:04:47,078: t15.2024.07.21 val PER: 0.0903
2026-01-05 00:04:47,078: t15.2024.07.28 val PER: 0.1316
2026-01-05 00:04:47,078: t15.2025.01.10 val PER: 0.3113
2026-01-05 00:04:47,082: t15.2025.01.12 val PER: 0.1386
2026-01-05 00:04:47,082: t15.2025.03.14 val PER: 0.3432
2026-01-05 00:04:47,082: t15.2025.03.16 val PER: 0.1911
2026-01-05 00:04:47,082: t15.2025.03.30 val PER: 0.2966
2026-01-05 00:04:47,082: t15.2025.04.13 val PER: 0.2254
2026-01-05 00:04:47,362: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_14000
2026-01-05 00:05:05,563: Train batch 14200: loss: 8.11 grad norm: 49.21 time: 0.056
2026-01-05 00:05:24,012: Train batch 14400: loss: 5.88 grad norm: 39.38 time: 0.063
2026-01-05 00:05:33,300: Running test after training batch: 14500
2026-01-05 00:05:33,451: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:05:38,497: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:05:38,531: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:05:40,325: Val batch 14500: PER (avg): 0.1520 CTC Loss (avg): 15.5335 WER(1gram): 46.45% (n=64) time: 7.024
2026-01-05 00:05:40,325: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 00:05:40,325: t15.2023.08.13 val PER: 0.1154
2026-01-05 00:05:40,325: t15.2023.08.18 val PER: 0.0997
2026-01-05 00:05:40,325: t15.2023.08.20 val PER: 0.1080
2026-01-05 00:05:40,326: t15.2023.08.25 val PER: 0.0889
2026-01-05 00:05:40,326: t15.2023.08.27 val PER: 0.1849
2026-01-05 00:05:40,326: t15.2023.09.01 val PER: 0.0787
2026-01-05 00:05:40,326: t15.2023.09.03 val PER: 0.1686
2026-01-05 00:05:40,326: t15.2023.09.24 val PER: 0.1286
2026-01-05 00:05:40,326: t15.2023.09.29 val PER: 0.1315
2026-01-05 00:05:40,326: t15.2023.10.01 val PER: 0.1803
2026-01-05 00:05:40,326: t15.2023.10.06 val PER: 0.0915
2026-01-05 00:05:40,326: t15.2023.10.08 val PER: 0.2530
2026-01-05 00:05:40,326: t15.2023.10.13 val PER: 0.2079
2026-01-05 00:05:40,326: t15.2023.10.15 val PER: 0.1529
2026-01-05 00:05:40,326: t15.2023.10.20 val PER: 0.1879
2026-01-05 00:05:40,326: t15.2023.10.22 val PER: 0.1192
2026-01-05 00:05:40,327: t15.2023.11.03 val PER: 0.1771
2026-01-05 00:05:40,327: t15.2023.11.04 val PER: 0.0375
2026-01-05 00:05:40,327: t15.2023.11.17 val PER: 0.0451
2026-01-05 00:05:40,327: t15.2023.11.19 val PER: 0.0399
2026-01-05 00:05:40,327: t15.2023.11.26 val PER: 0.1304
2026-01-05 00:05:40,327: t15.2023.12.03 val PER: 0.1113
2026-01-05 00:05:40,327: t15.2023.12.08 val PER: 0.1005
2026-01-05 00:05:40,327: t15.2023.12.10 val PER: 0.0894
2026-01-05 00:05:40,327: t15.2023.12.17 val PER: 0.1435
2026-01-05 00:05:40,327: t15.2023.12.29 val PER: 0.1283
2026-01-05 00:05:40,327: t15.2024.02.25 val PER: 0.1124
2026-01-05 00:05:40,327: t15.2024.03.08 val PER: 0.2205
2026-01-05 00:05:40,327: t15.2024.03.15 val PER: 0.2026
2026-01-05 00:05:40,327: t15.2024.03.17 val PER: 0.1360
2026-01-05 00:05:40,327: t15.2024.05.10 val PER: 0.1486
2026-01-05 00:05:40,327: t15.2024.06.14 val PER: 0.1640
2026-01-05 00:05:40,327: t15.2024.07.19 val PER: 0.2419
2026-01-05 00:05:40,328: t15.2024.07.21 val PER: 0.1000
2026-01-05 00:05:40,328: t15.2024.07.28 val PER: 0.1419
2026-01-05 00:05:40,328: t15.2025.01.10 val PER: 0.2934
2026-01-05 00:05:40,328: t15.2025.01.12 val PER: 0.1447
2026-01-05 00:05:40,328: t15.2025.03.14 val PER: 0.3432
2026-01-05 00:05:40,328: t15.2025.03.16 val PER: 0.1963
2026-01-05 00:05:40,328: t15.2025.03.30 val PER: 0.2862
2026-01-05 00:05:40,328: t15.2025.04.13 val PER: 0.2111
2026-01-05 00:05:40,620: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_14500
2026-01-05 00:05:49,736: Train batch 14600: loss: 12.44 grad norm: 59.47 time: 0.057
2026-01-05 00:06:08,241: Train batch 14800: loss: 5.76 grad norm: 44.15 time: 0.050
2026-01-05 00:06:26,709: Train batch 15000: loss: 8.68 grad norm: 50.08 time: 0.051
2026-01-05 00:06:26,709: Running test after training batch: 15000
2026-01-05 00:06:26,814: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:06:31,577: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:06:31,611: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:06:33,424: Val batch 15000: PER (avg): 0.1480 CTC Loss (avg): 15.2691 WER(1gram): 45.43% (n=64) time: 6.715
2026-01-05 00:06:33,425: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-05 00:06:33,425: t15.2023.08.13 val PER: 0.1071
2026-01-05 00:06:33,425: t15.2023.08.18 val PER: 0.0981
2026-01-05 00:06:33,426: t15.2023.08.20 val PER: 0.1096
2026-01-05 00:06:33,426: t15.2023.08.25 val PER: 0.0843
2026-01-05 00:06:33,426: t15.2023.08.27 val PER: 0.1849
2026-01-05 00:06:33,426: t15.2023.09.01 val PER: 0.0739
2026-01-05 00:06:33,426: t15.2023.09.03 val PER: 0.1568
2026-01-05 00:06:33,426: t15.2023.09.24 val PER: 0.1214
2026-01-05 00:06:33,426: t15.2023.09.29 val PER: 0.1334
2026-01-05 00:06:33,426: t15.2023.10.01 val PER: 0.1697
2026-01-05 00:06:33,426: t15.2023.10.06 val PER: 0.0818
2026-01-05 00:06:33,426: t15.2023.10.08 val PER: 0.2490
2026-01-05 00:06:33,426: t15.2023.10.13 val PER: 0.2048
2026-01-05 00:06:33,426: t15.2023.10.15 val PER: 0.1490
2026-01-05 00:06:33,426: t15.2023.10.20 val PER: 0.1913
2026-01-05 00:06:33,426: t15.2023.10.22 val PER: 0.1114
2026-01-05 00:06:33,427: t15.2023.11.03 val PER: 0.1764
2026-01-05 00:06:33,427: t15.2023.11.04 val PER: 0.0341
2026-01-05 00:06:33,427: t15.2023.11.17 val PER: 0.0404
2026-01-05 00:06:33,427: t15.2023.11.19 val PER: 0.0399
2026-01-05 00:06:33,427: t15.2023.11.26 val PER: 0.1188
2026-01-05 00:06:33,427: t15.2023.12.03 val PER: 0.0998
2026-01-05 00:06:33,427: t15.2023.12.08 val PER: 0.0999
2026-01-05 00:06:33,427: t15.2023.12.10 val PER: 0.0867
2026-01-05 00:06:33,427: t15.2023.12.17 val PER: 0.1383
2026-01-05 00:06:33,427: t15.2023.12.29 val PER: 0.1256
2026-01-05 00:06:33,427: t15.2024.02.25 val PER: 0.0997
2026-01-05 00:06:33,427: t15.2024.03.08 val PER: 0.2219
2026-01-05 00:06:33,427: t15.2024.03.15 val PER: 0.2014
2026-01-05 00:06:33,428: t15.2024.03.17 val PER: 0.1360
2026-01-05 00:06:33,428: t15.2024.05.10 val PER: 0.1649
2026-01-05 00:06:33,428: t15.2024.06.14 val PER: 0.1530
2026-01-05 00:06:33,428: t15.2024.07.19 val PER: 0.2380
2026-01-05 00:06:33,428: t15.2024.07.21 val PER: 0.0897
2026-01-05 00:06:33,428: t15.2024.07.28 val PER: 0.1287
2026-01-05 00:06:33,428: t15.2025.01.10 val PER: 0.3030
2026-01-05 00:06:33,428: t15.2025.01.12 val PER: 0.1363
2026-01-05 00:06:33,428: t15.2025.03.14 val PER: 0.3462
2026-01-05 00:06:33,428: t15.2025.03.16 val PER: 0.1832
2026-01-05 00:06:33,428: t15.2025.03.30 val PER: 0.2874
2026-01-05 00:06:33,428: t15.2025.04.13 val PER: 0.2240
2026-01-05 00:06:33,429: New best val WER(1gram) 45.94% --> 45.43%
2026-01-05 00:06:33,429: Checkpointing model
2026-01-05 00:06:34,051: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-05 00:06:34,352: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_15000
2026-01-05 00:06:53,100: Train batch 15200: loss: 5.06 grad norm: 42.24 time: 0.056
2026-01-05 00:07:11,277: Train batch 15400: loss: 11.68 grad norm: 61.83 time: 0.048
2026-01-05 00:07:20,501: Running test after training batch: 15500
2026-01-05 00:07:20,675: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:07:25,629: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:07:25,663: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:07:27,480: Val batch 15500: PER (avg): 0.1495 CTC Loss (avg): 15.2735 WER(1gram): 44.67% (n=64) time: 6.979
2026-01-05 00:07:27,481: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-05 00:07:27,481: t15.2023.08.13 val PER: 0.1164
2026-01-05 00:07:27,481: t15.2023.08.18 val PER: 0.1073
2026-01-05 00:07:27,481: t15.2023.08.20 val PER: 0.1056
2026-01-05 00:07:27,481: t15.2023.08.25 val PER: 0.0979
2026-01-05 00:07:27,481: t15.2023.08.27 val PER: 0.1945
2026-01-05 00:07:27,482: t15.2023.09.01 val PER: 0.0690
2026-01-05 00:07:27,482: t15.2023.09.03 val PER: 0.1544
2026-01-05 00:07:27,482: t15.2023.09.24 val PER: 0.1274
2026-01-05 00:07:27,482: t15.2023.09.29 val PER: 0.1321
2026-01-05 00:07:27,482: t15.2023.10.01 val PER: 0.1724
2026-01-05 00:07:27,482: t15.2023.10.06 val PER: 0.0861
2026-01-05 00:07:27,482: t15.2023.10.08 val PER: 0.2503
2026-01-05 00:07:27,482: t15.2023.10.13 val PER: 0.2017
2026-01-05 00:07:27,482: t15.2023.10.15 val PER: 0.1404
2026-01-05 00:07:27,482: t15.2023.10.20 val PER: 0.2047
2026-01-05 00:07:27,483: t15.2023.10.22 val PER: 0.1225
2026-01-05 00:07:27,483: t15.2023.11.03 val PER: 0.1791
2026-01-05 00:07:27,483: t15.2023.11.04 val PER: 0.0341
2026-01-05 00:07:27,483: t15.2023.11.17 val PER: 0.0342
2026-01-05 00:07:27,483: t15.2023.11.19 val PER: 0.0419
2026-01-05 00:07:27,483: t15.2023.11.26 val PER: 0.1225
2026-01-05 00:07:27,483: t15.2023.12.03 val PER: 0.1103
2026-01-05 00:07:27,483: t15.2023.12.08 val PER: 0.1019
2026-01-05 00:07:27,483: t15.2023.12.10 val PER: 0.0959
2026-01-05 00:07:27,483: t15.2023.12.17 val PER: 0.1383
2026-01-05 00:07:27,483: t15.2023.12.29 val PER: 0.1277
2026-01-05 00:07:27,483: t15.2024.02.25 val PER: 0.1067
2026-01-05 00:07:27,483: t15.2024.03.08 val PER: 0.2219
2026-01-05 00:07:27,483: t15.2024.03.15 val PER: 0.2070
2026-01-05 00:07:27,483: t15.2024.03.17 val PER: 0.1409
2026-01-05 00:07:27,483: t15.2024.05.10 val PER: 0.1501
2026-01-05 00:07:27,483: t15.2024.06.14 val PER: 0.1656
2026-01-05 00:07:27,484: t15.2024.07.19 val PER: 0.2307
2026-01-05 00:07:27,484: t15.2024.07.21 val PER: 0.0883
2026-01-05 00:07:27,484: t15.2024.07.28 val PER: 0.1324
2026-01-05 00:07:27,484: t15.2025.01.10 val PER: 0.3030
2026-01-05 00:07:27,484: t15.2025.01.12 val PER: 0.1378
2026-01-05 00:07:27,484: t15.2025.03.14 val PER: 0.3373
2026-01-05 00:07:27,484: t15.2025.03.16 val PER: 0.1859
2026-01-05 00:07:27,484: t15.2025.03.30 val PER: 0.2943
2026-01-05 00:07:27,485: t15.2025.04.13 val PER: 0.2011
2026-01-05 00:07:27,486: New best val WER(1gram) 45.43% --> 44.67%
2026-01-05 00:07:27,486: Checkpointing model
2026-01-05 00:07:28,119: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-05 00:07:28,413: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_15500
2026-01-05 00:07:37,330: Train batch 15600: loss: 11.29 grad norm: 53.88 time: 0.062
2026-01-05 00:07:55,021: Train batch 15800: loss: 13.48 grad norm: 64.16 time: 0.066
2026-01-05 00:08:12,915: Train batch 16000: loss: 8.38 grad norm: 43.38 time: 0.055
2026-01-05 00:08:12,916: Running test after training batch: 16000
2026-01-05 00:08:13,023: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:08:17,780: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:08:17,814: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:08:19,651: Val batch 16000: PER (avg): 0.1492 CTC Loss (avg): 15.3230 WER(1gram): 46.95% (n=64) time: 6.735
2026-01-05 00:08:19,651: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-05 00:08:19,651: t15.2023.08.13 val PER: 0.1195
2026-01-05 00:08:19,652: t15.2023.08.18 val PER: 0.1023
2026-01-05 00:08:19,652: t15.2023.08.20 val PER: 0.1048
2026-01-05 00:08:19,652: t15.2023.08.25 val PER: 0.0934
2026-01-05 00:08:19,652: t15.2023.08.27 val PER: 0.1817
2026-01-05 00:08:19,652: t15.2023.09.01 val PER: 0.0714
2026-01-05 00:08:19,652: t15.2023.09.03 val PER: 0.1485
2026-01-05 00:08:19,652: t15.2023.09.24 val PER: 0.1238
2026-01-05 00:08:19,652: t15.2023.09.29 val PER: 0.1385
2026-01-05 00:08:19,652: t15.2023.10.01 val PER: 0.1691
2026-01-05 00:08:19,652: t15.2023.10.06 val PER: 0.0893
2026-01-05 00:08:19,652: t15.2023.10.08 val PER: 0.2612
2026-01-05 00:08:19,652: t15.2023.10.13 val PER: 0.1932
2026-01-05 00:08:19,652: t15.2023.10.15 val PER: 0.1417
2026-01-05 00:08:19,652: t15.2023.10.20 val PER: 0.2013
2026-01-05 00:08:19,652: t15.2023.10.22 val PER: 0.1147
2026-01-05 00:08:19,652: t15.2023.11.03 val PER: 0.1798
2026-01-05 00:08:19,653: t15.2023.11.04 val PER: 0.0307
2026-01-05 00:08:19,653: t15.2023.11.17 val PER: 0.0404
2026-01-05 00:08:19,653: t15.2023.11.19 val PER: 0.0419
2026-01-05 00:08:19,653: t15.2023.11.26 val PER: 0.1232
2026-01-05 00:08:19,653: t15.2023.12.03 val PER: 0.1134
2026-01-05 00:08:19,653: t15.2023.12.08 val PER: 0.0985
2026-01-05 00:08:19,653: t15.2023.12.10 val PER: 0.0933
2026-01-05 00:08:19,654: t15.2023.12.17 val PER: 0.1393
2026-01-05 00:08:19,654: t15.2023.12.29 val PER: 0.1215
2026-01-05 00:08:19,654: t15.2024.02.25 val PER: 0.1011
2026-01-05 00:08:19,654: t15.2024.03.08 val PER: 0.2262
2026-01-05 00:08:19,654: t15.2024.03.15 val PER: 0.2008
2026-01-05 00:08:19,654: t15.2024.03.17 val PER: 0.1360
2026-01-05 00:08:19,654: t15.2024.05.10 val PER: 0.1545
2026-01-05 00:08:19,654: t15.2024.06.14 val PER: 0.1562
2026-01-05 00:08:19,654: t15.2024.07.19 val PER: 0.2334
2026-01-05 00:08:19,654: t15.2024.07.21 val PER: 0.0931
2026-01-05 00:08:19,654: t15.2024.07.28 val PER: 0.1382
2026-01-05 00:08:19,654: t15.2025.01.10 val PER: 0.3017
2026-01-05 00:08:19,654: t15.2025.01.12 val PER: 0.1363
2026-01-05 00:08:19,654: t15.2025.03.14 val PER: 0.3388
2026-01-05 00:08:19,654: t15.2025.03.16 val PER: 0.1976
2026-01-05 00:08:19,654: t15.2025.03.30 val PER: 0.2874
2026-01-05 00:08:19,655: t15.2025.04.13 val PER: 0.2240
2026-01-05 00:08:19,958: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_16000
2026-01-05 00:08:37,683: Train batch 16200: loss: 6.34 grad norm: 46.23 time: 0.055
2026-01-05 00:08:55,549: Train batch 16400: loss: 10.18 grad norm: 59.20 time: 0.057
2026-01-05 00:09:04,586: Running test after training batch: 16500
2026-01-05 00:09:04,683: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:09:09,668: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:09:09,702: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:09:11,542: Val batch 16500: PER (avg): 0.1480 CTC Loss (avg): 15.1427 WER(1gram): 44.42% (n=64) time: 6.955
2026-01-05 00:09:11,542: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 00:09:11,542: t15.2023.08.13 val PER: 0.1112
2026-01-05 00:09:11,542: t15.2023.08.18 val PER: 0.0981
2026-01-05 00:09:11,542: t15.2023.08.20 val PER: 0.1048
2026-01-05 00:09:11,543: t15.2023.08.25 val PER: 0.0919
2026-01-05 00:09:11,543: t15.2023.08.27 val PER: 0.1768
2026-01-05 00:09:11,543: t15.2023.09.01 val PER: 0.0739
2026-01-05 00:09:11,543: t15.2023.09.03 val PER: 0.1603
2026-01-05 00:09:11,543: t15.2023.09.24 val PER: 0.1274
2026-01-05 00:09:11,543: t15.2023.09.29 val PER: 0.1295
2026-01-05 00:09:11,543: t15.2023.10.01 val PER: 0.1678
2026-01-05 00:09:11,543: t15.2023.10.06 val PER: 0.0893
2026-01-05 00:09:11,543: t15.2023.10.08 val PER: 0.2571
2026-01-05 00:09:11,543: t15.2023.10.13 val PER: 0.1971
2026-01-05 00:09:11,543: t15.2023.10.15 val PER: 0.1417
2026-01-05 00:09:11,543: t15.2023.10.20 val PER: 0.2114
2026-01-05 00:09:11,543: t15.2023.10.22 val PER: 0.1258
2026-01-05 00:09:11,543: t15.2023.11.03 val PER: 0.1798
2026-01-05 00:09:11,543: t15.2023.11.04 val PER: 0.0307
2026-01-05 00:09:11,544: t15.2023.11.17 val PER: 0.0389
2026-01-05 00:09:11,544: t15.2023.11.19 val PER: 0.0419
2026-01-05 00:09:11,544: t15.2023.11.26 val PER: 0.1123
2026-01-05 00:09:11,544: t15.2023.12.03 val PER: 0.1029
2026-01-05 00:09:11,544: t15.2023.12.08 val PER: 0.0992
2026-01-05 00:09:11,544: t15.2023.12.10 val PER: 0.0907
2026-01-05 00:09:11,544: t15.2023.12.17 val PER: 0.1351
2026-01-05 00:09:11,544: t15.2023.12.29 val PER: 0.1201
2026-01-05 00:09:11,544: t15.2024.02.25 val PER: 0.0941
2026-01-05 00:09:11,544: t15.2024.03.08 val PER: 0.2390
2026-01-05 00:09:11,544: t15.2024.03.15 val PER: 0.2014
2026-01-05 00:09:11,544: t15.2024.03.17 val PER: 0.1297
2026-01-05 00:09:11,544: t15.2024.05.10 val PER: 0.1530
2026-01-05 00:09:11,544: t15.2024.06.14 val PER: 0.1593
2026-01-05 00:09:11,544: t15.2024.07.19 val PER: 0.2347
2026-01-05 00:09:11,544: t15.2024.07.21 val PER: 0.0938
2026-01-05 00:09:11,544: t15.2024.07.28 val PER: 0.1324
2026-01-05 00:09:11,545: t15.2025.01.10 val PER: 0.2934
2026-01-05 00:09:11,545: t15.2025.01.12 val PER: 0.1378
2026-01-05 00:09:11,545: t15.2025.03.14 val PER: 0.3506
2026-01-05 00:09:11,545: t15.2025.03.16 val PER: 0.1793
2026-01-05 00:09:11,545: t15.2025.03.30 val PER: 0.2966
2026-01-05 00:09:11,545: t15.2025.04.13 val PER: 0.2225
2026-01-05 00:09:11,546: New best val WER(1gram) 44.67% --> 44.42%
2026-01-05 00:09:11,546: Checkpointing model
2026-01-05 00:09:12,179: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-05 00:09:12,479: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_16500
2026-01-05 00:09:21,594: Train batch 16600: loss: 8.35 grad norm: 52.60 time: 0.052
2026-01-05 00:09:40,168: Train batch 16800: loss: 16.23 grad norm: 72.42 time: 0.062
2026-01-05 00:09:58,960: Train batch 17000: loss: 7.82 grad norm: 47.20 time: 0.082
2026-01-05 00:09:58,961: Running test after training batch: 17000
2026-01-05 00:09:59,059: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:10:03,801: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:10:03,834: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:10:05,700: Val batch 17000: PER (avg): 0.1473 CTC Loss (avg): 15.0641 WER(1gram): 45.43% (n=64) time: 6.739
2026-01-05 00:10:05,701: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-05 00:10:05,701: t15.2023.08.13 val PER: 0.1164
2026-01-05 00:10:05,701: t15.2023.08.18 val PER: 0.1090
2026-01-05 00:10:05,701: t15.2023.08.20 val PER: 0.1056
2026-01-05 00:10:05,701: t15.2023.08.25 val PER: 0.0904
2026-01-05 00:10:05,701: t15.2023.08.27 val PER: 0.1768
2026-01-05 00:10:05,701: t15.2023.09.01 val PER: 0.0731
2026-01-05 00:10:05,701: t15.2023.09.03 val PER: 0.1556
2026-01-05 00:10:05,701: t15.2023.09.24 val PER: 0.1299
2026-01-05 00:10:05,701: t15.2023.09.29 val PER: 0.1321
2026-01-05 00:10:05,701: t15.2023.10.01 val PER: 0.1678
2026-01-05 00:10:05,701: t15.2023.10.06 val PER: 0.0818
2026-01-05 00:10:05,701: t15.2023.10.08 val PER: 0.2395
2026-01-05 00:10:05,702: t15.2023.10.13 val PER: 0.1947
2026-01-05 00:10:05,702: t15.2023.10.15 val PER: 0.1444
2026-01-05 00:10:05,702: t15.2023.10.20 val PER: 0.1913
2026-01-05 00:10:05,702: t15.2023.10.22 val PER: 0.1169
2026-01-05 00:10:05,702: t15.2023.11.03 val PER: 0.1777
2026-01-05 00:10:05,702: t15.2023.11.04 val PER: 0.0307
2026-01-05 00:10:05,702: t15.2023.11.17 val PER: 0.0389
2026-01-05 00:10:05,702: t15.2023.11.19 val PER: 0.0379
2026-01-05 00:10:05,702: t15.2023.11.26 val PER: 0.1101
2026-01-05 00:10:05,702: t15.2023.12.03 val PER: 0.1029
2026-01-05 00:10:05,702: t15.2023.12.08 val PER: 0.0965
2026-01-05 00:10:05,702: t15.2023.12.10 val PER: 0.0894
2026-01-05 00:10:05,702: t15.2023.12.17 val PER: 0.1289
2026-01-05 00:10:05,702: t15.2023.12.29 val PER: 0.1208
2026-01-05 00:10:05,702: t15.2024.02.25 val PER: 0.0997
2026-01-05 00:10:05,703: t15.2024.03.08 val PER: 0.2304
2026-01-05 00:10:05,703: t15.2024.03.15 val PER: 0.2020
2026-01-05 00:10:05,703: t15.2024.03.17 val PER: 0.1388
2026-01-05 00:10:05,703: t15.2024.05.10 val PER: 0.1634
2026-01-05 00:10:05,703: t15.2024.06.14 val PER: 0.1640
2026-01-05 00:10:05,703: t15.2024.07.19 val PER: 0.2340
2026-01-05 00:10:05,703: t15.2024.07.21 val PER: 0.0903
2026-01-05 00:10:05,703: t15.2024.07.28 val PER: 0.1287
2026-01-05 00:10:05,703: t15.2025.01.10 val PER: 0.2934
2026-01-05 00:10:05,703: t15.2025.01.12 val PER: 0.1401
2026-01-05 00:10:05,703: t15.2025.03.14 val PER: 0.3388
2026-01-05 00:10:05,703: t15.2025.03.16 val PER: 0.1937
2026-01-05 00:10:05,703: t15.2025.03.30 val PER: 0.2874
2026-01-05 00:10:05,703: t15.2025.04.13 val PER: 0.2140
2026-01-05 00:10:05,992: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_17000
2026-01-05 00:10:24,566: Train batch 17200: loss: 9.11 grad norm: 47.79 time: 0.084
2026-01-05 00:10:43,077: Train batch 17400: loss: 11.70 grad norm: 60.04 time: 0.071
2026-01-05 00:10:52,278: Running test after training batch: 17500
2026-01-05 00:10:52,464: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:10:57,169: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:10:57,204: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost get
2026-01-05 00:10:59,119: Val batch 17500: PER (avg): 0.1463 CTC Loss (avg): 15.0630 WER(1gram): 44.67% (n=64) time: 6.840
2026-01-05 00:10:59,119: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-05 00:10:59,119: t15.2023.08.13 val PER: 0.1060
2026-01-05 00:10:59,119: t15.2023.08.18 val PER: 0.1014
2026-01-05 00:10:59,119: t15.2023.08.20 val PER: 0.1056
2026-01-05 00:10:59,119: t15.2023.08.25 val PER: 0.0889
2026-01-05 00:10:59,119: t15.2023.08.27 val PER: 0.1833
2026-01-05 00:10:59,120: t15.2023.09.01 val PER: 0.0722
2026-01-05 00:10:59,120: t15.2023.09.03 val PER: 0.1532
2026-01-05 00:10:59,120: t15.2023.09.24 val PER: 0.1238
2026-01-05 00:10:59,120: t15.2023.09.29 val PER: 0.1295
2026-01-05 00:10:59,120: t15.2023.10.01 val PER: 0.1658
2026-01-05 00:10:59,120: t15.2023.10.06 val PER: 0.0829
2026-01-05 00:10:59,120: t15.2023.10.08 val PER: 0.2476
2026-01-05 00:10:59,120: t15.2023.10.13 val PER: 0.1947
2026-01-05 00:10:59,120: t15.2023.10.15 val PER: 0.1437
2026-01-05 00:10:59,120: t15.2023.10.20 val PER: 0.2047
2026-01-05 00:10:59,120: t15.2023.10.22 val PER: 0.1180
2026-01-05 00:10:59,120: t15.2023.11.03 val PER: 0.1798
2026-01-05 00:10:59,120: t15.2023.11.04 val PER: 0.0273
2026-01-05 00:10:59,121: t15.2023.11.17 val PER: 0.0389
2026-01-05 00:10:59,121: t15.2023.11.19 val PER: 0.0399
2026-01-05 00:10:59,121: t15.2023.11.26 val PER: 0.1174
2026-01-05 00:10:59,121: t15.2023.12.03 val PER: 0.1061
2026-01-05 00:10:59,121: t15.2023.12.08 val PER: 0.1005
2026-01-05 00:10:59,121: t15.2023.12.10 val PER: 0.0880
2026-01-05 00:10:59,121: t15.2023.12.17 val PER: 0.1310
2026-01-05 00:10:59,121: t15.2023.12.29 val PER: 0.1208
2026-01-05 00:10:59,121: t15.2024.02.25 val PER: 0.0941
2026-01-05 00:10:59,121: t15.2024.03.08 val PER: 0.2290
2026-01-05 00:10:59,121: t15.2024.03.15 val PER: 0.1995
2026-01-05 00:10:59,121: t15.2024.03.17 val PER: 0.1339
2026-01-05 00:10:59,121: t15.2024.05.10 val PER: 0.1590
2026-01-05 00:10:59,121: t15.2024.06.14 val PER: 0.1546
2026-01-05 00:10:59,121: t15.2024.07.19 val PER: 0.2340
2026-01-05 00:10:59,122: t15.2024.07.21 val PER: 0.0876
2026-01-05 00:10:59,122: t15.2024.07.28 val PER: 0.1301
2026-01-05 00:10:59,122: t15.2025.01.10 val PER: 0.2934
2026-01-05 00:10:59,122: t15.2025.01.12 val PER: 0.1316
2026-01-05 00:10:59,122: t15.2025.03.14 val PER: 0.3476
2026-01-05 00:10:59,122: t15.2025.03.16 val PER: 0.1859
2026-01-05 00:10:59,122: t15.2025.03.30 val PER: 0.2793
2026-01-05 00:10:59,122: t15.2025.04.13 val PER: 0.2140
2026-01-05 00:10:59,400: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_17500
2026-01-05 00:11:08,577: Train batch 17600: loss: 9.42 grad norm: 55.46 time: 0.051
2026-01-05 00:11:27,483: Train batch 17800: loss: 6.04 grad norm: 46.96 time: 0.041
2026-01-05 00:11:46,012: Train batch 18000: loss: 10.98 grad norm: 63.82 time: 0.060
2026-01-05 00:11:46,013: Running test after training batch: 18000
2026-01-05 00:11:46,209: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:11:50,968: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:11:51,003: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:11:52,931: Val batch 18000: PER (avg): 0.1446 CTC Loss (avg): 15.0071 WER(1gram): 45.94% (n=64) time: 6.918
2026-01-05 00:11:52,931: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-05 00:11:52,931: t15.2023.08.13 val PER: 0.1123
2026-01-05 00:11:52,931: t15.2023.08.18 val PER: 0.1023
2026-01-05 00:11:52,931: t15.2023.08.20 val PER: 0.1056
2026-01-05 00:11:52,931: t15.2023.08.25 val PER: 0.0858
2026-01-05 00:11:52,931: t15.2023.08.27 val PER: 0.1849
2026-01-05 00:11:52,932: t15.2023.09.01 val PER: 0.0690
2026-01-05 00:11:52,932: t15.2023.09.03 val PER: 0.1568
2026-01-05 00:11:52,932: t15.2023.09.24 val PER: 0.1286
2026-01-05 00:11:52,932: t15.2023.09.29 val PER: 0.1264
2026-01-05 00:11:52,932: t15.2023.10.01 val PER: 0.1704
2026-01-05 00:11:52,932: t15.2023.10.06 val PER: 0.0786
2026-01-05 00:11:52,932: t15.2023.10.08 val PER: 0.2436
2026-01-05 00:11:52,932: t15.2023.10.13 val PER: 0.1939
2026-01-05 00:11:52,932: t15.2023.10.15 val PER: 0.1450
2026-01-05 00:11:52,932: t15.2023.10.20 val PER: 0.2013
2026-01-05 00:11:52,932: t15.2023.10.22 val PER: 0.1102
2026-01-05 00:11:52,932: t15.2023.11.03 val PER: 0.1703
2026-01-05 00:11:52,932: t15.2023.11.04 val PER: 0.0307
2026-01-05 00:11:52,932: t15.2023.11.17 val PER: 0.0373
2026-01-05 00:11:52,932: t15.2023.11.19 val PER: 0.0399
2026-01-05 00:11:52,933: t15.2023.11.26 val PER: 0.1080
2026-01-05 00:11:52,933: t15.2023.12.03 val PER: 0.0987
2026-01-05 00:11:52,933: t15.2023.12.08 val PER: 0.0965
2026-01-05 00:11:52,933: t15.2023.12.10 val PER: 0.0828
2026-01-05 00:11:52,933: t15.2023.12.17 val PER: 0.1227
2026-01-05 00:11:52,933: t15.2023.12.29 val PER: 0.1208
2026-01-05 00:11:52,933: t15.2024.02.25 val PER: 0.0927
2026-01-05 00:11:52,933: t15.2024.03.08 val PER: 0.2276
2026-01-05 00:11:52,933: t15.2024.03.15 val PER: 0.1970
2026-01-05 00:11:52,933: t15.2024.03.17 val PER: 0.1304
2026-01-05 00:11:52,933: t15.2024.05.10 val PER: 0.1471
2026-01-05 00:11:52,933: t15.2024.06.14 val PER: 0.1530
2026-01-05 00:11:52,933: t15.2024.07.19 val PER: 0.2281
2026-01-05 00:11:52,933: t15.2024.07.21 val PER: 0.0917
2026-01-05 00:11:52,933: t15.2024.07.28 val PER: 0.1294
2026-01-05 00:11:52,933: t15.2025.01.10 val PER: 0.2934
2026-01-05 00:11:52,934: t15.2025.01.12 val PER: 0.1363
2026-01-05 00:11:52,934: t15.2025.03.14 val PER: 0.3358
2026-01-05 00:11:52,934: t15.2025.03.16 val PER: 0.1924
2026-01-05 00:11:52,934: t15.2025.03.30 val PER: 0.2828
2026-01-05 00:11:52,934: t15.2025.04.13 val PER: 0.2183
2026-01-05 00:11:53,317: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_18000
2026-01-05 00:12:11,756: Train batch 18200: loss: 7.18 grad norm: 45.78 time: 0.072
2026-01-05 00:12:29,924: Train batch 18400: loss: 5.14 grad norm: 46.18 time: 0.057
2026-01-05 00:12:39,130: Running test after training batch: 18500
2026-01-05 00:12:39,287: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:12:43,975: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:12:44,010: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:12:45,904: Val batch 18500: PER (avg): 0.1462 CTC Loss (avg): 15.0540 WER(1gram): 45.18% (n=64) time: 6.773
2026-01-05 00:12:45,904: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-05 00:12:45,905: t15.2023.08.13 val PER: 0.1112
2026-01-05 00:12:45,905: t15.2023.08.18 val PER: 0.1081
2026-01-05 00:12:45,905: t15.2023.08.20 val PER: 0.1072
2026-01-05 00:12:45,905: t15.2023.08.25 val PER: 0.0919
2026-01-05 00:12:45,905: t15.2023.08.27 val PER: 0.1865
2026-01-05 00:12:45,905: t15.2023.09.01 val PER: 0.0690
2026-01-05 00:12:45,905: t15.2023.09.03 val PER: 0.1473
2026-01-05 00:12:45,905: t15.2023.09.24 val PER: 0.1262
2026-01-05 00:12:45,905: t15.2023.09.29 val PER: 0.1334
2026-01-05 00:12:45,905: t15.2023.10.01 val PER: 0.1684
2026-01-05 00:12:45,905: t15.2023.10.06 val PER: 0.0872
2026-01-05 00:12:45,906: t15.2023.10.08 val PER: 0.2503
2026-01-05 00:12:45,906: t15.2023.10.13 val PER: 0.1916
2026-01-05 00:12:45,906: t15.2023.10.15 val PER: 0.1417
2026-01-05 00:12:45,906: t15.2023.10.20 val PER: 0.1946
2026-01-05 00:12:45,906: t15.2023.10.22 val PER: 0.1147
2026-01-05 00:12:45,906: t15.2023.11.03 val PER: 0.1757
2026-01-05 00:12:45,906: t15.2023.11.04 val PER: 0.0307
2026-01-05 00:12:45,906: t15.2023.11.17 val PER: 0.0373
2026-01-05 00:12:45,906: t15.2023.11.19 val PER: 0.0339
2026-01-05 00:12:45,906: t15.2023.11.26 val PER: 0.1101
2026-01-05 00:12:45,906: t15.2023.12.03 val PER: 0.1050
2026-01-05 00:12:45,907: t15.2023.12.08 val PER: 0.0985
2026-01-05 00:12:45,907: t15.2023.12.10 val PER: 0.0880
2026-01-05 00:12:45,907: t15.2023.12.17 val PER: 0.1299
2026-01-05 00:12:45,907: t15.2023.12.29 val PER: 0.1174
2026-01-05 00:12:45,907: t15.2024.02.25 val PER: 0.0983
2026-01-05 00:12:45,907: t15.2024.03.08 val PER: 0.2248
2026-01-05 00:12:45,907: t15.2024.03.15 val PER: 0.1995
2026-01-05 00:12:45,907: t15.2024.03.17 val PER: 0.1318
2026-01-05 00:12:45,907: t15.2024.05.10 val PER: 0.1530
2026-01-05 00:12:45,907: t15.2024.06.14 val PER: 0.1530
2026-01-05 00:12:45,907: t15.2024.07.19 val PER: 0.2347
2026-01-05 00:12:45,907: t15.2024.07.21 val PER: 0.0917
2026-01-05 00:12:45,907: t15.2024.07.28 val PER: 0.1257
2026-01-05 00:12:45,907: t15.2025.01.10 val PER: 0.2975
2026-01-05 00:12:45,907: t15.2025.01.12 val PER: 0.1355
2026-01-05 00:12:45,907: t15.2025.03.14 val PER: 0.3462
2026-01-05 00:12:45,907: t15.2025.03.16 val PER: 0.1885
2026-01-05 00:12:45,907: t15.2025.03.30 val PER: 0.2874
2026-01-05 00:12:45,908: t15.2025.04.13 val PER: 0.2111
2026-01-05 00:12:46,177: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_18500
2026-01-05 00:12:55,281: Train batch 18600: loss: 12.62 grad norm: 64.27 time: 0.066
2026-01-05 00:13:13,510: Train batch 18800: loss: 8.30 grad norm: 51.57 time: 0.063
2026-01-05 00:13:31,876: Train batch 19000: loss: 8.13 grad norm: 45.33 time: 0.063
2026-01-05 00:13:31,876: Running test after training batch: 19000
2026-01-05 00:13:31,972: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:13:36,746: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:13:36,782: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:13:38,686: Val batch 19000: PER (avg): 0.1461 CTC Loss (avg): 15.0604 WER(1gram): 44.42% (n=64) time: 6.809
2026-01-05 00:13:38,686: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-05 00:13:38,686: t15.2023.08.13 val PER: 0.1081
2026-01-05 00:13:38,686: t15.2023.08.18 val PER: 0.1065
2026-01-05 00:13:38,686: t15.2023.08.20 val PER: 0.1025
2026-01-05 00:13:38,686: t15.2023.08.25 val PER: 0.0904
2026-01-05 00:13:38,686: t15.2023.08.27 val PER: 0.1817
2026-01-05 00:13:38,686: t15.2023.09.01 val PER: 0.0698
2026-01-05 00:13:38,687: t15.2023.09.03 val PER: 0.1520
2026-01-05 00:13:38,687: t15.2023.09.24 val PER: 0.1299
2026-01-05 00:13:38,687: t15.2023.09.29 val PER: 0.1334
2026-01-05 00:13:38,687: t15.2023.10.01 val PER: 0.1691
2026-01-05 00:13:38,687: t15.2023.10.06 val PER: 0.0861
2026-01-05 00:13:38,687: t15.2023.10.08 val PER: 0.2517
2026-01-05 00:13:38,687: t15.2023.10.13 val PER: 0.1885
2026-01-05 00:13:38,687: t15.2023.10.15 val PER: 0.1391
2026-01-05 00:13:38,687: t15.2023.10.20 val PER: 0.2047
2026-01-05 00:13:38,687: t15.2023.10.22 val PER: 0.1147
2026-01-05 00:13:38,687: t15.2023.11.03 val PER: 0.1791
2026-01-05 00:13:38,688: t15.2023.11.04 val PER: 0.0273
2026-01-05 00:13:38,688: t15.2023.11.17 val PER: 0.0358
2026-01-05 00:13:38,688: t15.2023.11.19 val PER: 0.0359
2026-01-05 00:13:38,688: t15.2023.11.26 val PER: 0.1116
2026-01-05 00:13:38,688: t15.2023.12.03 val PER: 0.1050
2026-01-05 00:13:38,688: t15.2023.12.08 val PER: 0.0939
2026-01-05 00:13:38,688: t15.2023.12.10 val PER: 0.0907
2026-01-05 00:13:38,688: t15.2023.12.17 val PER: 0.1289
2026-01-05 00:13:38,688: t15.2023.12.29 val PER: 0.1229
2026-01-05 00:13:38,688: t15.2024.02.25 val PER: 0.0969
2026-01-05 00:13:38,688: t15.2024.03.08 val PER: 0.2233
2026-01-05 00:13:38,688: t15.2024.03.15 val PER: 0.1964
2026-01-05 00:13:38,688: t15.2024.03.17 val PER: 0.1332
2026-01-05 00:13:38,688: t15.2024.05.10 val PER: 0.1516
2026-01-05 00:13:38,688: t15.2024.06.14 val PER: 0.1609
2026-01-05 00:13:38,689: t15.2024.07.19 val PER: 0.2327
2026-01-05 00:13:38,689: t15.2024.07.21 val PER: 0.0903
2026-01-05 00:13:38,689: t15.2024.07.28 val PER: 0.1309
2026-01-05 00:13:38,689: t15.2025.01.10 val PER: 0.2975
2026-01-05 00:13:38,689: t15.2025.01.12 val PER: 0.1332
2026-01-05 00:13:38,689: t15.2025.03.14 val PER: 0.3447
2026-01-05 00:13:38,689: t15.2025.03.16 val PER: 0.1937
2026-01-05 00:13:38,689: t15.2025.03.30 val PER: 0.2782
2026-01-05 00:13:38,689: t15.2025.04.13 val PER: 0.2197
2026-01-05 00:13:38,976: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_19000
2026-01-05 00:13:56,826: Train batch 19200: loss: 5.68 grad norm: 46.78 time: 0.063
2026-01-05 00:14:14,894: Train batch 19400: loss: 4.81 grad norm: 35.94 time: 0.053
2026-01-05 00:14:23,858: Running test after training batch: 19500
2026-01-05 00:14:24,020: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:14:28,749: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:14:28,784: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:14:30,691: Val batch 19500: PER (avg): 0.1459 CTC Loss (avg): 14.9892 WER(1gram): 44.42% (n=64) time: 6.832
2026-01-05 00:14:30,691: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-05 00:14:30,691: t15.2023.08.13 val PER: 0.1102
2026-01-05 00:14:30,691: t15.2023.08.18 val PER: 0.1014
2026-01-05 00:14:30,691: t15.2023.08.20 val PER: 0.1048
2026-01-05 00:14:30,691: t15.2023.08.25 val PER: 0.0904
2026-01-05 00:14:30,691: t15.2023.08.27 val PER: 0.1817
2026-01-05 00:14:30,691: t15.2023.09.01 val PER: 0.0682
2026-01-05 00:14:30,691: t15.2023.09.03 val PER: 0.1532
2026-01-05 00:14:30,692: t15.2023.09.24 val PER: 0.1311
2026-01-05 00:14:30,692: t15.2023.09.29 val PER: 0.1366
2026-01-05 00:14:30,692: t15.2023.10.01 val PER: 0.1631
2026-01-05 00:14:30,692: t15.2023.10.06 val PER: 0.0915
2026-01-05 00:14:30,692: t15.2023.10.08 val PER: 0.2476
2026-01-05 00:14:30,692: t15.2023.10.13 val PER: 0.1885
2026-01-05 00:14:30,692: t15.2023.10.15 val PER: 0.1424
2026-01-05 00:14:30,692: t15.2023.10.20 val PER: 0.2081
2026-01-05 00:14:30,692: t15.2023.10.22 val PER: 0.1180
2026-01-05 00:14:30,692: t15.2023.11.03 val PER: 0.1777
2026-01-05 00:14:30,692: t15.2023.11.04 val PER: 0.0341
2026-01-05 00:14:30,693: t15.2023.11.17 val PER: 0.0327
2026-01-05 00:14:30,693: t15.2023.11.19 val PER: 0.0379
2026-01-05 00:14:30,693: t15.2023.11.26 val PER: 0.1123
2026-01-05 00:14:30,693: t15.2023.12.03 val PER: 0.1029
2026-01-05 00:14:30,693: t15.2023.12.08 val PER: 0.0952
2026-01-05 00:14:30,693: t15.2023.12.10 val PER: 0.0880
2026-01-05 00:14:30,693: t15.2023.12.17 val PER: 0.1268
2026-01-05 00:14:30,693: t15.2023.12.29 val PER: 0.1194
2026-01-05 00:14:30,693: t15.2024.02.25 val PER: 0.1011
2026-01-05 00:14:30,693: t15.2024.03.08 val PER: 0.2191
2026-01-05 00:14:30,693: t15.2024.03.15 val PER: 0.2014
2026-01-05 00:14:30,693: t15.2024.03.17 val PER: 0.1276
2026-01-05 00:14:30,693: t15.2024.05.10 val PER: 0.1486
2026-01-05 00:14:30,693: t15.2024.06.14 val PER: 0.1593
2026-01-05 00:14:30,693: t15.2024.07.19 val PER: 0.2327
2026-01-05 00:14:30,693: t15.2024.07.21 val PER: 0.0924
2026-01-05 00:14:30,694: t15.2024.07.28 val PER: 0.1287
2026-01-05 00:14:30,694: t15.2025.01.10 val PER: 0.2920
2026-01-05 00:14:30,694: t15.2025.01.12 val PER: 0.1363
2026-01-05 00:14:30,694: t15.2025.03.14 val PER: 0.3476
2026-01-05 00:14:30,694: t15.2025.03.16 val PER: 0.1859
2026-01-05 00:14:30,694: t15.2025.03.30 val PER: 0.2839
2026-01-05 00:14:30,694: t15.2025.04.13 val PER: 0.2154
2026-01-05 00:14:30,990: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_19500
2026-01-05 00:14:40,159: Train batch 19600: loss: 7.38 grad norm: 46.11 time: 0.057
2026-01-05 00:14:58,614: Train batch 19800: loss: 7.17 grad norm: 47.62 time: 0.055
2026-01-05 00:15:17,157: Running test after training batch: 19999
2026-01-05 00:15:17,247: WER debug GT example: You can see the code at this point as well.
2026-01-05 00:15:21,910: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 00:15:21,946: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 00:15:23,902: Val batch 19999: PER (avg): 0.1453 CTC Loss (avg): 14.9911 WER(1gram): 44.16% (n=64) time: 6.744
2026-01-05 00:15:23,902: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-05 00:15:23,902: t15.2023.08.13 val PER: 0.1091
2026-01-05 00:15:23,903: t15.2023.08.18 val PER: 0.1039
2026-01-05 00:15:23,903: t15.2023.08.20 val PER: 0.1064
2026-01-05 00:15:23,903: t15.2023.08.25 val PER: 0.0904
2026-01-05 00:15:23,903: t15.2023.08.27 val PER: 0.1833
2026-01-05 00:15:23,903: t15.2023.09.01 val PER: 0.0682
2026-01-05 00:15:23,903: t15.2023.09.03 val PER: 0.1508
2026-01-05 00:15:23,903: t15.2023.09.24 val PER: 0.1286
2026-01-05 00:15:23,903: t15.2023.09.29 val PER: 0.1347
2026-01-05 00:15:23,903: t15.2023.10.01 val PER: 0.1638
2026-01-05 00:15:23,903: t15.2023.10.06 val PER: 0.0840
2026-01-05 00:15:23,903: t15.2023.10.08 val PER: 0.2463
2026-01-05 00:15:23,903: t15.2023.10.13 val PER: 0.1901
2026-01-05 00:15:23,904: t15.2023.10.15 val PER: 0.1444
2026-01-05 00:15:23,904: t15.2023.10.20 val PER: 0.1980
2026-01-05 00:15:23,904: t15.2023.10.22 val PER: 0.1158
2026-01-05 00:15:23,904: t15.2023.11.03 val PER: 0.1791
2026-01-05 00:15:23,904: t15.2023.11.04 val PER: 0.0341
2026-01-05 00:15:23,904: t15.2023.11.17 val PER: 0.0342
2026-01-05 00:15:23,904: t15.2023.11.19 val PER: 0.0319
2026-01-05 00:15:23,904: t15.2023.11.26 val PER: 0.1087
2026-01-05 00:15:23,904: t15.2023.12.03 val PER: 0.1061
2026-01-05 00:15:23,904: t15.2023.12.08 val PER: 0.0965
2026-01-05 00:15:23,904: t15.2023.12.10 val PER: 0.0920
2026-01-05 00:15:23,904: t15.2023.12.17 val PER: 0.1279
2026-01-05 00:15:23,905: t15.2023.12.29 val PER: 0.1215
2026-01-05 00:15:23,905: t15.2024.02.25 val PER: 0.0969
2026-01-05 00:15:23,905: t15.2024.03.08 val PER: 0.2205
2026-01-05 00:15:23,905: t15.2024.03.15 val PER: 0.1982
2026-01-05 00:15:23,905: t15.2024.03.17 val PER: 0.1262
2026-01-05 00:15:23,905: t15.2024.05.10 val PER: 0.1456
2026-01-05 00:15:23,905: t15.2024.06.14 val PER: 0.1562
2026-01-05 00:15:23,905: t15.2024.07.19 val PER: 0.2340
2026-01-05 00:15:23,905: t15.2024.07.21 val PER: 0.0924
2026-01-05 00:15:23,905: t15.2024.07.28 val PER: 0.1272
2026-01-05 00:15:23,905: t15.2025.01.10 val PER: 0.2934
2026-01-05 00:15:23,906: t15.2025.01.12 val PER: 0.1339
2026-01-05 00:15:23,906: t15.2025.03.14 val PER: 0.3402
2026-01-05 00:15:23,906: t15.2025.03.16 val PER: 0.1898
2026-01-05 00:15:23,906: t15.2025.03.30 val PER: 0.2816
2026-01-05 00:15:23,906: t15.2025.04.13 val PER: 0.2111
2026-01-05 00:15:23,907: New best val WER(1gram) 44.42% --> 44.16%
2026-01-05 00:15:23,907: Checkpointing model
2026-01-05 00:15:24,538: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/best_checkpoint
2026-01-05 00:15:24,828: Saved model to checkpoint: /tmp/e12511253_b2t_348874/trained_models/input_dropout/lr40_wd1e-5/id20_wd1e-5/checkpoint/checkpoint_batch_19999
2026-01-05 00:15:24,858: Best avg val PER achieved: 0.14529
2026-01-05 00:15:24,858: Total training time: 36.11 minutes
All runs finished. Outputs in: /tmp/e12511253_b2t_348874/trained_models
